{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6cf09d",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Inho Lee/testAI/Breast_cancer_data.csv\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        print(line)\n",
    "        line = f.readline()\n",
    "\n",
    "\n",
    "with open(\"C:/Users/Inho Lee/testAI/Breast_cancer_data.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        print(parts)\n",
    "\n",
    "s = \"apple,banana,grape\"\n",
    "fruits = s.split(\",\")\n",
    "print(fruits) # ['apple', 'banana', 'grape']\n",
    "\n",
    "s = \"The quick brown fox\"\n",
    "words = s.split()\n",
    "print(words) # ['The', 'quick', 'brown', 'fox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "aa = []\n",
    "bb = []\n",
    "colors = []\n",
    "afile = open(\"fort.11\", \"r\")\n",
    "ii = 0\n",
    "for line in afile:\n",
    "    if len(line.split()) == 4:\n",
    "        continue\n",
    "    if len(line.split()) == 2:\n",
    "        ii = ii+1\n",
    "        bb.append(float(line.split()[1]))\n",
    "        aa.append(int(line.split()[0]))\n",
    "        colors.append(float(line.split()[0])/230.)\n",
    "afile.close()\n",
    "aa = np.array(aa)\n",
    "bb = np.array(bb)\n",
    "colors = np.array(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#딕셔너리를 만드는 법: 두 개의 리스트로 부터 딕셔너리를 만드는 방법.\n",
    "fruits = [\"Apple\", \"Pear\", \"Peach\", \"Banana\"]\n",
    "prices = [0.35, 0.40, 0.40, 0.28]\n",
    "adict = dict(zip(fruits, prices))\n",
    "print(adict)\n",
    "\n",
    "#리스트를 만드는 방법: range 함수를 이용하는 방법.\n",
    "alist=[ j for j in range(10)  ]\n",
    "alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71acdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_line(file_name, text_to_append):\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        file_object.seek(0)\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            file_object.write(\"\\n\")\n",
    "        file_object.write(text_to_append)\n",
    "\n",
    "\n",
    "def append_multiple_lines(file_name, lines_to_append):\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        appendEOL = False\n",
    "        file_object.seek(0)\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            appendEOL = True\n",
    "        for line in lines_to_append:\n",
    "            if appendEOL == True:\n",
    "                file_object.write(\"\\n\")\n",
    "            else:\n",
    "                appendEOL = True\n",
    "            file_object.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968783a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lines = ['First line', 'Second line', 'Third line']\n",
    "append_multiple_lines('target00.txt', list_of_lines)\n",
    "\n",
    "trialobj=0.\n",
    "lines_to_append = []\n",
    "lines_to_append.append(str(trialobj))\n",
    "append_multiple_lines('target00.txt', list_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df031c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "\n",
    "v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},\n",
    "                           {'baz': 1.0, 'foo': 3.0}]\n",
    "\n",
    "print(v.transform({'foo': 4, 'unseen_feature': 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(X.toarray())\n",
    "\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b7c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=10)\n",
    "D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n",
    "f = h.transform(D)\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = FeatureHasher(n_features=8, input_type=\"string\")\n",
    "raw_X = [[\"dog\", \"cat\", \"snake\"], [\"snake\", \"dog\"], [\"cat\", \"bird\"]]\n",
    "f = h.transform(raw_X)\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=100,\n",
    "                           n_informative=50, n_redundant=50, random_state=1)\n",
    "\n",
    "model = LGBMClassifier(max_bin=255, n_estimators=100)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2fb17",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[1, 1], [2, 3], [3, 2], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled = scaler.transform(data)\n",
    "print(scaled)\n",
    "# for inverse transformation\n",
    "inversed = scaler.inverse_transform(scaled)\n",
    "print(inversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "df = [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]\n",
    "le_fitted = le.fit_transform(df)\n",
    "print(le_fitted)\n",
    "inverted = le.inverse_transform(le_fitted)\n",
    "print(inverted)\n",
    "# array(['paris', 'paris', 'tokyo', 'amsterdam'], dtype='|S9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler divergence 계산 함수\n",
    "    \"\"\"\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "\n",
    "def jensen_shannon_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Jensen-Shannon divergence 계산 함수\n",
    "    \"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "\n",
    "# 예시\n",
    "p = np.array([0.1, 0.4, 0.5])\n",
    "q = np.array([0.4, 0.3, 0.3])\n",
    "kl_div = kl_divergence(p, q)\n",
    "print(\"Kullback-Leibler divergence: \", kl_div)\n",
    "\n",
    "kl_div = entropy(p, q)\n",
    "print(\"Kullback-Leibler divergence: \", kl_div)\n",
    "\n",
    "kl_div = kl_divergence(q, p)\n",
    "print(\"Kullback-Leibler divergence: \", kl_div)\n",
    "\n",
    "kl_div = entropy(q, p)\n",
    "print(\"Kullback-Leibler divergence: \", kl_div)\n",
    "\n",
    "jsd = jensen_shannon_divergence(p, q)\n",
    "print(\"Jensen-Shannon divergence: \", jsd)\n",
    "jsd = jensen_shannon_divergence(q, p)\n",
    "print(\"Jensen-Shannon divergence: \", jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample and plot imbalanced dataset with SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = where(y == label)[0]\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                       max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       random_state=None, splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cf0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :3]  # we only take the first three features.\n",
    "y = iris.target\n",
    "\n",
    "# make it binary classification problem\n",
    "X = X[np.logical_or(y == 0, y == 1)]\n",
    "y = y[np.logical_or(y == 0, y == 1)]\n",
    "\n",
    "model = svm.SVC(kernel='linear')\n",
    "#model = svm.SVC(kernel=\"rbf\", gamma=0.5, C=2)\n",
    "clf = model.fit(X, y)\n",
    "\n",
    "# The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.\n",
    "# Solve for w3 (z)\n",
    "\n",
    "\n",
    "def zz(x, y):\n",
    "    return (-clf.intercept_[0]-clf.coef_[0]\n",
    "            [0]*x - clf.coef_[0][1]*y) / clf.coef_[0][2]\n",
    "\n",
    "\n",
    "tmp = np.linspace(-4, 4, 30)\n",
    "xx, yy = np.meshgrid(tmp, tmp)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot3D(X[y == 0, 0], X[y == 0, 1], X[y == 0, 2],\n",
    "          'ob', markersize=3, alpha=0.5)\n",
    "ax.plot3D(X[y == 1, 0], X[y == 1, 1], X[y == 1, 2],\n",
    "          'sr', markersize=3, alpha=0.5)\n",
    "ax.plot_surface(xx, yy, zz(xx, yy))\n",
    "ax.view_init(19, 66)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "np.random.seed(17)\n",
    "npoints=100\n",
    "X = np.r_[np.random.randn(npoints, 2)-[2, 2], np.random.randn(npoints, 2)+[2, 2]]\n",
    "y = [0] * npoints + [1] * npoints\n",
    "# fit the model, 훈련\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X, y)\n",
    "# get the separating hyperplane, 초평면\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-5, 5)\n",
    "yy = a * xx-(clf.intercept_[0]) / w[1]\n",
    "margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "yy_down = yy-np.sqrt(1 + a ** 2) * margin\n",
    "yy_up = yy+np.sqrt(1 + a ** 2) * margin\n",
    "plt.figure(1, figsize=(4, 4))\n",
    "plt.clf()\n",
    "plt.plot(xx, yy, \"k-\")\n",
    "plt.plot(xx, yy_down, \"k-\")\n",
    "plt.plot(xx, yy_up, \"k-\")\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "            facecolors=\"none\", zorder=10, edgecolors=\"cyan\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "            edgecolors=\"k\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "feature_names = ['sl', 'sw', 'pl', 'pw']\n",
    "irisDF = pd.DataFrame(data=iris.data, columns=feature_names)\n",
    "irisDF['target'] = iris.target\n",
    "gmm = GaussianMixture(n_components=3, random_state=17).fit(iris.data)\n",
    "gmm_labels = gmm.predict(iris.data)\n",
    "irisDF['gmm_cluster'] = gmm_labels\n",
    "print(irisDF.groupby('target')['gmm_cluster'].value_counts())\n",
    "Xynew = gmm.sample(10)\n",
    "Xynew\n",
    "n_c=np.arange(1,21)\n",
    "models=[GaussianMixture(n,random_state=17).fit(iris.data) for n in n_c]\n",
    "plt.plot(n_c,[m.bic(iris.data) for m in models], label='BIC')\n",
    "plt.plot(n_c,[m.aic(iris.data) for m in models], label='AIC')\n",
    "plt.legend()\n",
    "plt.xlabel('n_components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cancer = load_breast_cancer()\n",
    "np.random.seed(9)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target)\n",
    "\n",
    "params = {'n_estimators': [100], 'max_depth': [6, 8, 10, 12], 'min_samples_leaf': [8, 12, 18],\n",
    "          'min_samples_split': [8, 16, 20]}\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "grid_clf = GridSearchCV(clf, param_grid=params, cv=2,\n",
    "                        n_jobs=-1)  # -1 은 cpu를 다 쓴다는 의미\n",
    "grid_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"optimal parameters\\n{grid_clf.best_params_}\")\n",
    "print(f\"best score: {grid_clf.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "import lightgbm as lgb\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(trial):\n",
    "    if False:\n",
    "        X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    df = pd.read_csv('C:/Users/Inho Lee/testAI/Breast_cancer_data.csv')\n",
    "    X = df[['mean_radius', 'mean_texture',\n",
    "            'mean_perimeter', 'mean_area', 'mean_smoothness']]\n",
    "    y = df['diagnosis']\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        X, y, test_size=0.3)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(valid_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b54ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "import warnings\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, validation_curve\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n",
    "\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "from numpy import argmax\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "\n",
    "if False:\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "if True:\n",
    "    df = pd.read_csv('C:/Users/Inho Lee/testAI/Breast_cancer_data.csv')\n",
    "    df.head()\n",
    "    X = df[['mean_radius', 'mean_texture',\n",
    "            'mean_perimeter', 'mean_area', 'mean_smoothness']]\n",
    "    y = df['diagnosis']\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(X)\n",
    "\n",
    "X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n",
    "\n",
    "def create_base_model(X, y, test_size=0.20, cv=5, plot=False, save_results=False):\n",
    "    names = []\n",
    "    acc_results = []\n",
    "    acc_train_results = []\n",
    "    acc_test_results = []\n",
    "    r2_results = []\n",
    "    r2_train_results = []\n",
    "    r2_test_results = []\n",
    "    cv_results_acc = []\n",
    "    cv_results_f1 = []\n",
    "    cv_results_roc_auc = []\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=17)\n",
    "    rs = 1234\n",
    "    models = [('LogR', LogisticRegression(random_state=rs)),\n",
    "              (\"NB\", GaussianNB()),\n",
    "              (\"KNN\", KNeighborsClassifier()),\n",
    "              (\"SVC\", SVC(random_state=rs)),\n",
    "              ('ANN', MLPClassifier(random_state=rs)),\n",
    "              ('CART', DecisionTreeClassifier(random_state=rs)),\n",
    "              ('RF', RandomForestClassifier(random_state=rs)),\n",
    "              (\"AdaBoost\", AdaBoostClassifier(random_state=rs)),\n",
    "              ('BGTrees', BaggingClassifier(bootstrap_features=True, random_state=rs)),\n",
    "              ('GBM', GradientBoostingClassifier(random_state=rs)),\n",
    "              (\"XGBoost\", XGBClassifier(objective='reg:squarederror', random_state=rs)),\n",
    "              (\"LightGBM\", LGBMClassifier(random_state=rs)),\n",
    "              (\"CatBoost\", CatBoostClassifier(verbose=False, random_state=rs))]\n",
    "\n",
    "    for name, classifier in models:\n",
    "        model_fit = classifier.fit(X_train, y_train)\n",
    "        # Acc Score\n",
    "        acc = accuracy_score(y, model_fit.predict(X))\n",
    "        acc_train = accuracy_score(y_train, model_fit.predict(X_train))\n",
    "        acc_test = accuracy_score(y_test, model_fit.predict(X_test))\n",
    "        acc_results.append(acc)\n",
    "        acc_train_results.append(acc_train)\n",
    "        acc_test_results.append(acc_test)\n",
    "\n",
    "        # R2 Score\n",
    "        r2 = model_fit.score(X, y)\n",
    "        r2_train = model_fit.score(X_train, y_train)\n",
    "        r2_test = model_fit.score(X_test, y_test)\n",
    "        r2_results.append(r2)\n",
    "        r2_train_results.append(r2_train)\n",
    "        r2_test_results.append(r2_test)\n",
    "\n",
    "        # Cross Validate Score\n",
    "        cv_result = cross_validate(model_fit, X, y, cv=cv, scoring=[\n",
    "                                   \"accuracy\", \"f1\", \"roc_auc\"])\n",
    "        cv_result_acc = cv_result[\"test_accuracy\"].mean()\n",
    "        cv_result_f1 = cv_result[\"test_f1\"].mean()\n",
    "        cv_result_roc_auc = cv_result[\"test_roc_auc\"].mean()\n",
    "        cv_results_acc.append(cv_result_acc)\n",
    "        cv_results_f1.append(cv_result_f1)\n",
    "        cv_results_roc_auc.append(cv_result_roc_auc)\n",
    "\n",
    "        # Model names\n",
    "        names.append(name)\n",
    "\n",
    "    model_results = pd.DataFrame({'Model_Names': names,\n",
    "                                  'Acc': acc_results,\n",
    "                                  'Acc_Train': acc_train_results,\n",
    "                                  'Acc_Test': acc_test_results,\n",
    "                                  'R2': acc_train_results,\n",
    "                                  'R2_Train': r2_train_results,\n",
    "                                  'R2_Test': r2_test_results,\n",
    "                                  'CV_Acc': cv_results_acc,\n",
    "                                  'CV_f1': cv_results_f1,\n",
    "                                  'CV_roc_auc': cv_results_roc_auc\n",
    "                                  }).set_index(\"Model_Names\")\n",
    "    model_results = model_results.sort_values(by=\"CV_roc_auc\", ascending=False)\n",
    "    print(model_results)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        sns.barplot(x='CV_roc_auc', y=model_results.index,\n",
    "                    data=model_results, color=\"r\")\n",
    "        plt.xlabel('Roc_Auc')\n",
    "        plt.ylabel('Model Names')\n",
    "        plt.title('Roc_Auc for All Models')\n",
    "        plt.show()\n",
    "\n",
    "    if save_results:\n",
    "        model_results.to_csv(\"model_results.csv\")\n",
    "\n",
    "    return model_results\n",
    "\n",
    "model_results = create_base_model(\n",
    "    X, y, test_size=0.20, cv=3, plot=True, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b017543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# Build a classification task using 3 informative features\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=10,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False)\n",
    "\n",
    "# Build a forest and compute the impurity-based feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std(\n",
    "    [tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (12.0, 8.0)})\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "#boston = load_boston()\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "X = data\n",
    "y = target\n",
    "#X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "#y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=12)\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ee07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc23cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from numpy import quantile, random, where\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(17)\n",
    "X, _ = make_blobs(n_samples=300, centers=1, cluster_std=.3, center_box=(20, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=_, s=25, edgecolor=\"k\")\n",
    "\n",
    "iiff= IsolationForest(n_estimators=100, contamination=.03)\n",
    "predictions = iiff.fit_predict(X)\n",
    "\n",
    "outlier_index = where(predictions == -1)\n",
    "values = X[outlier_index]\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(values[:, 0], values[:, 1], color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# 이상치 탐지 모델 생성\n",
    "clf = IsolationForest(n_estimators=100, contamination=.03,\n",
    "                      random_state=0).fit(X)\n",
    "\n",
    "# 이상치 탐지\n",
    "outliers = clf.predict(X) == -1\n",
    "\n",
    "# 결과 출력\n",
    "print('outliers:', outliers.sum())\n",
    "print('outlier indices:', np.where(outliers))\n",
    "\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# 랜덤으로 데이터 생성\n",
    "X = 0.3 * np.random.randn(1000, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "\n",
    "# One-class SVM 모델 훈련\n",
    "clf = svm.OneClassSVM(nu=0.01, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# 이상치 비율 계산\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "error_rate_train = n_error_train / y_pred_train.size\n",
    "error_rate_test = n_error_test / y_pred_test.size\n",
    "\n",
    "# 결과 출력\n",
    "print(\"ratio (training data):\", error_rate_train)\n",
    "print(\"ratio (test data):\", error_rate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from minisom import MiniSom\n",
    "from matplotlib.patches import RegularPolygon, Ellipse\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import cm, colorbar\n",
    "from matplotlib.lines import Line2D\n",
    "from bokeh.colors import RGB\n",
    "from bokeh.io import curdoc, show, output_notebook\n",
    "from bokeh.transform import factor_mark, factor_cmap\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.plotting import figure, output_file\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(y)\n",
    "amax = np.amax(X)\n",
    "print(amax)\n",
    "X = X/amax\n",
    "\n",
    "aarray = unique_labels(y)\n",
    "print(len(aarray))\n",
    "\n",
    "som_shape = (7, 7)\n",
    "# initialization and training of 7x7 SOM\n",
    "som = MiniSom(som_shape[0], som_shape[1], X.shape[1], sigma=1.5, learning_rate=.7, activation_distance='euclidean',\n",
    "              topology='hexagonal', neighborhood_function='gaussian', random_seed=10)\n",
    "som.train(X, 1000, verbose=True)\n",
    "\n",
    "xx, yy = som.get_euclidean_coordinates()\n",
    "umatrix = som.distance_map()\n",
    "weights = som.get_weights()\n",
    "\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "ax = f.add_subplot(111)\n",
    "ax.set_aspect('equal')\n",
    "# iteratively add hexagons\n",
    "for i in range(weights.shape[0]):\n",
    "    for j in range(weights.shape[1]):\n",
    "        wy = yy[(i, j)] * np.sqrt(3) / 2\n",
    "        hexa = RegularPolygon((xx[(i, j)], wy),\n",
    "                              numVertices=6,\n",
    "                              radius=0.95 / np.sqrt(3),\n",
    "                              facecolor=cm.Blues(umatrix[i, j]),\n",
    "                              alpha=0.4,\n",
    "                              edgecolor='gray')\n",
    "        ax.add_patch(hexa)\n",
    "\n",
    "markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"s\", \"p\", \"P\", \"*\", \"h\", \"H\", \"+\",\n",
    "           \"x\", \"X\", \"D\", \"d\", \"|\", \"_\", \"1\", \"2\", \"3\", \"4\", \"8\",\n",
    "           0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "colors = ['C'+str(i) for i in range(len(markers))]\n",
    "\n",
    "for cnt, x in enumerate(X):\n",
    "    # getting the winner\n",
    "    w = som.winner(x)\n",
    "    # place a marker on the winning position for the sample xx\n",
    "    wx, wy = som.convert_map_to_euclidean(w)\n",
    "    wy = wy * np.sqrt(3) / 2\n",
    "    plt.plot(wx, wy,\n",
    "             markers[y[cnt]],\n",
    "             markerfacecolor='None',\n",
    "             markeredgecolor=colors[y[cnt]],\n",
    "             markersize=12,\n",
    "             markeredgewidth=2)\n",
    "\n",
    "xrange = np.arange(weights.shape[0])\n",
    "yrange = np.arange(weights.shape[1])\n",
    "plt.xticks(xrange-.5, xrange)\n",
    "plt.yticks(yrange * np.sqrt(3) / 2, yrange)\n",
    "\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "ax_cb = divider.new_horizontal(size=\"5%\", pad=0.05)\n",
    "cb1 = colorbar.ColorbarBase(ax_cb, cmap=cm.Blues,\n",
    "                            orientation='vertical', alpha=.4)\n",
    "cb1.ax.get_yaxis().labelpad = 16\n",
    "cb1.ax.set_ylabel('distance from neurons in the neighbourhood',\n",
    "                  rotation=270, fontsize=16)\n",
    "plt.gcf().add_axes(ax_cb)\n",
    "legend_elements = [Line2D([0], [0], marker=markers[i], color=colors[i], label=str(i),\n",
    "                   markerfacecolor='w', markersize=14, linestyle='None', markeredgewidth=2) for i in range(len(aarray))]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(0.1, 1.08), loc='upper left',\n",
    "          borderaxespad=0., ncol=3, fontsize=14)\n",
    "# plt.savefig('som_seed_hex.png')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the response for each pattern in the iris dataset\n",
    "plt.bone()\n",
    "plt.pcolor(som.distance_map().T)  # plotting the distance map as background\n",
    "plt.colorbar()\n",
    "plt.bone()\n",
    "\n",
    "plt.pcolor(som.distance_map().T)  # distance map as background\n",
    "plt.colorbar()\n",
    "# loading the labels\n",
    "# use different colors and markers for each label\n",
    "for cnt, xx in enumerate(X):\n",
    "    w = som.winner(xx)  # getting the winner\n",
    "    # palce a marker on the winning position for the sample xx\n",
    "    plt.plot(w[0]+.5, w[1]+.5, markers[y[cnt]], markerfacecolor='None',\n",
    "             markeredgecolor=colors[y[cnt]], markersize=12, markeredgewidth=2)\n",
    "#plt.axis([0, 7, 0, 7])\n",
    "plt.show()\n",
    "\n",
    "if True:\n",
    "    som = MiniSom(som_shape[0], som_shape[1], X.shape[1], sigma=1.5, learning_rate=.7,           activation_distance='euclidean',\n",
    "                  neighborhood_function='gaussian', random_seed=10)\n",
    "    max_iter = 1000\n",
    "    q_error = []\n",
    "    t_error = []\n",
    "    for i in range(max_iter):\n",
    "        rand_i = np.random.randint(len(X))\n",
    "        som.update(X[rand_i], som.winner(X[rand_i]), i, max_iter)\n",
    "        q_error.append(som.quantization_error(X))\n",
    "        t_error.append(som.topographic_error(X))\n",
    "    plt.plot(np.arange(max_iter), q_error, label='Quantization error')\n",
    "    plt.plot(np.arange(max_iter), t_error, label='Topographic error')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "sns.set()\n",
    "sns.set(rc={\"figure.figsize\": (10, 8)})\n",
    "sns.set(font_scale=1.5)\n",
    "PALETTE = sns.color_palette('deep', n_colors=5)\n",
    "CMAP = ListedColormap(PALETTE.as_hex())\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def plot_iris_2d(xx, yy, y, title):\n",
    "    markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"s\", \"p\", \"P\", \"*\", \"h\", \"H\", \"+\",\n",
    "               \"x\", \"X\", \"D\", \"d\", \"|\", \"_\", \"1\", \"2\", \"3\", \"4\", \"8\",\n",
    "               0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    colors = ['C'+str(i) for i in range(len(markers))]\n",
    "    sns.set_style(\"darkgrid\")\n",
    "#    plt.scatter(xx, yy, c=y, cmap=CMAP, s=70)\n",
    "    for i in range(len(unique_labels(y))):\n",
    "        plt.scatter(xx[y == i], yy[y == i], color=colors[i],\n",
    "                    alpha=0.8, lw=2, label=str(i))\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(\"1st axis\", fontsize=16)\n",
    "    plt.ylabel(\"2nd axis\", fontsize=16)\n",
    "    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "\n",
    "\n",
    "def plot_iris_3d(xx, yy, zz, y, title):\n",
    "    markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"s\", \"p\", \"P\", \"*\", \"h\", \"H\", \"+\",\n",
    "               \"x\", \"X\", \"D\", \"d\", \"|\", \"_\", \"1\", \"2\", \"3\", \"4\", \"8\",\n",
    "               0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    colors = ['C'+str(i) for i in range(len(markers))]\n",
    "    sns.set_style('whitegrid')\n",
    "    fig = plt.figure(1, figsize=(8, 6))\n",
    "    ax = Axes3D(fig, elev=-150, azim=110)\n",
    "#    ax.scatter(xx, yy, zz, c=y, cmap=CMAP, s=40)\n",
    "    for i in range(len(unique_labels(y))):\n",
    "        ax.scatter(xx[y == i], yy[y == i], zz[y == i], color=colors[i],\n",
    "                   alpha=0.8, lw=2, label=str(i))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    fsize = 14\n",
    "    ax.set_xlabel(\"1st axis\", fontsize=fsize)\n",
    "    ax.set_ylabel(\"2nd axis\", fontsize=fsize)\n",
    "    ax.set_zlabel(\"3rd axis\", fontsize=fsize)\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(X)\n",
    "plot_iris_2d(points[:, 0], points[:, 1], y,\n",
    "             title='dataset visualized with PCA')\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=RANDOM_STATE)\n",
    "points = tsne.fit_transform(X)\n",
    "plot_iris_2d(points[:, 0], points[:, 1], y,\n",
    "             title='dataset visualized with TSNE')\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "points = pca.fit_transform(X)\n",
    "plot_iris_3d(points[:, 0], points[:, 1], points[:, 2],\n",
    "             y, title=\"dataset visualized with PCA\")\n",
    "\n",
    "tsne = TSNE(n_components=3,  n_iter=2000, random_state=RANDOM_STATE)\n",
    "points = tsne.fit_transform(X)\n",
    "plot_iris_3d(points[:, 0], points[:, 1], points[:, 2],\n",
    "             y, title=\"dataset visualized with TSNE\")\n",
    "\n",
    "clusters = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i).fit(X)\n",
    "    clusters.append(km.inertia_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\n",
    "ax.set_title('Searching for Elbow')\n",
    "ax.set_xlabel('Clusters')\n",
    "ax.set_ylabel('Inertia')\n",
    "\n",
    "# Annotate arrow\n",
    "ax.annotate('Possible Elbow Point', xy=(3, 2), xytext=(3, 5), xycoords='data',\n",
    "            arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n",
    "\n",
    "ax.annotate('Possible Elbow Point', xy=(5, 2), xytext=(5, 5), xycoords='data',\n",
    "            arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# synthetic classification dataset\n",
    "#from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=2000, n_features=30, n_informative=10, n_redundant=10, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "num_inputs = X.shape[1]\n",
    "num_hidden = 2\n",
    "num_outputs = num_inputs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_inputs, input_shape=[num_inputs]))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(num_hidden, activation='relu'))\n",
    "model.add(Dense(10,  activation='relu'))\n",
    "model.add(Dense(25,  activation='relu'))\n",
    "model.add(Dense(50,  activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_outputs, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "history = model.fit(X, X, validation_split=0.20,\n",
    "                    epochs=250, batch_size=5, verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss', fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "intermediate_layer_model = Model(\n",
    "    inputs=model.input, outputs=model.get_layer(index=5).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X)\n",
    "\n",
    "intermediate_output.shape\n",
    "sns.scatterplot(intermediate_output[:, 0], intermediate_output[:, 1], hue=y)\n",
    "\n",
    "\n",
    "YY = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "YY.shape\n",
    "nfeatures = X.shape[1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, YY, test_size=0.2)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=nfeatures, activation='relu'))\n",
    "for i in range(4):\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(YY.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, validation_split=0.10,\n",
    "                    epochs=100, batch_size=5, verbose=2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['training', 'validation'], loc='lower right', bbox_to_anchor=(\n",
    "    0.6, 0.2), ncol=1, frameon=True, shadow=True, fontsize=14)\n",
    "plt.show()\n",
    "if True:\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"saved model to disk\")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(9, 9))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5,\n",
    "            square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "all_sample_title = 'Confusion matrix - ' + \\\n",
    "    str(metrics.accuracy_score(y_test, y_pred))\n",
    "plt.title(all_sample_title, size=15)\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(y)\n",
    "classes = le.classes_\n",
    "len(list(le.classes_))\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y_label, test_size=0.30, random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": len(list(le.classes_)),\n",
    "    \"num_leaves\": 60,\n",
    "    \"max_depth\": -1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"bagging_fraction\": 0.9,  # subsample\n",
    "    \"feature_fraction\": 0.9,  # colsample_bytree\n",
    "    \"bagging_freq\": 5,        # subsample_freq\n",
    "    \"bagging_seed\": 2018,\n",
    "    \"verbosity\": -1}\n",
    "\n",
    "lgtrain, lgval = lgb.Dataset(x_train, y_train), lgb.Dataset(x_test, y_test)\n",
    "lgbmodel = lgb.train(params, lgtrain, 2000, valid_sets=[\n",
    "                     lgtrain, lgval], early_stopping_rounds=100, verbose_eval=200)\n",
    "\n",
    "\n",
    "y_pred = np.argmax(lgbmodel.predict(x_test), axis=1)\n",
    "y_true = y_test\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, classes=classes,\n",
    "                      title='Confusion matrix')\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 22)\n",
    "lgb.plot_importance(lgbmodel, max_num_features=x_test.shape[1], height=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb58a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "# Take the first 500 data points: it's hard to see 1500 points\n",
    "X = digits.data[:500]\n",
    "y = digits.target[:500]\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "target_ids = range(len(digits.target_names))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple'\n",
    "for i, c, label in zip(target_ids, colors, digits.target_names):\n",
    "    plt.scatter(X_2d[y == i, 0], X_2d[y == i, 1], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04821949",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea58c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "# Make data: Two circles on x-y plane as a classification problem\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    Dense(5, \"relu\"),\n",
    "    Dense(1, \"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X, y, batch_size=32, epochs=100, verbose=0)\n",
    "print(model.evaluate(X, y))\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    Dense(5, \"sigmoid\"),\n",
    "    Dense(5, \"sigmoid\"),\n",
    "    Dense(5, \"sigmoid\"),\n",
    "    Dense(1, \"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X, y, batch_size=32, epochs=100, verbose=0)\n",
    "print(model.evaluate(X, y))\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    Dense(5, \"relu\"),\n",
    "    Dense(5, \"relu\"),\n",
    "    Dense(5, \"relu\"),\n",
    "    Dense(1, \"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.fit(X, y, batch_size=32, epochs=100, verbose=0)\n",
    "print(model.evaluate(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "#(x_train, y_train), (x_test, y_test) = tf.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_train = tf.one_hot(y_train, depth=len(np.unique(y_train)))\n",
    "y_test = tf.one_hot(y_test, depth=len(np.unique(y_train)))\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "#model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# CPU\n",
    "print(\"CPU\")\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    model.fit(x_train, y_train, batch_size=1000, epochs=20)\n",
    "\n",
    "print(\"GPU\")\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    model.fit(x_train, y_train, batch_size=1000, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())\n",
    "tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "import tensorflow.python.platform.build_info as build\n",
    "print(build.build_info)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "tf.config.list_physical_devices('CPU')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20,\n",
    "                       noise=0.1, random_state=1)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y), 1))[:, 0]\n",
    "\n",
    "n_train = 800\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(25, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_logarithmic_error',optimizer=opt, metrics=['mse'])\n",
    "history = model.fit(trainX, trainy, validation_data=(\n",
    "    testX, testy), epochs=150, verbose=0)\n",
    "_, train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['mse'], label='train')\n",
    "pyplot.plot(history.history['val_mse'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Dropout, Flatten, SpatialDropout2D, SpatialDropout1D, AlphaDropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "def get_dropout(input_tensor, p=0.5, mc=False):\n",
    "    if mc:\n",
    "        return Dropout(p)(input_tensor, training=True)\n",
    "    else:\n",
    "        return Dropout(p)(input_tensor)\n",
    "\n",
    "\n",
    "def get_model(mc=False, act=\"relu\"):\n",
    "    inp = Input(input_shape)\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation=act)(inp)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation=act)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = get_dropout(x, p=0.25, mc=mc)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation=act)(x)\n",
    "    x = get_dropout(x, p=0.5, mc=mc)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(mc=False, act=\"relu\")\n",
    "mc_model = get_model(mc=True, act=\"relu\")\n",
    "h = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "# score of the normal model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "h_mc = mc_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "mc_predictions = []\n",
    "for i in tqdm.tqdm(range(500)):\n",
    "    y_p = mc_model.predict(x_test, batch_size=1000)\n",
    "    mc_predictions.append(y_p)\n",
    "# score of the mc model\n",
    "accs = []\n",
    "for y_p in mc_predictions:\n",
    "    acc = accuracy_score(y_test.argmax(axis=1), y_p.argmax(axis=1))\n",
    "    accs.append(acc)\n",
    "print(\"MC accuracy: {:.1%}\".format(sum(accs)/len(accs)))\n",
    "mc_ensemble_pred = np.array(mc_predictions).mean(axis=0).argmax(axis=1)\n",
    "ensemble_acc = accuracy_score(y_test.argmax(axis=1), mc_ensemble_pred)\n",
    "print(\"MC-ensemble accuracy: {:.1%}\".format(ensemble_acc))\n",
    "plt.hist(accs)\n",
    "plt.axvline(x=ensemble_acc, color=\"b\")\n",
    "idx = 247\n",
    "plt.imshow(x_test[idx][:, :, 0])\n",
    "p0 = np.array([p[idx] for p in mc_predictions])\n",
    "print(\"posterior mean: {}\".format(p0.mean(axis=0).argmax()))\n",
    "print(\"true label: {}\".format(y_test[idx].argmax()))\n",
    "print()\n",
    "# probability + variance\n",
    "for i, (prob, var) in enumerate(zip(p0.mean(axis=0), p0.std(axis=0))):\n",
    "    print(\"class: {}; proba: {:.1%}; var: {:.2%} \".format(i, prob, var))\n",
    "x, y = list(range(len(p0.mean(axis=0)))), p0.mean(axis=0)\n",
    "plt.plot(x, y)\n",
    "fig, axes = plt.subplots(5, 2, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(fig.get_axes()):\n",
    "    ax.hist(p0[:, i], bins=100, range=(0, 1))\n",
    "    ax.set_title(f\"class {i}\")\n",
    "    ax.label_outer()\n",
    "max_means = []\n",
    "preds = []\n",
    "for idx in range(len(mc_predictions)):\n",
    "    px = np.array([p[idx] for p in mc_predictions])\n",
    "    preds.append(px.mean(axis=0).argmax())\n",
    "    max_means.append(px.mean(axis=0).max())\n",
    "(np.array(max_means)).argsort()[:10]\n",
    "plt.imshow(x_test[247][:, :, 0])\n",
    "max_vars = []\n",
    "for idx in range(len(mc_predictions)):\n",
    "    px = np.array([p[idx] for p in mc_predictions])\n",
    "    max_vars.append(px.std(axis=0)[px.mean(axis=0).argmax()])\n",
    "(-np.array(max_vars)).argsort()[:10]\n",
    "plt.imshow(x_test[259][:, :, 0])\n",
    "random_img = np.random.random(input_shape)\n",
    "plt.imshow(random_img[:,:,0])\n",
    "random_predictions = []\n",
    "for i in tqdm.tqdm(range(500)):\n",
    "    y_p = mc_model.predict(np.array([random_img]))\n",
    "    random_predictions.append(y_p)\n",
    "p0 = np.array([p[0] for p in random_predictions])\n",
    "print(\"posterior mean: {}\".format(p0.mean(axis=0).argmax()))\n",
    "print()\n",
    "# probability + variance\n",
    "for i, (prob, var) in enumerate(zip(p0.mean(axis=0), p0.std(axis=0))):\n",
    "    print(\"class: {}; proba: {:.1%}; var: {:.2%} \".format(i, prob, var))\n",
    "x, y = list(range(len(p0.mean(axis=0)))), p0.mean(axis=0)\n",
    "plt.plot(x, y)\n",
    "fig, axes = plt.subplots(5, 2, figsize=(12,12))\n",
    "\n",
    "for i, ax in enumerate(fig.get_axes()):\n",
    "    ax.hist(p0[:,i], bins=100, range=(0,1));\n",
    "    ax.set_title(f\"class {i}\")\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591aecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "iris_data = load_iris()  # load the iris dataset\n",
    "if True:\n",
    "    print('Example data: ')\n",
    "    print(iris_data.data[:5])\n",
    "    print('Example labels: ')\n",
    "    print(iris_data.target[:5])\n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)  # Convert data to a single column\n",
    "# One Hot encode the class labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\n",
    "model.add(Dense(10, activation='relu', name='fc2'))\n",
    "model.add(Dense(3, activation='softmax', name='output'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\n",
    "results = model.evaluate(test_x, test_y)\n",
    "print('Final test set loss: {:4f}'.format(results[0]))\n",
    "print('Final test set accuracy: {:4f}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=n_inputs,\n",
    "              kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(n_outputs, kernel_initializer='he_uniform'))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "if True:\n",
    "    X, y = make_regression(n_samples=1000, n_features=10,\n",
    "                           n_informative=5, n_targets=3, random_state=17)\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "# get model\n",
    "model = get_model(n_inputs, n_outputs)\n",
    "# fit the model on all data\n",
    "model.fit(X, y, verbose=1, epochs=100)\n",
    "# make a prediction for new data\n",
    "row = [-0.99859353, 2.19284309, -0.42632569, -0.21043258, -1.13655612, \n",
    "       -0.55671602, -0.63169045, -0.87625098, -0.99445578, -0.3677487]\n",
    "newX = np.asarray([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ecc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "def spiral_xy(i, spiral_num):\n",
    "    \"\"\"\n",
    "    Create the data for a spiral.\n",
    "\n",
    "    Arguments:\n",
    "        i runs from 0 to 96\n",
    "        spiral_num is 1 or -1\n",
    "    \"\"\"\n",
    "    φ = i/16 * math.pi\n",
    "    r = 6.5 * ((104 - i)/104)\n",
    "    x = (r * math.cos(φ) * spiral_num)/13 + 0.5\n",
    "    y = (r * math.sin(φ) * spiral_num)/13 + 0.5\n",
    "    return (x, y)\n",
    "\n",
    "def spiral(spiral_num):\n",
    "    return [spiral_xy(i, spiral_num) for i in range(97)]\n",
    "\n",
    "def spirals(points, noise=.5):\n",
    "    n = np.sqrt(np.random.rand(points, 1)) * 780 * (2*np.pi)/360\n",
    "    d1x = -np.cos(n)*n + np.random.rand(points, 1) * noise\n",
    "    d1y = np.sin(n)*n + np.random.rand(points, 1) * noise\n",
    "    d2x = np.cos(n)*n + np.random.rand(points, 1) * noise\n",
    "    d2y = -np.sin(n)*n + np.random.rand(points, 1) * noise\n",
    "    return (np.vstack((np.hstack((d1x, d1y)), np.hstack((d2x, d2y)))),\n",
    "            np.hstack((np.zeros(points), np.ones(points))))\n",
    "\n",
    "\n",
    "kase= -5\n",
    "kase= -20\n",
    "if kase == -5 :\n",
    "    X, y = make_moons(n_samples=1000, noise=0.1, random_state=17)\n",
    "if kase == -20:\n",
    "    X, y = spirals(1000)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(y[0:10])\n",
    "print(y[-10:])\n",
    "if True:\n",
    "    alist = []\n",
    "    blist = []\n",
    "    clist = []\n",
    "    dlist = []\n",
    "    ay = []\n",
    "    cy = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] > 0.5:\n",
    "            alist.append(X[i, 0])\n",
    "            blist.append(X[i, 1])\n",
    "            ay.append(y[i])\n",
    "        else:\n",
    "            clist.append(X[i, 0])\n",
    "            dlist.append(X[i, 1])\n",
    "            cy.append(y[i])\n",
    "    alist = np.array(alist)\n",
    "    blist = np.array(blist)\n",
    "    clist = np.array(clist)\n",
    "    dlist = np.array(dlist)\n",
    "    ay = np.array(ay)\n",
    "    cy = np.array(cy)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(alist[:], blist[:])\n",
    "    plt.scatter(clist[:], dlist[:])\n",
    "    plt.xlabel(r'feature$_1$', fontsize=20)\n",
    "    plt.ylabel(r'feature$_2$', fontsize=20)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "\n",
    "if True:\n",
    "    model = tf.keras.Sequential()\n",
    "    # Input layer\n",
    "    model.add(tf.keras.layers.Dense(12, input_dim=2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    results = model.fit(X_train, y_train, epochs=500,\n",
    "                        validation_data=(X_test, y_test), verbose=0)\n",
    "#   prediction_values = model.predict_classes(X_test)\n",
    "    y_prob = model.predict(X_test, verbose=0)\n",
    "#    prediction_values = y_prob.argmax(axis=-1)\n",
    "    y_prob= (y_prob > 0.5).astype('int32')\n",
    "    print(y_test[:10])\n",
    "    print(y_prob[:10])\n",
    "#        if y_prob.shape[-1] > 1:\n",
    "#            return proba.argmax(axis=-1)\n",
    "#        else:\n",
    "#            return (y_prob > 0.5).astype('int32')\n",
    "    \n",
    "\n",
    "print(\"Prediction values shape:\", y_prob.shape)\n",
    "print(np.mean(results.history[\"val_accuracy\"]))\n",
    "print(\"Evaluating on training set\")\n",
    "(loss, accuracy) = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "print(\"Evaluating on test set\")\n",
    "(loss, accuracy) = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "if True:\n",
    "    from sklearn import metrics\n",
    "    y_prob = model.predict(X_test, verbose=0)\n",
    "#    y_pred = y_prob.argmax(axis=-1)\n",
    "    y_prob= (y_prob > 0.5).astype('int32')\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_prob)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'test'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(212)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1],\n",
    "            c=y_prob[:], cmap=cm.coolwarm)\n",
    "plt.title('Model predictions on our Test set')\n",
    "plt.axis('equal')\n",
    "xx = np.linspace(kase, -kase, 400)\n",
    "yy = np.linspace(kase, -kase, 400)\n",
    "gx, gy = np.meshgrid(xx, yy)\n",
    "Z = model.predict(np.c_[gx.ravel(), gy.ravel()])\n",
    "Z = Z.reshape(gx.shape)\n",
    "plt.contourf(gx, gy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([kase, -kase])\n",
    "axes.set_ylim([kase, -kase])\n",
    "plt.grid('off')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1],\n",
    "            c=y_prob[:], cmap=cm.coolwarm)\n",
    "plt.title('Model predictions on our test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d649f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b465a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiclass(N=2000, D=2, K=3):\n",
    "    \"\"\"\n",
    "    N: number of points per class\n",
    "    D: dimensionality\n",
    "    K: number of classes\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K)\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        # radius\n",
    "        r = np.linspace(0.0, 1, N)\n",
    "        # theta\n",
    "        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.11\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([-1, 1])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, X, y):\n",
    "    #    y_pred = model.predict_classes(X, verbose=0)\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    y_pred.ndim\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = (y_pred > 0.5).astype('int32')\n",
    "    else:\n",
    "        y_pred = y_pred.argmax(axis=-1)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)),\n",
    "                annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['accuracy'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "\n",
    "\n",
    "def plot_multiclass_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 601),\n",
    "                         np.linspace(y_min, y_max, 601))\n",
    "#    cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    cmap = ListedColormap(['r', 'g', 'b'])\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()], verbose=0)\n",
    "    if Z.ndim == 1:\n",
    "        Z = (Z > 0.5).astype('int32')\n",
    "    else:\n",
    "        Z = Z.argmax(axis=-1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ffc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_multiclass(K=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(Adam(learning_rate=0.005),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "y_cat = to_categorical(y_train)\n",
    "history = model.fit(X_train, y_cat, verbose=0, epochs=100)\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_decision_boundary(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d942693",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, verbose=0)\n",
    "print(y_pred.ndim)\n",
    "if y_pred.ndim == 1:\n",
    "    y_pred = (y_pred > 0.5).astype('int32')\n",
    "else:\n",
    "    y_pred = y_pred.argmax(axis=-1)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_confusion_matrix(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, validation_data=(\n",
    "    X_test, y_test), epochs=10, batch_size=128, verbose=0)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "#     this is the size of our encoded representations\n",
    "# 32 floats -> compression of factor 24.5, assuming the input is 784 floats, 28*28/32 = 24.5\n",
    "encoding_dim = 32\n",
    "\n",
    "#     this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "#     \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "#      \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "#      this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "#      this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "#     create a placeholder for an encoded 32D input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "#     retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "#     create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256,\n",
    "                shuffle=True, validation_data=(x_test, x_test))\n",
    "#     encode and decode some digits\n",
    "#     note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "#     use matplotlib\n",
    "n = 10                     # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    #      display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    #      display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25207234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2,\n",
    "                  padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\",\n",
    "                           strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\",\n",
    "                           strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(\n",
    "    1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var -\n",
    "                              tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=30, batch_size=128)\n",
    "\n",
    "\n",
    "def plot_latent_space(vae, n=30, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size: (i + 1) * digit_size,\n",
    "                j * digit_size: (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(vae)\n",
    "\n",
    "\n",
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "X, y = make_classification(n_samples=100000, n_features=32, n_informative=32,\n",
    "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.995, 0.005],\n",
    "                           class_sep=0.5, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "print('The number of records in the training dataset is', X_train.shape[0])\n",
    "print('The number of records in the test dataset is', X_test.shape[0])\n",
    "print(f\"The training dataset has {sorted(Counter(y_train).items())[0][1]} \\\n",
    "    records for the majority class and {sorted(Counter(y_train).items())[1][1]} \\\n",
    "    records for the minority class.\")\n",
    "X_train_normal = X_train[np.where(y_train == 0)]\n",
    "input = tf.keras.layers.Input(shape=(32,))\n",
    "encoder = tf.keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(4, activation='relu')])(input)\n",
    "decoder = tf.keras.Sequential([\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"sigmoid\")])(encoder)\n",
    "autoencoder = tf.keras.Model(inputs=input, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "history = autoencoder.fit(X_train_normal, X_train_normal,\n",
    "                          epochs=50,\n",
    "                          batch_size=64,\n",
    "                          validation_data=(X_test, X_test),\n",
    "                          shuffle=True)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "# ae anomaly detection threshold\n",
    "# predict anomalies/outliers in the training dataset\n",
    "prediction = autoencoder.predict(X_test)\n",
    "# the mae between actual and reconstruction/prediction\n",
    "prediction_loss = tf.keras.losses.mae(prediction, X_test)\n",
    "# prediction loss threshold for 2% of outliers\n",
    "loss_threshold = np.percentile(prediction_loss, 98)\n",
    "print(f'The prediction loss threshold for 2% of outliers is\\\n",
    "    {loss_threshold:.2f}')\n",
    "# visualize the threshold\n",
    "sns.histplot(prediction_loss, bins=30, alpha=0.8)\n",
    "plt.axvline(x=loss_threshold, color='orange')\n",
    "threshold_prediction = [\n",
    "    0 if i < loss_threshold else 1 for i in prediction_loss]\n",
    "# prediction performance\n",
    "print(classification_report(y_test, threshold_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f423b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "z_dim = 100\n",
    "adam = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "g = Sequential()\n",
    "g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(784, activation='sigmoid'))  # Values between 0 and 1\n",
    "g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "d = Sequential()\n",
    "d.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(1, activation='sigmoid'))  # Values between 0 and 1\n",
    "d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "d.trainable = False\n",
    "inputs = Input(shape=(z_dim, ))\n",
    "hidden = g(inputs)\n",
    "output = d(hidden)\n",
    "gan = Model(inputs, output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def plot_loss(losses):\n",
    "    if True:\n",
    "        d_loss = [v[0] for v in losses[\"D\"]]\n",
    "        g_loss = [v[0] for v in losses[\"G\"]]\n",
    "    if False:\n",
    "        d_acc = [v[1] for v in losses[\"D\"]]\n",
    "        g_acc = [v[1] for v in losses[\"G\"]]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if True:\n",
    "        plt.plot(d_loss, label=\"Discriminator loss\")\n",
    "        plt.plot(g_loss, label=\"Generator loss\")\n",
    "    if False:\n",
    "        plt.plot(d_acc, label=\"Discriminator accuracy\")\n",
    "        plt.plot(g_acc, label=\"Generator accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_generated(n_ex=10, dim=(1, 10), figsize=(12, 2)):\n",
    "    noise = np.random.normal(0, 1, size=(n_ex, z_dim))\n",
    "    generated_images = g.predict(noise)\n",
    "    generated_images = generated_images.reshape(n_ex, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "losses = {\"D\": [], \"G\": []}\n",
    "\n",
    "\n",
    "def train(epochs=1, plt_frq=1, BATCH_SIZE=128):\n",
    "    batchCount = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', BATCH_SIZE)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "\n",
    "    for e in tqdm_notebook(range(1, epochs+1)):\n",
    "        if e == 1 or e % plt_frq == 0:\n",
    "            print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        # tqdm_notebook(range(batchCount), leave=False):\n",
    "        for _ in range(batchCount):\n",
    "            # Create a batch by drawing random index numbers from the training set\n",
    "            image_batch = X_train[np.random.randint(\n",
    "                0, X_train.shape[0], size=BATCH_SIZE)]\n",
    "            # Create noise vectors for the generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "\n",
    "            # Generate the images from the noise\n",
    "            generated_images = g.predict(noise)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            # Create labels\n",
    "            y = np.zeros(2*BATCH_SIZE)\n",
    "            y[:BATCH_SIZE] = 0.9  # One-sided label smoothing\n",
    "\n",
    "            # Train discriminator on generated images\n",
    "            d.trainable = True\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            y2 = np.ones(BATCH_SIZE)\n",
    "            d.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y2)\n",
    "\n",
    "        # Only store losses from final batch of epoch\n",
    "        losses[\"D\"].append(d_loss)\n",
    "        losses[\"G\"].append(g_loss)\n",
    "\n",
    "        # Update the plots\n",
    "        if e == 1 or e % plt_frq == 0:\n",
    "            plot_generated()\n",
    "    plot_loss(losses)\n",
    "\n",
    "\n",
    "train(epochs=200, plt_frq=20, BATCH_SIZE=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f902dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=4, n_classes=2, random_state=1)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu',\n",
    "          kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "for _ in range(3):\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy')\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=32, verbose=1, validation_split=0.3)\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=4, n_classes=2, random_state=1)\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "row = [1.91518414, 1.14995454, -1.52847073, 0.79430654]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=784))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(x_train_noisy, x_train, epochs=100, batch_size=256,\n",
    "          shuffle=True, validation_data=(x_test_noisy, x_test))\n",
    "decoded_imgs = model.predict(x_test)\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(1, n+1):\n",
    "    # display original\n",
    "    ax = plt.subplot(3, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display noisy\n",
    "    ax = plt.subplot(3, n, i + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(3, n, i + 2*n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844958c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "#     noising\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "#     adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "#     adapt this if using `channels_first` image data format\n",
    "noise_factor = 0.9\n",
    "x_train_noisy = x_train + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "#     make a model\n",
    "#     adapt this if using `channels_first` image data format\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "#     at this point the representation is (7, 7, 32)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "#     this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "#     this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "#     compile\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "#     train\n",
    "history = autoencoder.fit(x_train_noisy, x_train, epochs=300, batch_size=256, shuffle=True, validation_data=(\n",
    "    x_test, x_test))\n",
    "#     test\n",
    "#     encode and decode some digits\n",
    "#     note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "print(encoded_imgs.shape)\n",
    "print('z: ' + str(encoded_imgs))\n",
    "#     structure of model\n",
    "plot_model(autoencoder, show_shapes=True, to_file='autoencoder.png')\n",
    "#     visualize\n",
    "n = 10\n",
    "#     how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    #     display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "#     display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(f\"The maximum pixel value: {X_train[0].max()}\")\n",
    "print(f\"The minimum pixel value: {X_train[0].min()}\")\n",
    "# print(X_train[0])\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255.\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(f\"The maximum pixel value: {X_train[0].max()}\")\n",
    "print(f\"The minimum pixel value: {X_train[0].min()}\")\n",
    "noise_factor = .8\n",
    "X_train_noisy = X_train + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "x_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "n = 10000\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "ax[0].imshow(X_train[n], cmap='gray')\n",
    "ax[1].imshow(x_train_noisy[n], cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].set_title(\"Noisy Image\")\n",
    "plt.show()\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Define the input tensor\n",
    "inputs = Input(input_shape)\n",
    "\n",
    "# Define the encoder part of the network\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "# Define the decoder part of the network\n",
    "up4 = UpSampling2D((2, 2))(conv3)\n",
    "up4 = Conv2D(64, (2, 2), activation='relu', padding='same')(up4)\n",
    "merge4 = concatenate([conv2, up4], axis=3)\n",
    "\n",
    "conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge4)\n",
    "conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "up5 = UpSampling2D((2, 2))(conv4)\n",
    "up5 = Conv2D(32, (2, 2), activation='relu', padding='same')(up5)\n",
    "merge5 = concatenate([conv1, up5], axis=3)\n",
    "\n",
    "conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(merge5)\n",
    "conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "# Define the output layer of the network\n",
    "output = Conv2D(1, (1, 1), activation='sigmoid')(conv5)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()\n",
    "history = model.fit(X_train_noisy, X_train, epochs=100,\n",
    "                    verbose=0, batch_size=128, shuffle=True)\n",
    "noise_factor = .5\n",
    "X_test_noisy_50 = X_test + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "X_test_noisy_50 = np.clip(X_test_noisy_50, 0., 1.)\n",
    "denoised_image = model.predict(X_test_noisy_50)\n",
    "model.evaluate(X_test_noisy_50, X_test)\n",
    "\n",
    "for i in range(2):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n",
    "    ax[0].imshow(X_test[i], cmap='gray')\n",
    "    ax[1].imshow(X_test_noisy_50[i], cmap='gray')\n",
    "    ax[2].imshow(denoised_image[i], cmap='gray')\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].set_title(\"Noised Image\")\n",
    "    ax[2].set_title(\"denoised Image\")\n",
    "    plt.show()\n",
    "noise_factor = .65\n",
    "X_test_noisy_65 = X_test + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "X_test_noisy_65 = np.clip(X_test_noisy_65, 0., 1.)\n",
    "denoised_image = model.predict(X_test_noisy_65)\n",
    "model.evaluate(X_test_noisy_65, X_test)\n",
    "\n",
    "for i in range(2):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n",
    "    ax[0].imshow(X_test[i], cmap='gray')\n",
    "    ax[1].imshow(X_test_noisy_65[i], cmap='gray')\n",
    "    ax[2].imshow(denoised_image[i], cmap='gray')\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].set_title(\"Noised Image\")\n",
    "    ax[2].set_title(\"denoised Image\")\n",
    "    plt.show()\n",
    "noise_factor = .8\n",
    "X_test_noisy_80 = X_test + noise_factor * \\\n",
    "    np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "X_test_noisy_80 = np.clip(X_test_noisy_80, 0., 1.)\n",
    "denoised_image = model.predict(X_test_noisy_80)\n",
    "model.evaluate(X_test_noisy_80, X_test)\n",
    "\n",
    "for i in range(2):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n",
    "    ax[0].imshow(X_test[i], cmap='gray')\n",
    "    ax[1].imshow(X_test_noisy_80[i], cmap='gray')\n",
    "    ax[2].imshow(denoised_image[i], cmap='gray')\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].set_title(\"Noised Image\")\n",
    "    ax[2].set_title(\"denoised Image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a76942",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4069f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ms(seq):\n",
    "    ndim = len(seq)\n",
    "    ndim = int(np.sqrt(ndim))\n",
    "    m = np.zeros((ndim, ndim))\n",
    "    k = 0\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            m[i, j] = seq[k]\n",
    "            k = k+1\n",
    "    tmp = (ndim*ndim*ndim+ndim)/2.\n",
    "    score = 0.\n",
    "    tmq = 0.\n",
    "    tmr = 0.\n",
    "    for i in range(ndim):\n",
    "        tmq = tmq+m[i, i]\n",
    "        tmr = tmr+m[ndim-i-1, i]\n",
    "    score = score+(tmq-tmp)**2\n",
    "    score = score+(tmr-tmp)**2\n",
    "    for i in range(ndim):\n",
    "        score = score+(sum(m[i, :])-tmp)**2\n",
    "        score = score+(sum(m[:, i])-tmp)**2\n",
    "    return score\n",
    "\n",
    "\n",
    "def objective_nq(seq):\n",
    "    ndim = len(seq)\n",
    "    score = (ndim*ndim-ndim)/2.\n",
    "    for row in range(ndim):\n",
    "        col = seq[row]\n",
    "        for other_row in range(ndim):\n",
    "            # queens cannot pair with itself\n",
    "            if other_row == row:\n",
    "                continue\n",
    "            if seq[other_row] == col:\n",
    "                continue\n",
    "            if other_row + seq[other_row] == row + col:\n",
    "                continue\n",
    "            if other_row - seq[other_row] == row - col:\n",
    "                continue\n",
    "            # score++ if every pair of queens are non-attacking.\n",
    "            score -= 0.5\n",
    "    # divide by 2 as pairs of queens are commutative\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(start=0, stop=10, num=1000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"True generative process\")\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "\n",
    "\n",
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "gaussian_process.kernel_\n",
    "\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "\n",
    "noise_std = 0.75\n",
    "y_train_noisy = y_train + \\\n",
    "    rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)\n",
    "\n",
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n",
    ")\n",
    "gaussian_process.fit(X_train, y_train_noisy)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.errorbar(\n",
    "    X_train,\n",
    "    y_train_noisy,\n",
    "    noise_std,\n",
    "    linestyle=\"None\",\n",
    "    color=\"tab:blue\",\n",
    "    marker=\".\",\n",
    "    markersize=10,\n",
    "    label=\"Observations\",\n",
    ")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    color=\"tab:orange\",\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on a noisy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ba9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# train/test 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Gaussian Naive Bayes 분류 모델 생성 및 학습\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 데이터에 대한 예측 결과 출력\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt.plots import plot_gaussian_process\n",
    "\n",
    "np.random.seed(237)\n",
    "noise_level = 0.1\n",
    "\n",
    "def f(x, noise_level=noise_level):\n",
    "    return np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2)) + np.random.randn() * noise_level\n",
    "\n",
    "# Plot f(x) + contours\n",
    "x = np.linspace(-2, 2, 400).reshape(-1, 1)\n",
    "fx = [f(x_i, noise_level=0.0) for x_i in x]\n",
    "plt.plot(x, fx, \"r--\", label=\"True (unknown)\")\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate(([fx_i - 1.9600 * noise_level for fx_i in fx],\n",
    "                         [fx_i + 1.9600 * noise_level for fx_i in fx[::-1]])),\n",
    "         alpha=.2, fc=\"r\", ec=\"None\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "from skopt import gp_minimize\n",
    "\n",
    "res = gp_minimize(f,                  # the function to minimize\n",
    "                  [(-2.0, 2.0)],      # the bounds on each dimension of x\n",
    "                  acq_func=\"EI\",      # the acquisition function\n",
    "                  n_calls=15,         # the number of evaluations of f\n",
    "                  n_random_starts=5,  # the number of random initialization points\n",
    "                  noise=0.1**2,       # the noise level (optional)\n",
    "                  random_state=1234)   # the random seed\n",
    "\n",
    "\"x^*=%.4f, f(x^*)=%.4f\" % (res.x[0], res.fun)\n",
    "\n",
    "print(res)\n",
    "\n",
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "x = np.arange(10, 20)\n",
    "y = np.array([2, 1, 4, 5, 9, 12, 18, 25, 96, 48])\n",
    "print(x)\n",
    "print(y)\n",
    "scipy.stats.pearsonr(x, y)[0]    # Pearson's r\n",
    "\n",
    "scipy.stats.spearmanr(x, y)[0]   # Spearman's rho\n",
    "\n",
    "scipy.stats.kendalltau(x, y)[0]  # Kendall's tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "if True:\n",
    "    N = 2\n",
    "    M = int(1e7)\n",
    "    y = np.random.uniform(low=-0.5, high=0.5, size=(M, N))\n",
    "    p = np.sum(np.sqrt(y[:, 0]**2 + y[:, 1]**2) < 0.5)/M\n",
    "    print(p, 4*p)\n",
    "\n",
    "N_MAX = 16\n",
    "M = int(1e7)\n",
    "dims = np.zeros(N_MAX, dtype=np.int32)\n",
    "volume = np.zeros(N_MAX)\n",
    "for N in range(1, N_MAX+1):\n",
    "    y = np.random.uniform(low=-0.5, high=0.5, size=(M, N))\n",
    "    # 거리 계산에 해당하는 부분\n",
    "    dist = cdist(y, np.expand_dims(np.zeros(N), 0), metric='euclidean')\n",
    "    p = np.sum(dist < 0.5)/M\n",
    "    dims[N-1] = N\n",
    "    volume[N-1] = p\n",
    "df = pd.DataFrame(data={'dims': dims, 'volume': volume})\n",
    "print(df)\n",
    "plt.plot(df.dims, df.volume, 'o-')\n",
    "plt.title('Volume of n-ball inscribed in unit n-cube', size=18)\n",
    "plt.xlabel('Dimensions', size=18)\n",
    "plt.ylabel('Volume of inscribed n-ball', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 두 데이터 포인트 간의 유클리드 거리 계산\n",
    "\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "dimensions = 5\n",
    "num_points = 100000\n",
    "data = np.random.uniform(0, 1, size=(num_points, dimensions))\n",
    "# 첫번째 데이터 포인트와 모든 데이터 포인트 간의 거리 계산\n",
    "distances = [euclidean_distance(data[0], data[i])\n",
    "             for i in range(1, num_points)]\n",
    "# 거리의 평균과 표준편차 출력\n",
    "print(\"Mean distance:\", np.mean(distances))\n",
    "print(\"Standard deviation of distance:\", np.std(distances))\n",
    "\n",
    "dimensions = 5000\n",
    "data = np.random.uniform(0, 1, size=(num_points, dimensions))\n",
    "# 첫번째 데이터 포인트와 모든 데이터 포인트 간의 거리 계산\n",
    "distances = [euclidean_distance(data[0], data[i])\n",
    "             for i in range(1, num_points)]\n",
    "# 거리의 평균과 표준편차 출력\n",
    "print(\"Mean distance:\", np.mean(distances))\n",
    "print(\"Standard deviation of distance:\", np.std(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# 데이터 생성\n",
    "X, y = make_classification(n_samples=20000, n_features=2000, n_classes=2)\n",
    "if True:\n",
    "    X, y = make_classification(\n",
    "        n_samples=20000, n_features=2000, n_informative=2, n_redundant=5, random_state=17)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "icase = 1\n",
    "if icase == 1:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    score_high_dimension = knn.score(X_test, y_test)\n",
    "\n",
    "# PCA를 사용하여 차원을 줄인 후 KNN 분류기로 예측 모델 학습\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "if icase == 1:\n",
    "    knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_pca.fit(X_train_pca, y_train)\n",
    "    score_low_dimension = knn_pca.score(X_test_pca, y_test)\n",
    "\n",
    "print(f\"high dim data prediction accuracy: {score_high_dimension:.2f}\")\n",
    "print(f\"low dim data prediction accuracy: {score_low_dimension:.2f}\")\n",
    "\n",
    "if icase == 2:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    score_high_dimsion = model.score(X_test, y_test)\n",
    "    model_pca = LogisticRegression()\n",
    "    model_pca.fit(X_train_pca, y_train)\n",
    "    score_low_dimension = model_pca.score(X_test_pca, y_test)\n",
    "\n",
    "# 두 모델의 정확도 비교\n",
    "print(f\"high dim data prediction accuracy: {score_high_dimension:.2f}\")\n",
    "print(f\"low dim data prediction accuracy: {score_low_dimension:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# 1000차원 벡터를 10,000개 생성\n",
    "n_samples = 10000\n",
    "n_features = 1000\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# PCA 차원 축소\n",
    "pca_100 = PCA(n_components=100)\n",
    "X_pca_100 = pca_100.fit_transform(X)\n",
    "\n",
    "pca_50 = PCA(n_components=50)\n",
    "X_pca_50 = pca_50.fit_transform(X)\n",
    "\n",
    "pca_10 = PCA(n_components=10)\n",
    "X_pca_10 = pca_10.fit_transform(X)\n",
    "# 각각의 축소된 차원에서 데이터 간 거리 계산\n",
    "\n",
    "distance_original = euclidean_distances(X)\n",
    "distance_pca_100 = euclidean_distances(X_pca_100)\n",
    "distance_pca_50 = euclidean_distances(X_pca_50)\n",
    "distance_pca_10 = euclidean_distances(X_pca_10)\n",
    "# 원래 고차원에서의 거리와 각각의 축소된 차원에서의 거리 비교\n",
    "print(\"Distance in original space: \", np.mean(distance_original))\n",
    "print(\"Distance in 100-dimensional space: \", np.mean(distance_pca_100))\n",
    "print(\"Distance in 50-dimensional space: \", np.mean(distance_pca_50))\n",
    "print(\"Distance in 10-dimensional space: \", np.mean(distance_pca_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9279551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "\n",
    "def mahalanobis_distance(p, distr):\n",
    "    # p: a point\n",
    "    # distr : a distribution\n",
    "    # covariance matrix\n",
    "    cov = np.cov(distr, rowvar=False)\n",
    "    # average of the points in distr\n",
    "    avg_distri = np.average(distr, axis=0)\n",
    "    dis = mahalanobis(p, avg_distri, cov)\n",
    "    return dis\n",
    "\n",
    "\n",
    "X = np.array([[1, 2], [2, 2], [3, 3], [1, 3], [2, 3]])\n",
    "cov = np.cov(X, rowvar=False)\n",
    "covI = np.linalg.inv(cov)\n",
    "mean = np.mean(X)\n",
    "maha = mahalanobis(X[0], X[1], covI)\n",
    "pca = PCA(whiten=True)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "print('Mahalanobis distance: '+str(maha))\n",
    "print('Euclidean distance: ' +\n",
    "      str(euclidean(X_transformed[0], X_transformed[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.colorbar()\n",
    "\n",
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "\n",
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks': [], 'yticks': []},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8), cmap='binary',\n",
    "                  interpolation='nearest', clim=(0, 16))\n",
    "\n",
    "\n",
    "plot_digits(digits.data)\n",
    "\n",
    "np.random.seed(17)\n",
    "noisy = np.random.normal(digits.data, 2.)\n",
    "plot_digits(noisy)\n",
    "\n",
    "pca = PCA(0.50).fit(noisy)\n",
    "pca.n_components_\n",
    "\n",
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ac2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "img1 = img.imread(\n",
    "    'C:/Users/Inho Lee/testAI/data/denoising2/2021.08.09/line/Line 50x_001.tif')\n",
    "# img1=img1[0:target_height,0:target_width]\n",
    "plt.imshow(img1, cmap=plt.cm.gray)\n",
    "\n",
    "sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "path = \"EDSR_x4.pb\"\n",
    "sr.readModel(path)\n",
    "sr.setModel(\"edsr\",4)\n",
    "result = sr.upsample(img1)\n",
    "# Resized image\n",
    "resized = cv2.resize(img1,dsize=None,fx=4,fy=4)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(1,3,1)\n",
    "# Original image\n",
    "plt.imshow(img1[:,:,::-1])\n",
    "plt.subplot(1,3,2)\n",
    "# SR upscaled\n",
    "plt.imshow(result[:,:,::-1])\n",
    "plt.subplot(1,3,3)\n",
    "# OpenCV upscaled\n",
    "plt.imshow(resized[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e539ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('input.png')\n",
    "width = img1.shape[1]\n",
    "height = img1.shape[0]\n",
    "bicubic = cv2.resize(img1, (width*4, height*4))\n",
    "\n",
    "super_res = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "start = time.time()\n",
    "super_res.readModel('EDSR_x4.pb')\n",
    "super_res.setModel('edsr', 4)\n",
    "edsr_image = super_res.upsample(img1)\n",
    "end = time.time()\n",
    "print('Time taken in seconds by edsr', end-start)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(1,3,1)\n",
    "# Original image\n",
    "plt.imshow(img1[:,:,::-1])\n",
    "plt.subplot(1,3,2)\n",
    "# SR upscaled\n",
    "plt.imshow(edsr_image[:,:,::-1])\n",
    "plt.subplot(1,3,3)\n",
    "# OpenCV upscaled\n",
    "plt.imshow(bicubic[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6540f1b",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bfgs algorithm local optimization of a convex function\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import rand\n",
    "# objective function\n",
    "\n",
    "\n",
    "def objective(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    "# derivative of the objective function\n",
    "\n",
    "\n",
    "def derivative(x):\n",
    "    return [x[0] * 2, x[1] * 2]\n",
    "\n",
    "\n",
    "# define range for input\n",
    "r_min, r_max = -5.0, 5.0\n",
    "# define the starting point as a random sample from the domain\n",
    "pt = r_min + rand(2) * (r_max - r_min)\n",
    "# perform the bfgs algorithm search\n",
    "result = minimize(objective, pt, method='BFGS', jac=derivative)\n",
    "# summarize the result\n",
    "print('Status : %s' % result['message'])\n",
    "print('Total Evaluations: %d' % result['nfev'])\n",
    "# evaluate solution\n",
    "solution = result['x']\n",
    "evaluation = objective(solution)\n",
    "print('Solution: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a82f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def objective(x, y):\n",
    "    return x**2.0 + y**2.0\n",
    "\n",
    "def derivative(x, y):\n",
    "    return np.asarray([x * 2.0, y * 2.0])\n",
    "\n",
    "def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):\n",
    "    x = bounds[:, 0] + np.random.rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    score = objective(x[0], x[1])\n",
    "    m = [0.0 for _ in range(bounds.shape[0])]\n",
    "    v = [0.0 for _ in range(bounds.shape[0])]\n",
    "    # run the gradient descent updates\n",
    "    for t in range(n_iter):\n",
    "        # calculate gradient g(t)\n",
    "        g = derivative(x[0], x[1])\n",
    "        # build a solution one variable at a time\n",
    "        for i in range(x.shape[0]):\n",
    "            # m(t) = beta1 * m(t-1) + (1 - beta1) * g(t)\n",
    "            m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\n",
    "            # v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2\n",
    "            v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\n",
    "            # mhat(t) = m(t) / (1 - beta1(t))\n",
    "            mhat = m[i] / (1.0 - beta1**(t+1))\n",
    "            # vhat(t) = v(t) / (1 - beta2(t))\n",
    "            vhat = v[i] / (1.0 - beta2**(t+1))\n",
    "            # x(t) = x(t-1) - alpha * mhat(t) / (sqrt(vhat(t)) + eps)\n",
    "            x[i] = x[i] - alpha * mhat / (np.sqrt(vhat) + eps)\n",
    "        # evaluate candidate point\n",
    "        score = objective(x[0], x[1])\n",
    "        # report progress\n",
    "        print('>%d f(%s) = %.5f' % (t, x, score))\n",
    "    return [x, score]\n",
    "\n",
    "\n",
    "# seed the pseudo random number generator\n",
    "np.random.seed(17)\n",
    "# define range for input\n",
    "bounds = np.asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 60\n",
    "# steps size\n",
    "alpha = 0.02\n",
    "# factor for average gradient\n",
    "beta1 = 0.8\n",
    "# factor for average squared gradient\n",
    "beta2 = 0.999\n",
    "# perform the gradient descent search with adam\n",
    "best, score = adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d096c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import scipy.optimize as optimize\n",
    "from scipy.optimize import dual_annealing\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "def append_multiple_lines(file_name, lines_to_append):\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        appendEOL = False\n",
    "        file_object.seek(0)\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            appendEOL = True\n",
    "        for line in lines_to_append:\n",
    "            if appendEOL == True:\n",
    "                file_object.write(\"\\n\")\n",
    "            else:\n",
    "                appendEOL = True\n",
    "            file_object.write(line)\n",
    "\n",
    "\n",
    "def eggholder(x):\n",
    "    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1] + 47))))\n",
    "            - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47)))))\n",
    "\n",
    "\n",
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "\n",
    "if False:\n",
    "    optimize.show_options(solver='minimize', method='nelder-mead')\n",
    "if False:\n",
    "    x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "    ndim = len(x0)\n",
    "    bnds = []\n",
    "    for _ in range(ndim):\n",
    "        bnds.append((-512., 512.))\n",
    "if True:\n",
    "    fname = 'input.txt'\n",
    "    if not os.path.isfile(fname):\n",
    "        print('input.txt is not present')\n",
    "        sys.exit()\n",
    "    afile = open(fname, 'r')\n",
    "    jline = 0\n",
    "    for line in afile:\n",
    "        if jline == 0:\n",
    "            ndim = int(line.split()[0])\n",
    "            x0 = np.zeros(ndim)\n",
    "        if jline > 0:\n",
    "            if jline-1 < ndim:\n",
    "                x0[jline-1] = float(line.split()[0])\n",
    "                print(x0[jline-1])\n",
    "        if jline == 1+ndim:\n",
    "            ncal = int(line.split()[0])\n",
    "        jline = jline+1\n",
    "    afile.close()\n",
    "    fname = 'bnds.txt'\n",
    "    if not os.path.isfile(fname):\n",
    "        print('bnds.txt is not present')\n",
    "        sys.exit()\n",
    "    afile = open(fname, 'r')\n",
    "    jline = 0\n",
    "    for line in afile:\n",
    "        if jline == 0:\n",
    "            bnds = []\n",
    "        if jline > 0:\n",
    "            if jline-1 < ndim:\n",
    "                print((float(line.split()[0]), float(line.split()[1])))\n",
    "                bnds.append((float(line.split()[0]), float(line.split()[1])))\n",
    "        jline = jline+1\n",
    "    afile.close()\n",
    "    bnds = np.array(bnds)\n",
    "if True:\n",
    "    res = minimize(rosen, x0, method='nelder-mead', bounds=bnds,\n",
    "                   options={'xatol': 1e-8, 'disp': True})\n",
    "if False:\n",
    "    res = dual_annealing(rosen, x0=x0, bounds=bnds)\n",
    "if False:\n",
    "    res = differential_evolution(rosen, bounds=bnds, maxiter=10)\n",
    "print(res.x)\n",
    "print(res.fun)\n",
    "\n",
    "if False:\n",
    "    lines_to_append = []\n",
    "    lines_to_append.append(str(ndim))\n",
    "    for i in range(ndim):\n",
    "        lines_to_append.append(str(res.x[i]))\n",
    "    lines_to_append.append(str(res.fun))\n",
    "    lines_to_append.append(str(ncal))\n",
    "    fname = 'output.txt'\n",
    "    if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "    append_multiple_lines(fname, lines_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.optimize import minimize\n",
    "from scipy.optimize import fmin\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "N = 100  # for testing\n",
    "step = N//size  # say that N is divisible by size\n",
    "\n",
    "\n",
    "def parallel_function_caller(x, stopp):\n",
    "    stopp[0] = comm.bcast(stopp[0], root=0)\n",
    "    summ = 0\n",
    "    if stopp[0] == 0:\n",
    "        # your function here in parallel\n",
    "        x = comm.bcast(x, root=0)\n",
    "        array = np.arange(x[0]-N/2.+rank*step-42, x[0] -\n",
    "                        N/2.+(rank+1)*step-42, 1.)\n",
    "        summl = np.sum(np.square(array))\n",
    "        summ = comm.reduce(summl, op=MPI.SUM, root=0)\n",
    "        if rank == 0:\n",
    "            print( \"value is \"+str(summ))\n",
    "    return summ\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "    stop = [0]\n",
    "    x = np.zeros(1)\n",
    "    x[0] = 20\n",
    "    #xs = minimize(parallel_function_caller, x, args=(stop))\n",
    "    xs = fmin(parallel_function_caller, x0=x, args=(stop,))\n",
    "    print(\"the argmin is \"+str(xs))\n",
    "    stop = [1]\n",
    "    parallel_function_caller(x, stop)\n",
    "\n",
    "else:\n",
    "    stop = [0]\n",
    "    x = np.zeros(1)\n",
    "    while stop[0] == 0:\n",
    "        parallel_function_caller(x, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61444aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import scipy.optimize as optimize\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "\n",
    "def append_multiple_lines(file_name, lines_to_append):\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        appendEOL = False\n",
    "        file_object.seek(0)\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            appendEOL = True\n",
    "        for line in lines_to_append:\n",
    "            if appendEOL == True:\n",
    "                file_object.write(\"\\n\")\n",
    "            else:\n",
    "                appendEOL = True\n",
    "            file_object.write(line)\n",
    "\n",
    "\n",
    "def eggholder(x):\n",
    "    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1] + 47))))\n",
    "            - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47)))))\n",
    "\n",
    "\n",
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "\n",
    "if False:\n",
    "    optimize.show_options(solver='minimize', method='nelder-mead')\n",
    "if True:\n",
    "    x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "    ndim = len(x0)\n",
    "    bnds = []\n",
    "    for _ in range(ndim):\n",
    "        bnds.append((-512., 512.))\n",
    "if False:\n",
    "    fname = 'input.txt'\n",
    "    if not os.path.isfile(fname):\n",
    "        print('input.txt is not present')\n",
    "        sys.exit()\n",
    "    afile = open(fname, 'r')\n",
    "    jline = 0\n",
    "    for line in afile:\n",
    "        if jline == 0:\n",
    "            ndim = int(line.split()[0])\n",
    "            x0 = np.zeros(ndim)\n",
    "        if jline > 0:\n",
    "            if jline-1 < ndim:\n",
    "                x0[jline-1] = float(line.split()[0])\n",
    "                print(x0[jline-1])\n",
    "        if jline == 1+ndim:\n",
    "            ncal = int(line.split()[0])\n",
    "        jline = jline+1\n",
    "    afile.close()\n",
    "    fname = 'bnds.txt'\n",
    "    if not os.path.isfile(fname):\n",
    "        print('bnds.txt is not present')\n",
    "        sys.exit()\n",
    "    afile = open(fname, 'r')\n",
    "    jline = 0\n",
    "    for line in afile:\n",
    "        if jline == 0:\n",
    "            bnds = []\n",
    "        if jline > 0:\n",
    "            if jline-1 < ndim:\n",
    "                print((float(line.split()[0]), float(line.split()[1])))\n",
    "                bnds.append((float(line.split()[0]), float(line.split()[1])))\n",
    "        jline = jline+1\n",
    "    afile.close()\n",
    "    bnds = np.array(bnds)\n",
    "if True:\n",
    "    res = minimize(rosen, x0, method='nelder-mead', bounds=bnds,\n",
    "                   options={'xatol': 1e-8, 'disp': True})\n",
    "if False:\n",
    "    res = dual_annealing(rosen, x0=x0, bounds=bnds)\n",
    "print(res.x)\n",
    "print(res.fun)\n",
    "\n",
    "if False:\n",
    "    lines_to_append = []\n",
    "    lines_to_append.append(str(ndim))\n",
    "    for i in range(ndim):\n",
    "        lines_to_append.append(str(res.x[i]))\n",
    "    lines_to_append.append(str(res.fun))\n",
    "    lines_to_append.append(str(ncal))\n",
    "    fname = 'output.txt'\n",
    "    if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "    append_multiple_lines(fname, lines_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1de5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import minimize, NonlinearConstraint, SR1\n",
    "\n",
    "def f(x):\n",
    "    return math.log(x[0]**2 + 1) + x[1]**4 + x[0]*x[2]\n",
    "\n",
    "constr_func = lambda x: np.array( [ x[0]**3 - x[1]**2 - 1, x[0], x[2] ] )\n",
    "\n",
    "x0=[0.,0.,0.]\n",
    "nonlin_con = NonlinearConstraint( constr_func, 0., np.inf )\n",
    "res = minimize( f, x0, method='trust-constr', jac='2-point', hess=SR1(), constraints = nonlin_con )\n",
    "print( res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "a1, a2, a3 = 1167,1327,1907\n",
    "b1,b2,b3 = 24000, 34400, 36000\n",
    "c1,c2,c3 = 69500,15100,12700\n",
    "x = [10000,10000,10000] \n",
    "res = minimize(\n",
    "    lambda x: c1*x[0]+c2*x[1]+c3*x[2], #what we want to minimize\n",
    "    x, \n",
    "    constraints = (\n",
    "        {'type':'eq','fun': lambda x: x[0]*a1-x[1]*a2}, #1st subject\n",
    "        {'type':'ineq','fun': lambda x: a1*x[0]+a2*x[1]+a3*x[2]-7}, #2st subject\n",
    "        {'type':'ineq','fun': lambda x: b1*x[0]+b2*x[1]+b3*x[2]-0}, #3st subject\n",
    "        {'type':'eq','fun': lambda x: x[0]%5+x[1]%5+x[2]%5-0}, # x1 x2 x3 are multiple of 5\n",
    "\n",
    "                  ),\n",
    "    bounds = ((0,None),(0,None),(0,None)),\n",
    "    method='SLSQP',options={'disp': True,'maxiter' : 10000})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "# declare the decision variable bounds\n",
    "x1_bounds = (0, None)\n",
    "x2_bounds = (0, None)\n",
    "# declare coefficients of the objective function\n",
    "c = [-10, -5]\n",
    "# declare the inequality constraint matrix\n",
    "A = [[1,  1],\n",
    "     [10, 0],\n",
    "     [0,  5]]\n",
    "# declare the inequality constraint vector\n",
    "b = [24, 100, 100]\n",
    "# solve\n",
    "results = linprog(c=c, A_ub=A, b_ub=b, bounds=[\n",
    "                  x1_bounds, x2_bounds], method='highs-ds')\n",
    "# print results\n",
    "if results.status == 0:\n",
    "    print(f'The solution is optimal.')\n",
    "print(f'Objective value: z* = {results.fun}')\n",
    "print(f'Solution: x1* = {results.x[0]}, x2* = {results.x[1]}')\n",
    "\n",
    "\n",
    "def objective(xvector):\n",
    "    x = xvector[0]\n",
    "    y = xvector[1]\n",
    "    tmp = 5.*x+3.*y\n",
    "    if x+2.*y > 14.:\n",
    "        tmp = tmp+1.e8*(x+2.*y-14.)\n",
    "    if 3.*x-y < 0.:\n",
    "        tmp = tmp+1.e8*(3.*x-y)*(-1.)\n",
    "    if x-y-2. > 0.:\n",
    "        tmp = tmp+1.e8*(x-y-2.)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def objective1(xvector):\n",
    "    x = xvector[0]\n",
    "    y = xvector[1]\n",
    "    tmp = -5.*x-3.*y\n",
    "    if x+2.*y > 14.:\n",
    "        tmp = tmp+1.e8*(x+2.*y-14.)\n",
    "    if 3.*x-y < 0.:\n",
    "        tmp = tmp+1.e8*(3.*x-y)*(-1.)\n",
    "    if x-y-2. > 0.:\n",
    "        tmp = tmp+1.e8*(x-y-2.)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "best = 1e99\n",
    "xbest = np.zeros((2))\n",
    "for _ in range(30):\n",
    "    bnds = [(-4., 8.) for i in range(2)]\n",
    "    xvector = np.zeros((2))\n",
    "    xvector[0] = -1.+(np.random.random()-0.5)*3.\n",
    "    xvector[1] = -3.+(np.random.random()-0.5)*3.\n",
    "    res = minimize(objective, xvector, method='Nelder-Mead', bounds=bnds,\n",
    "                   options={'maxiter': 6000, 'maxfev': 9000, 'xtol': 1e-8, 'disp': False})\n",
    "    xvector = res.x\n",
    "    obj = res.fun\n",
    "    print(obj)\n",
    "    if best > obj:\n",
    "        best = obj\n",
    "        xbest = xvector\n",
    "obj = best\n",
    "xvector = xbest\n",
    "print(xvector)\n",
    "print(obj)\n",
    "\n",
    "best = 1e99\n",
    "xbest = np.zeros((2))\n",
    "for _ in range(30):\n",
    "    bnds = [(-4., 8.) for i in range(2)]\n",
    "    xvector = np.zeros((2))\n",
    "    xvector[0] = 6.+(np.random.random()-0.5)*3.\n",
    "    xvector[1] = 4.+(np.random.random()-0.5)*3.\n",
    "    res = minimize(objective1, xvector, method='Nelder-Mead', bounds=bnds,\n",
    "                   options={'maxiter': 6000, 'maxfev': 9000, 'xtol': 1e-8, 'disp': False})\n",
    "    xvector = res.x\n",
    "    obj = res.fun\n",
    "    print(obj)\n",
    "    if best > obj:\n",
    "        best = obj\n",
    "        xbest = xvector\n",
    "obj = best\n",
    "xvector = xbest\n",
    "print(xvector)\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c836d4b",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def functuser(x):\n",
    "    case = 3\n",
    "\n",
    "    if case == 1:\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j])**2\n",
    "    if case == 2:\n",
    "        #    Rastrigin\n",
    "        total = 10.*len(x)\n",
    "        for j in range(len(x)):\n",
    "            total += x[j]**2-10.*np.cos(2.*np.pi*x[j])\n",
    "    if case == 3:\n",
    "        #   Rosenbrock\n",
    "        xarray0 = np.zeros(len(x))\n",
    "        for j in range(len(x)):\n",
    "            xarray0[j] = x[j]\n",
    "        total = sum(100.0*(xarray0[1:]-xarray0[:-1]\n",
    "                    ** 2.0)**2.0 + (1-xarray0[:-1])**2.0)\n",
    "    if case == 4:\n",
    "        #   Styblinski-Tang\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j]**4-16.*x[j]**2+5.*x[j])/2.\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "class PARTICLE:\n",
    "    def __init__(self, startx0, tmprt, xbounds, lverbose):\n",
    "        self.position_i = []\n",
    "        self.qosition_i = []\n",
    "        self.position_best_i = []\n",
    "        self.obj_best_i = 1e18\n",
    "        self.obj_i = 1e18\n",
    "        self.dimensions = len(startx0)\n",
    "        self.tmprt = tmprt\n",
    "        if lverbose:\n",
    "            print(self.tmprt)\n",
    "        for j in range(self.dimensions):\n",
    "            self.position_i.append(\n",
    "                startx0[j]+(np.random.random()-0.5)*2.*np.sqrt(self.tmprt)*0.101)\n",
    "        if np.random.random() < 0.8:\n",
    "            for j in range(self.dimensions):\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        for j in range(self.dimensions):\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        self.position_best_i = self.position_i.copy()\n",
    "        self.qosition_i = self.position_i.copy()\n",
    "\n",
    "    def evaluate(self, objfunct, xbounds):\n",
    "        before = objfunct(self.position_i)\n",
    "        for _ in range(200):\n",
    "            for j in range(self.dimensions):\n",
    "                self.qosition_i[j] = self.position_i[j] + \\\n",
    "                    (np.random.random()-0.5)*2.*np.sqrt(self.tmprt)*0.101\n",
    "                if self.qosition_i[j] > xbounds[j][1]:\n",
    "                    self.qosition_i[j] = xbounds[j][0] + \\\n",
    "                        (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "                if self.qosition_i[j] < xbounds[j][0]:\n",
    "                    self.qosition_i[j] = xbounds[j][0] + \\\n",
    "                        (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            after = objfunct(self.qosition_i)\n",
    "            tmp = -(after-before)/self.tmprt\n",
    "            if tmp > 300.:\n",
    "                tmp = 300.\n",
    "            if tmp < -300.:\n",
    "                tmp = -300.\n",
    "            if min(1., np.exp(tmp)) > np.random.random():\n",
    "                before = after\n",
    "                self.obj_i = after\n",
    "                self.position_i = self.qosition_i.copy()\n",
    "            if self.obj_i < self.obj_best_i:\n",
    "                self.position_best_i = self.position_i.copy()\n",
    "                self.obj_best_i = self.obj_i\n",
    "        for _ in range(200):\n",
    "            for j in range(self.dimensions):\n",
    "                self.qosition_i[j] = self.position_i[j] + \\\n",
    "                    (np.random.random()-0.5)*2.*np.sqrt(self.tmprt)*0.101\n",
    "                if self.qosition_i[j] > xbounds[j][1]:\n",
    "                    self.qosition_i[j] = xbounds[j][0] + \\\n",
    "                        (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "                if self.qosition_i[j] < xbounds[j][0]:\n",
    "                    self.qosition_i[j] = xbounds[j][0] + \\\n",
    "                        (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            after = objfunct(self.qosition_i)\n",
    "            tmp = -(after-before)/self.tmprt\n",
    "            if tmp > 300.:\n",
    "                tmp = 300.\n",
    "            if tmp < -300.:\n",
    "                tmp = -300.\n",
    "            if min(1., np.exp(tmp)) > np.random.random():\n",
    "                before = after\n",
    "                self.obj_i = after\n",
    "                self.position_i = self.qosition_i.copy()\n",
    "            if self.obj_i < self.obj_best_i:\n",
    "                self.position_best_i = self.position_i.copy()\n",
    "                self.obj_best_i = self.obj_i\n",
    "\n",
    "\n",
    "class REMC():\n",
    "    def __init__(self, objfunct, startx0, xbounds, nparticles, maxiter, verbose=False):\n",
    "        obj_best_g = 1e18\n",
    "        position_best_g = []\n",
    "        swarm = []\n",
    "        tpset = []\n",
    "        x1vec = []\n",
    "        x2vec = []\n",
    "        for i in range(nparticles):\n",
    "            tmprt = 0.01+1.0*float(i)/float(nparticles-1)\n",
    "            tpset.append(tmprt)\n",
    "            swarm.append(PARTICLE(startx0, tmprt, xbounds, verbose))\n",
    "        it = 0\n",
    "        while it < maxiter:\n",
    "            if verbose:\n",
    "                print(f'iter: {it:>6d} best solution: {obj_best_g:16.8e}')\n",
    "            for i in range(nparticles):\n",
    "                swarm[i].evaluate(objfunct, xbounds)\n",
    "                if swarm[i].obj_i < obj_best_g:\n",
    "                    position_best_g = list(swarm[i].position_best_i)\n",
    "                    obj_best_g = float(swarm[i].obj_best_i)\n",
    "            lxcd = False\n",
    "            for i in range(nparticles-1, 0, -1):\n",
    "                if lxcd == True:\n",
    "                    lxcd = False\n",
    "                    continue\n",
    "                if lxcd == False:\n",
    "                    x1vec = list(swarm[i].position_i)\n",
    "                    x2vec = list(swarm[i-1].position_i)\n",
    "                    tmp = (1./tpset[i]-1./tpset[i-1]) * \\\n",
    "                        (swarm[i].obj_i-swarm[i-1].obj_i)\n",
    "                    if tmp > 300.:\n",
    "                        tmp = 300.\n",
    "                    if tmp < -300.:\n",
    "                        tmp = -300.\n",
    "                    if min(1., np.exp(tmp)) > np.random.random():\n",
    "                        lxcd = True\n",
    "                        swarm[i].position_i = x2vec.copy()\n",
    "                        swarm[i-1].position_i = x1vec.copy()\n",
    "                        print('exchanged', i, i-1)\n",
    "            it += 1\n",
    "        print('\\nfinal solution:')\n",
    "        print(f'   > {position_best_g}')\n",
    "        print(f'   > {obj_best_g}\\n')\n",
    "        if True:\n",
    "            abc = np.zeros(nparticles)\n",
    "            abcvec = np.zeros((nparticles, len(startx0)))\n",
    "            for i in range(nparticles):\n",
    "                abc[i] = swarm[i].obj_best_i\n",
    "                abcvec[i] = swarm[i].position_best_i\n",
    "            idx = abc.argsort()\n",
    "            abc = abc[idx]\n",
    "            abcvec = abcvec[idx, :]\n",
    "            for i in range(nparticles):\n",
    "                print(abc[i])\n",
    "                print(abcvec[i, :])\n",
    "\n",
    "\n",
    "startx0 = []\n",
    "xbounds = []\n",
    "for j in range(10):\n",
    "    startx0.append(-20.+np.random.random()*(20.-(-20.)))\n",
    "for j in range(len(startx0)):\n",
    "    xbounds.append((-20., 20.))\n",
    "REMC(functuser, startx0, xbounds, nparticles=50, maxiter=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d2c81",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82110162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def functuser(x):\n",
    "    case = 3\n",
    "\n",
    "    if case == 1:\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j])**2\n",
    "    if case == 2:\n",
    "        #    Rastrigin\n",
    "        total = 10.*len(x)\n",
    "        for j in range(len(x)):\n",
    "            total += x[j]**2-10.*np.cos(2.*np.pi*x[j])\n",
    "    if case == 3:\n",
    "        #   Rosenbrock\n",
    "        xarray0 = np.zeros(len(x))\n",
    "        for j in range(len(x)):\n",
    "            xarray0[j] = x[j]\n",
    "        total = sum(100.0*(xarray0[1:]-xarray0[:-1]\n",
    "                    ** 2.0)**2.0 + (1-xarray0[:-1])**2.0)\n",
    "    if case == 4:\n",
    "        #   Styblinski-Tang\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j]**4-16.*x[j]**2+5.*x[j])/2.\n",
    "    return total\n",
    "\n",
    "\n",
    "class PARTICLE:\n",
    "    def __init__(self, startx0, ww, c1, c2, xbounds, lverbose):\n",
    "        self.position_i = []\n",
    "        self.velocity_i = []\n",
    "        self.position_best_i = []\n",
    "        self.obj_best_i = 9e99\n",
    "        self.obj_i = 9e99\n",
    "        self.dimensions = len(startx0)\n",
    "        self.ww = ww+(np.random.random()-0.5)*0.2\n",
    "        self.c1 = c1+(np.random.random()-0.5)*0.2\n",
    "        self.c2 = c2+(np.random.random()-0.5)*0.2\n",
    "        if lverbose:\n",
    "            print(self.ww, self.c1, self.c2)\n",
    "        for j in range(self.dimensions):\n",
    "            self.velocity_i.append(np.random.uniform(-1, 1))\n",
    "            self.position_i.append(startx0[j]*(1.+(np.random.random()-0.5)*2.))\n",
    "        if np.random.random() < 0.8:\n",
    "            for j in range(self.dimensions):\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        for j in range(self.dimensions):\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        self.position_best_i = self.position_i.copy()\n",
    "\n",
    "    def evaluate(self, objfunct):\n",
    "        #       self.obj_i=objfunct(self.position_i)\n",
    "        xarray0 = np.zeros(self.dimensions)\n",
    "        for j in range(self.dimensions):\n",
    "            xarray0[j] = self.position_i[j]\n",
    "        res = minimize(objfunct, xarray0, method='nelder-mead',\n",
    "                       options={'xatol': 1e-6, 'disp': True, 'maxiter': 100000, 'maxfev': 40000 } )\n",
    "        self.position_i = res.x.copy()\n",
    "        self.obj_i = res.fun\n",
    "        if self.obj_i < self.obj_best_i:\n",
    "            self.position_best_i = self.position_i.copy()\n",
    "            self.obj_best_i = self.obj_i\n",
    "\n",
    "    def update_velocity(self, position_best_g):\n",
    "        for j in range(self.dimensions):\n",
    "            vc = self.c1 * \\\n",
    "                (self.position_best_i[j]-self.position_i[j])*np.random.random()\n",
    "            vs = self.c2*(position_best_g[j] -\n",
    "                          self.position_i[j])*np.random.random()\n",
    "            self.velocity_i[j] = self.ww*self.velocity_i[j]+vc+vs\n",
    "\n",
    "    def update_position(self, xbounds):\n",
    "        for j in range(self.dimensions):\n",
    "            self.position_i[j] = self.position_i[j]+self.velocity_i[j]\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "\n",
    "\n",
    "class PSO():\n",
    "    def __init__(self, objfunct, startx0, xbounds, ww=0.5, c1=1.0, c2=2.0, nparticles=50, maxiter=50000, verbose=False):\n",
    "        obj_best_g = 9e99\n",
    "        position_best_g = []\n",
    "        swarm = []\n",
    "        for _ in range(nparticles):\n",
    "            swarm.append(PARTICLE(startx0, ww, c1, c2, xbounds, verbose))\n",
    "        it = 0\n",
    "        while it < maxiter:\n",
    "            if verbose:\n",
    "                print(f'iter: {it:>6d} best solution: {obj_best_g:16.8e}')\n",
    "            for i in range(nparticles):\n",
    "                swarm[i].evaluate(objfunct)\n",
    "                if swarm[i].obj_i < obj_best_g:\n",
    "                    position_best_g = list(swarm[i].position_i)\n",
    "                    obj_best_g = float(swarm[i].obj_i)\n",
    "            for i in range(nparticles):\n",
    "                swarm[i].update_velocity(position_best_g)\n",
    "                swarm[i].update_position(xbounds)\n",
    "            it += 1\n",
    "        print('\\nfinal solution:')\n",
    "        print(f'   > {position_best_g}')\n",
    "        print(f'   > {obj_best_g}\\n')\n",
    "        if True:\n",
    "            abc = np.zeros(nparticles)\n",
    "            abcvec = np.zeros((nparticles, len(startx0)))\n",
    "            for i in range(nparticles):\n",
    "                abc[i] = swarm[i].obj_best_i\n",
    "                abcvec[i] = swarm[i].position_best_i\n",
    "            idx = abc.argsort()\n",
    "            abc = abc[idx]\n",
    "            abcvec = abcvec[idx, :]\n",
    "            for i in range(nparticles):\n",
    "                print(abc[i])\n",
    "                print(abcvec[i, :])\n",
    "\n",
    "\n",
    "startx0 = []\n",
    "xbounds = []\n",
    "for j in range(10):\n",
    "    startx0.append(-20.+np.random.random()*(20.-(-20.)))\n",
    "for j in range(len(startx0)):\n",
    "    xbounds.append((-20., 20.))\n",
    "PSO(functuser, startx0, xbounds, ww=0.5, c1=1.0,\n",
    "    c2=2.0, nparticles=50, maxiter=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390a98f",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a44885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def functuser(x):\n",
    "    case = 3\n",
    "\n",
    "    if case == 1:\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j])**2\n",
    "    if case == 2:\n",
    "        #    Rastrigin\n",
    "        total = 10.*len(x)\n",
    "        for j in range(len(x)):\n",
    "            total += x[j]**2-10.*np.cos(2.*np.pi*x[j])\n",
    "    if case == 3:\n",
    "        #   Rosenbrock\n",
    "        xarray0 = np.zeros(len(x))\n",
    "        for j in range(len(x)):\n",
    "            xarray0[j] = x[j]\n",
    "        total = sum(100.0*(xarray0[1:]-xarray0[:-1]\n",
    "                    ** 2.0)**2.0 + (1-xarray0[:-1])**2.0)\n",
    "    if case == 4:\n",
    "        #   Styblinski-Tang\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j]**4-16.*x[j]**2+5.*x[j])/2.\n",
    "    return total\n",
    "\n",
    "\n",
    "class PARTICLE:\n",
    "    def __init__(self, startx0, ptbmp, pmut, pcross, xbounds, lverbose):\n",
    "        self.position_i = []\n",
    "        self.position_best_i = []\n",
    "        self.obj_best_i = 9e99\n",
    "        self.obj_i = 9e99\n",
    "        self.dimensions = len(startx0)\n",
    "        self.ptbmp = ptbmp+(np.random.random()-0.5)*0.2\n",
    "        self.pmut = pmut+(np.random.random()-0.5)*0.1\n",
    "        self.pcross = pcross+(np.random.random()-0.5)*0.1\n",
    "        if self.pmut > 0.999 or self.pmut < 0.001:\n",
    "            self.pmut = np.random.random()\n",
    "        if self.pcross > 0.999 or self.pcross < 0.001:\n",
    "            self.pcross = np.ramdom.random()\n",
    "        if lverbose:\n",
    "            print(self.ptbmp, self.pmut, self.pcross)\n",
    "        for j in range(self.dimensions):\n",
    "            self.position_i.append(startx0[j]*(1.+(np.random.random()-0.5)*2.))\n",
    "        if np.random.random() < 0.8:\n",
    "            for j in range(self.dimensions):\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        for j in range(self.dimensions):\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        self.position_best_i = self.position_i.copy()\n",
    "\n",
    "    def evaluate(self, objfunct):\n",
    "        #       self.obj_i=objfunct(self.position_i)\n",
    "        xarray0 = np.zeros(self.dimensions)\n",
    "        for j in range(self.dimensions):\n",
    "            xarray0[j] = self.position_i[j]\n",
    "        res = minimize(objfunct, xarray0, method='nelder-mead',\n",
    "                       options={'xatol': 1e-6, 'disp': True, 'maxiter': 100000, 'maxfev': 40000})\n",
    "        self.position_i = res.x.copy()\n",
    "        self.obj_i = res.fun\n",
    "        if self.obj_i < self.obj_best_i:\n",
    "            self.position_best_i = self.position_i.copy()\n",
    "            self.obj_best_i = self.obj_i\n",
    "\n",
    "    def update_mutationcrossover(self, x1vec, x2vec):\n",
    "        if np.random.random() < 0.5:\n",
    "            for j in range(self.dimensions):\n",
    "                self.position_i[j] = x1vec[j]\n",
    "                if np.random.random() < self.pmut:\n",
    "                    self.position_i[j] = x1vec[j] * \\\n",
    "                        (1.+(np.random.random()-0.5)*self.ptbmp)\n",
    "        else:\n",
    "            for j in range(self.dimensions):\n",
    "                self.position_i[j] = x1vec[j]\n",
    "                if np.random.random() < self.pcross:\n",
    "                    self.position_i[j] = x2vec[j]\n",
    "\n",
    "    def update_position(self, xbounds):\n",
    "        for j in range(self.dimensions):\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "\n",
    "\n",
    "class GA():\n",
    "    def __init__(self, objfunct, startx0, xbounds, ptbmp=0.1, pmut=0.5, pcross=0.5, nparticles=50, maxiter=50000, verbose=False):\n",
    "        obj_best_g = 9e99\n",
    "        position_best_g = []\n",
    "        swarm = []\n",
    "        x1vec = []\n",
    "        x2vec = []\n",
    "        nsubpop = 0\n",
    "        for _ in range(nparticles):\n",
    "            swarm.append(PARTICLE(startx0, ptbmp, pmut,\n",
    "                         pcross, xbounds, verbose))\n",
    "        it = 0\n",
    "        while it < maxiter:\n",
    "            if verbose:\n",
    "                print(f'iter: {it:>6d} best solution: {obj_best_g:16.8e}')\n",
    "                if True and nparticles > 4:\n",
    "                    print('lowest five')\n",
    "                    abc = np.zeros(nparticles)\n",
    "                    abcvec = np.zeros((nparticles, len(startx0)))\n",
    "                    for i in range(nparticles):\n",
    "                        abc[i] = swarm[i].obj_best_i\n",
    "                        abcvec[i] = swarm[i].position_best_i\n",
    "                    idx = abc.argsort()\n",
    "                    abc = abc[idx]\n",
    "                    abcvec = abcvec[idx, :]\n",
    "                    print(abc[0], abc[1], abc[2], abc[3], abc[4])\n",
    "                    print(abcvec[0, :])\n",
    "                    print(abcvec[1, :])\n",
    "                    print(abcvec[2, :])\n",
    "                    print(abcvec[3, :])\n",
    "                    print(abcvec[4, :])\n",
    "            for i in range(nparticles):\n",
    "                swarm[i].evaluate(objfunct)\n",
    "                if swarm[i].obj_i < obj_best_g:\n",
    "                    position_best_g = list(swarm[i].position_i)\n",
    "                    obj_best_g = float(swarm[i].obj_i)\n",
    "            for i in range(nparticles):\n",
    "                i1 = int(np.random.random()*nparticles)\n",
    "                i2 = int(np.random.random()*nparticles)\n",
    "                k1 = i2\n",
    "                if swarm[i1].obj_best_i < swarm[i2].obj_best_i:\n",
    "                    k1 = i1\n",
    "                for _ in range(nsubpop-1):\n",
    "                    i1 = int(np.random.random()*nparticles)\n",
    "                    if swarm[i1].obj_best_i < swarm[k1].obj_best_i:\n",
    "                        k1 = i1\n",
    "                i1 = int(np.random.random()*nparticles)\n",
    "                i2 = int(np.random.random()*nparticles)\n",
    "                k2 = i2\n",
    "                if swarm[i1].obj_best_i < swarm[i2].obj_best_i:\n",
    "                    k2 = i1\n",
    "                for _ in range(nsubpop-1):\n",
    "                    i1 = int(np.random.random()*nparticles)\n",
    "                    if swarm[i1].obj_best_i < swarm[k2].obj_best_i:\n",
    "                        k2 = i1\n",
    "                x1vec = list(swarm[k1].position_best_i)\n",
    "                x2vec = list(swarm[k2].position_best_i)\n",
    "                swarm[i].update_mutationcrossover(x1vec, x2vec)\n",
    "                swarm[i].update_position(xbounds)\n",
    "            it += 1\n",
    "        print('\\nfinal solution:')\n",
    "        print(f'   > {position_best_g}')\n",
    "        print(f'   > {obj_best_g}\\n')\n",
    "        if True:\n",
    "            abc = np.zeros(nparticles)\n",
    "            abcvec = np.zeros((nparticles, len(startx0)))\n",
    "            for i in range(nparticles):\n",
    "                abc[i] = swarm[i].obj_best_i\n",
    "                abcvec[i] = swarm[i].position_best_i\n",
    "            idx = abc.argsort()\n",
    "            abc = abc[idx]\n",
    "            abcvec = abcvec[idx, :]\n",
    "            for i in range(nparticles):\n",
    "                print(abc[i])\n",
    "                print(abcvec[i, :])\n",
    "\n",
    "\n",
    "startx0 = []\n",
    "xbounds = []\n",
    "for j in range(10):\n",
    "    startx0.append(-20.+np.random.random()*(20.-(-20.)))\n",
    "for j in range(len(startx0)):\n",
    "    xbounds.append((-20., 20.))\n",
    "GA(functuser, startx0, xbounds, ptbmp=0.1, pmut=0.5,\n",
    "   pcross=0.5, nparticles=50, maxiter=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd26b0",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def functuser(x):\n",
    "    case = 3\n",
    "\n",
    "    if case == 1:\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j])**2\n",
    "    if case == 2:\n",
    "        #    Rastrigin\n",
    "        total = 10.*len(x)\n",
    "        for j in range(len(x)):\n",
    "            total += x[j]**2-10.*np.cos(2.*np.pi*x[j])\n",
    "    if case == 3:\n",
    "        #   Rosenbrock\n",
    "        xarray0 = np.zeros(len(x))\n",
    "        for j in range(len(x)):\n",
    "            xarray0[j] = x[j]\n",
    "        total = sum(100.0*(xarray0[1:]-xarray0[:-1]\n",
    "                    ** 2.0)**2.0 + (1-xarray0[:-1])**2.0)\n",
    "    if case == 4:\n",
    "        #   Styblinski-Tang\n",
    "        total = 0.\n",
    "        for j in range(len(x)):\n",
    "            total += (x[j]**4-16.*x[j]**2+5.*x[j])/2.\n",
    "    return total\n",
    "\n",
    "\n",
    "class PARTICLE:\n",
    "    def __init__(self, startx0, ptbmp, pccrr, ff, xbounds, lverbose):\n",
    "        self.position_i = []\n",
    "        self.position_best_i = []\n",
    "        self.obj_best_i = 9e99\n",
    "        self.obj_i = 9e99\n",
    "        self.dimensions = len(startx0)\n",
    "        self.ptbmp = ptbmp+(np.random.random()-0.5)*0.2\n",
    "        self.pccrr = pccrr+(np.random.random()-0.5)*0.2\n",
    "        self.ff = ff+(np.random.random()-0.5)*0.2\n",
    "        if self.pccrr > 0.999 or self.pccrr < 0.001:\n",
    "            self.pccrr = np.random.random()\n",
    "        if self.ff > 1.999 or self.ff < 0.001:\n",
    "            self.ff = np.random.random()*2.\n",
    "        if lverbose:\n",
    "            print(self.ptbmp, self.pccrr, self.ff)\n",
    "        for j in range(self.dimensions):\n",
    "            self.position_i.append(\n",
    "                startx0[j]*(1.+(np.random.random()-0.5)*2.*ptbmp))\n",
    "        if np.random.random() < 0.8:\n",
    "            for j in range(self.dimensions):\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        for j in range(self.dimensions):\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "        self.position_best_i = self.position_i.copy()\n",
    "\n",
    "    def evaluate(self, objfunct):\n",
    "        #       self.obj_i=objfunct(self.position_i)\n",
    "        xarray0 = np.zeros(self.dimensions)\n",
    "        for j in range(self.dimensions):\n",
    "            xarray0[j] = self.position_i[j]\n",
    "        res = minimize(objfunct, xarray0, method='nelder-mead',\n",
    "                       options={'xatol': 1e-6, 'disp': True, 'maxiter': 100000, 'maxfev': 40000})\n",
    "        self.position_i = res.x.copy()\n",
    "        self.obj_i = res.fun\n",
    "        if self.obj_i < self.obj_best_i:\n",
    "            self.position_best_i = self.position_i.copy()\n",
    "            self.obj_best_i = self.obj_i\n",
    "\n",
    "    def update_mutationcrossover(self, x1vec, x2vec, x3vec, ff):\n",
    "        ir = int(np.random.random()*self.dimensions)\n",
    "        for j in range(self.dimensions):\n",
    "            if np.random.random() < self.pccrr or j == ir:\n",
    "                self.position_i[j] = x1vec[j]+ff*(x2vec[j]-x3vec[j])\n",
    "            else:\n",
    "                self.position_i[j] = self.position_best_i[j]\n",
    "\n",
    "    def update_position(self, xbounds):\n",
    "        for j in range(self.dimensions):\n",
    "            if self.position_i[j] > xbounds[j][1]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "            if self.position_i[j] < xbounds[j][0]:\n",
    "                self.position_i[j] = xbounds[j][0] + \\\n",
    "                    (xbounds[j][1]-xbounds[j][0])*np.random.random()\n",
    "\n",
    "\n",
    "class DE():\n",
    "    def __init__(self, objfunct, startx0, xbounds, ptbmp=1.1, pccrr=0.5, ff=1.0, nparticles=50, maxiter=50000, verbose=False):\n",
    "        obj_best_g = 9e99\n",
    "        position_best_g = []\n",
    "        swarm = []\n",
    "        x1vec = []\n",
    "        x2vec = []\n",
    "        x3vec = []\n",
    "        for _ in range(nparticles):\n",
    "            swarm.append(PARTICLE(startx0, ptbmp, pccrr, ff, xbounds, verbose))\n",
    "        it = 0\n",
    "        while it < maxiter:\n",
    "            if verbose:\n",
    "                print(f'iter: {it:>6d} best solution: {obj_best_g:16.8e}')\n",
    "            for i in range(nparticles):\n",
    "                swarm[i].evaluate(objfunct)\n",
    "                if swarm[i].obj_i < obj_best_g:\n",
    "                    position_best_g = list(swarm[i].position_i)\n",
    "                    obj_best_g = float(swarm[i].obj_i)\n",
    "            for i in range(nparticles):\n",
    "                while True:\n",
    "                    i1 = int(np.random.random()*nparticles)\n",
    "                    i2 = int(np.random.random()*nparticles)\n",
    "                    i3 = int(np.random.random()*nparticles)\n",
    "                    if i1 == i:\n",
    "                        continue\n",
    "                    if i2 == i:\n",
    "                        continue\n",
    "                    if i3 == i:\n",
    "                        continue\n",
    "                    if i1 != i2 and i2 != i3 and i3 != i1:\n",
    "                        break\n",
    "                x1vec = list(swarm[i1].position_best_i)\n",
    "                x2vec = list(swarm[i2].position_best_i)\n",
    "                x3vec = list(swarm[i3].position_best_i)\n",
    "                swarm[i].update_mutationcrossover(x1vec, x2vec, x3vec, ff)\n",
    "                swarm[i].update_position(xbounds)\n",
    "            it += 1\n",
    "        print('\\nfinal solution:')\n",
    "        print(f'   > {position_best_g}')\n",
    "        print(f'   > {obj_best_g}\\n')\n",
    "        if True:\n",
    "            abc = np.zeros(nparticles)\n",
    "            abcvec = np.zeros((nparticles, len(startx0)))\n",
    "            for i in range(nparticles):\n",
    "                abc[i] = swarm[i].obj_best_i\n",
    "                abcvec[i] = swarm[i].position_best_i\n",
    "            idx = abc.argsort()\n",
    "            abc = abc[idx]\n",
    "            abcvec = abcvec[idx, :]\n",
    "            for i in range(nparticles):\n",
    "                print(abc[i])\n",
    "                print(abcvec[i, :])\n",
    "\n",
    "\n",
    "startx0 = []\n",
    "xbounds = []\n",
    "for j in range(10):\n",
    "    startx0.append(-20.+np.random.random()*(20.-(-20.)))\n",
    "for j in range(len(startx0)):\n",
    "    xbounds.append((-20., 20.))\n",
    "DE(functuser, startx0, xbounds, ptbmp=1.1, pccrr=0.5,\n",
    "   ff=1.0, nparticles=50, maxiter=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74be37",
   "metadata": {},
   "source": [
    "이공학적 설계를 위한 인공지능 최적화\n",
    "(Artificial Intelligence and Optimization Techniques for Engineering Design)\n",
    "\n",
    "Chapter 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_directories(npop, apath):\n",
    "    for i in range(npop):\n",
    "        astring = 'mkdir '+apath+'/'+str(i).zfill(4)\n",
    "        os.system(astring)\n",
    "    for i in range(npop):\n",
    "        astring = 'cp '+apath+'/CSA_SOLDIER.pbs'+' '+apath+'/'+str(i).zfill(4)\n",
    "        os.system(astring)\n",
    "\n",
    "def del_directories(npop, apath):\n",
    "    for i in range(npop):\n",
    "        astring = 'rm '+apath+'/'+str(i).zfill(4)\n",
    "        os.system(astring)\n",
    "\n",
    "def write_trial_solution(ndim0, xvector, apath, iidd, ncal, iobj):\n",
    "    fname = apath+'/'+str(iidd).zfill(4)+'/input.txt'\n",
    "#   print(fname)\n",
    "    gname = apath+'/'+str(iidd).zfill(4)+'/STATUS'\n",
    "    hname = apath+'/'+str(iidd).zfill(4)+'/output.txt'\n",
    "    iname = apath+'/'+str(iidd).zfill(4)+'/OUTPUT'\n",
    "    jname = apath+'/'+str(iidd).zfill(4)+'/STOP'\n",
    "    if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "    isign = 1\n",
    "    if ndim0 < 0:\n",
    "        isign = -1\n",
    "    ndim = -ndim0\n",
    "    if isign == -1:\n",
    "        gen_trial_solution(fname, ndim, ncal, iobj)\n",
    "#       print(fname,'r input.txt and qsub')\n",
    "    if isign == 1:\n",
    "        ndim = ndim0\n",
    "        n3 = int(ndim/3)\n",
    "        lines_to_append = []\n",
    "        list1 = [int(xvector[i]) for i in range(n3)]\n",
    "        astring = ' '\n",
    "        for i in range(n3):\n",
    "            astring = astring+str(list1[i])+' '\n",
    "        lines_to_append.append(astring)\n",
    "        list1 = [int(xvector[n3+i]) for i in range(n3)]\n",
    "        astring = ' '\n",
    "        for i in range(n3):\n",
    "            astring = astring+str(list1[i])+' '\n",
    "        lines_to_append.append(astring)\n",
    "        list1 = [int(xvector[2*n3+i]) for i in range(n3)]\n",
    "        astring = ' '\n",
    "        for i in range(n3):\n",
    "            astring = astring+str(list1[i])+' '\n",
    "        lines_to_append.append(astring)\n",
    "        lines_to_append.append(str(ncal)+' '+str(iobj)+' '+'\\n')\n",
    "        append_multiple_lines(fname, lines_to_append)\n",
    "#       print(fname,'c or m input.txt and qsub')\n",
    "    if False:\n",
    "        astring = 'cd '+apath+'/'+str(iidd).zfill(4)+' ; qsub CSA_SOLDIER.pbs'\n",
    "    else:\n",
    "        astring = 'cd '+apath+'/' + \\\n",
    "                 str(iidd).zfill(4)+' ; sbatch CSA_SOLDIER.pbs'\n",
    "    os.system(astring)\n",
    "    astring = 'echo \"ING\" >> '+gname\n",
    "    os.system(astring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5c0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testAI",
   "language": "python",
   "name": "testai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
