{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     x     |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8386  \u001b[0m | \u001b[0m 2.391   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6615  \u001b[0m | \u001b[0m 3.722   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8388  \u001b[0m | \u001b[95m 2.391   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9439  \u001b[0m | \u001b[95m 2.541   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 1.023   \u001b[0m | \u001b[95m 2.676   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 1.089   \u001b[0m | \u001b[95m 2.856   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 1.098   \u001b[0m | \u001b[95m 3.02    \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.2003  \u001b[0m | \u001b[0m-1.998   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.02715 \u001b[0m | \u001b[0m 6.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 1.051   \u001b[0m | \u001b[0m 0.112   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 1.707   \u001b[0m | \u001b[95m 0.6321  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 1.244   \u001b[0m | \u001b[0m 0.8781  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 1.633   \u001b[0m | \u001b[0m 0.5334  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 1.089   \u001b[0m | \u001b[0m 2.856   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6326  \u001b[0m | \u001b[0m-0.7621  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.09098 \u001b[0m | \u001b[0m 4.74    \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.4304  \u001b[0m | \u001b[0m 1.65    \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.3637  \u001b[0m | \u001b[0m-1.323   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 1.693   \u001b[0m | \u001b[0m 0.6719  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.9135  \u001b[0m | \u001b[0m-0.3082  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.03733 \u001b[0m | \u001b[0m 5.364   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.2988  \u001b[0m | \u001b[0m 4.186   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9588  \u001b[0m | \u001b[0m 3.361   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5683  \u001b[0m | \u001b[0m 2.001   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.4828  \u001b[0m | \u001b[0m 1.251   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.2655  \u001b[0m | \u001b[0m-1.663   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 1.312   \u001b[0m | \u001b[0m 0.3585  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 1.701   \u001b[0m | \u001b[0m 0.6046  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.03068 \u001b[0m | \u001b[0m 5.69    \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.4879  \u001b[0m | \u001b[0m-1.024   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.994   \u001b[0m | \u001b[0m-0.1044  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.05371 \u001b[0m | \u001b[0m 5.037   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.1717  \u001b[0m | \u001b[0m 4.446   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 1.707   \u001b[0m | \u001b[0m 0.637   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.7818  \u001b[0m | \u001b[0m-0.5284  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 1.616   \u001b[0m | \u001b[0m 0.7341  \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "def target(x):\n",
    "    return np.exp(-(x-3)**2) + np.exp(-(3*x-2)**2) + 1/(x**2+1)\n",
    "bayes_optimizer = BayesianOptimization(target, {'x': (-2, 6)}, random_state=0)\n",
    "bayes_optimizer.maximize(init_points=2, n_iter=34, acq='ei', xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  2.5]\n",
      "2.6000073326290792e-17\n",
      "|   iter    |  target   |    x1     |    x2     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.085   \u001b[0m | \u001b[0m 0.2211  \u001b[0m | \u001b[0m 1.808   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.64    \u001b[0m | \u001b[0m 1.43    \u001b[0m | \u001b[0m 0.9332  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.5259  \u001b[0m | \u001b[95m 0.9565  \u001b[0m | \u001b[95m 1.776   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.5518  \u001b[0m | \u001b[0m 0.9364  \u001b[0m | \u001b[0m 1.76    \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-0.02086 \u001b[0m | \u001b[95m 1.023   \u001b[0m | \u001b[95m 2.643   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-1.25    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.7299  \u001b[0m | \u001b[0m 0.3072  \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-7.25    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.149   \u001b[0m | \u001b[0m 0.6405  \u001b[0m | \u001b[0m 2.36    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.208   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.044   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.2503  \u001b[0m | \u001b[0m 1.018   \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.02832 \u001b[0m | \u001b[0m 1.072   \u001b[0m | \u001b[0m 2.348   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-0.009925\u001b[0m | \u001b[95m 0.9036  \u001b[0m | \u001b[95m 2.475   \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m-0.002315\u001b[0m | \u001b[95m 1.047   \u001b[0m | \u001b[95m 2.49    \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.008668\u001b[0m | \u001b[0m 0.9711  \u001b[0m | \u001b[0m 2.411   \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m-0.000151\u001b[0m | \u001b[95m 0.9972  \u001b[0m | \u001b[95m 2.512   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.000228\u001b[0m | \u001b[0m 1.009   \u001b[0m | \u001b[0m 2.488   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.000466\u001b[0m | \u001b[0m 0.9908  \u001b[0m | \u001b[0m 2.52    \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.000158\u001b[0m | \u001b[0m 0.9897  \u001b[0m | \u001b[0m 2.493   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.001809\u001b[0m | \u001b[0m 0.9695  \u001b[0m | \u001b[0m 2.53    \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.000153\u001b[0m | \u001b[0m 1.012   \u001b[0m | \u001b[0m 2.502   \u001b[0m |\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m-2.157e-0\u001b[0m | \u001b[95m 0.9963  \u001b[0m | \u001b[95m 2.497   \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m-1.637e-0\u001b[0m | \u001b[95m 0.9983  \u001b[0m | \u001b[95m 2.504   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.000867\u001b[0m | \u001b[0m 1.029   \u001b[0m | \u001b[0m 2.507   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.000491\u001b[0m | \u001b[0m 0.9792  \u001b[0m | \u001b[0m 2.508   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.001499\u001b[0m | \u001b[0m 1.026   \u001b[0m | \u001b[0m 2.529   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.000530\u001b[0m | \u001b[0m 1.021   \u001b[0m | \u001b[0m 2.51    \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.000249\u001b[0m | \u001b[0m 0.9981  \u001b[0m | \u001b[0m 2.484   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.000471\u001b[0m | \u001b[0m 0.9978  \u001b[0m | \u001b[0m 2.522   \u001b[0m |\n",
      "| \u001b[95m 30      \u001b[0m | \u001b[95m-2.017e-0\u001b[0m | \u001b[95m 0.9993  \u001b[0m | \u001b[95m 2.501   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.000102\u001b[0m | \u001b[0m 0.9967  \u001b[0m | \u001b[0m 2.49    \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.000179\u001b[0m | \u001b[0m 0.9915  \u001b[0m | \u001b[0m 2.49    \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.000123\u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 2.499   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.000186\u001b[0m | \u001b[0m 0.9915  \u001b[0m | \u001b[0m 2.511   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.000218\u001b[0m | \u001b[0m 1.01    \u001b[0m | \u001b[0m 2.489   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.000396\u001b[0m | \u001b[0m 0.9844  \u001b[0m | \u001b[0m 2.512   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.000198\u001b[0m | \u001b[0m 0.9933  \u001b[0m | \u001b[0m 2.488   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.000102\u001b[0m | \u001b[0m 0.9955  \u001b[0m | \u001b[0m 2.509   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.000187\u001b[0m | \u001b[0m 0.998   \u001b[0m | \u001b[0m 2.486   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.000228\u001b[0m | \u001b[0m 0.9994  \u001b[0m | \u001b[0m 2.485   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.000644\u001b[0m | \u001b[0m 1.022   \u001b[0m | \u001b[0m 2.513   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.000116\u001b[0m | \u001b[0m 0.9973  \u001b[0m | \u001b[0m 2.51    \u001b[0m |\n",
      "=================================================\n",
      "Iteration 0: \n",
      "\t{'target': -1.0853630178801268, 'params': {'x1': 0.22110743669848998, 'x2': 1.8081261728366003}}\n",
      "Iteration 1: \n",
      "\t{'target': -2.6395132715854372, 'params': {'x1': 1.4298327112379794, 'x2': 0.9332335490140067}}\n",
      "Iteration 2: \n",
      "\t{'target': -0.525874091441912, 'params': {'x1': 0.9564999903367855, 'x2': 1.7761341004017304}}\n",
      "Iteration 3: \n",
      "\t{'target': -0.5518278330257161, 'params': {'x1': 0.9363536794702918, 'x2': 1.7598804293164882}}\n",
      "Iteration 4: \n",
      "\t{'target': -0.020858045843434547, 'params': {'x1': 1.022500357765252, 'x2': 2.6426596640395252}}\n",
      "Iteration 5: \n",
      "\t{'target': -1.25, 'params': {'x1': 2.0, 'x2': 3.0}}\n",
      "Iteration 6: \n",
      "\t{'target': -0.7299442249410985, 'params': {'x1': 0.3072199303234117, 'x2': 3.0}}\n",
      "Iteration 7: \n",
      "\t{'target': -7.25, 'params': {'x1': 0.0, 'x2': 0.0}}\n",
      "Iteration 8: \n",
      "\t{'target': -0.14895526902624037, 'params': {'x1': 0.6404890294554424, 'x2': 2.3596179103861514}}\n",
      "Iteration 9: \n",
      "\t{'target': -1.2080102945562103, 'params': {'x1': 2.0, 'x2': 2.0439185439461385}}\n",
      "Iteration 10: \n",
      "\t{'target': -0.2503349217430209, 'params': {'x1': 1.0183008672751026, 'x2': 3.0}}\n",
      "Iteration 11: \n",
      "\t{'target': -0.02831706436860015, 'params': {'x1': 1.07188561827675, 'x2': 2.3478503294332533}}\n",
      "Iteration 12: \n",
      "\t{'target': -0.009925009693743396, 'params': {'x1': 0.9036192806386383, 'x2': 2.4747855868771484}}\n",
      "Iteration 13: \n",
      "\t{'target': -0.0023151061602888034, 'params': {'x1': 1.0469902743021666, 'x2': 2.4896549392802196}}\n",
      "Iteration 14: \n",
      "\t{'target': -0.008668311064313039, 'params': {'x1': 0.9711416272661924, 'x2': 2.41148160989111}}\n",
      "Iteration 15: \n",
      "\t{'target': -0.00015140278936652998, 'params': {'x1': 0.9971642236867273, 'x2': 2.511973352165021}}\n",
      "Iteration 16: \n",
      "\t{'target': -0.00022883816059695536, 'params': {'x1': 1.0088329023265563, 'x2': 2.4877192020989485}}\n",
      "Iteration 17: \n",
      "\t{'target': -0.0004663098011983151, 'params': {'x1': 0.9908124745175049, 'x2': 2.519542240831256}}\n",
      "Iteration 18: \n",
      "\t{'target': -0.00015816469305035105, 'params': {'x1': 0.989664690266365, 'x2': 2.492834383085862}}\n",
      "Iteration 19: \n",
      "\t{'target': -0.0018094605667081886, 'params': {'x1': 0.969543539627383, 'x2': 2.529696204947424}}\n",
      "Iteration 20: \n",
      "\t{'target': -0.00015373846060125787, 'params': {'x1': 1.012236150968262, 'x2': 2.5020037639789052}}\n",
      "Iteration 21: \n",
      "\t{'target': -2.1572927034740635e-05, 'params': {'x1': 0.9963042767277861, 'x2': 2.4971867178367684}}\n",
      "Iteration 22: \n",
      "\t{'target': -1.637463262776594e-05, 'params': {'x1': 0.9983015245023621, 'x2': 2.503672848160718}}\n",
      "Iteration 23: \n",
      "\t{'target': -0.0008672543856279191, 'params': {'x1': 1.0286217865327252, 'x2': 2.5069316463630966}}\n",
      "Iteration 24: \n",
      "\t{'target': -0.0004913605242270734, 'params': {'x1': 0.9791896905705468, 'x2': 2.5076348900239287}}\n",
      "Iteration 25: \n",
      "\t{'target': -0.0014993726937629222, 'params': {'x1': 1.0257838157270207, 'x2': 2.5288888826422546}}\n",
      "Iteration 26: \n",
      "\t{'target': -0.0005304457444959868, 'params': {'x1': 1.0205460921619809, 'x2': 2.5104069131526776}}\n",
      "Iteration 27: \n",
      "\t{'target': -0.0002497298344120034, 'params': {'x1': 0.9981181480061403, 'x2': 2.4843096058849623}}\n",
      "Iteration 28: \n",
      "\t{'target': -0.0004719218417507794, 'params': {'x1': 0.9978447238087584, 'x2': 2.521616582206497}}\n",
      "Iteration 29: \n",
      "\t{'target': -2.017178533514881e-06, 'params': {'x1': 0.9993227627401029, 'x2': 2.501248410280045}}\n",
      "Iteration 30: \n",
      "\t{'target': -0.00010250899916888646, 'params': {'x1': 0.9966586156291415, 'x2': 2.4904425866650506}}\n",
      "Iteration 31: \n",
      "\t{'target': -0.00017912954536838515, 'params': {'x1': 0.9914850201441712, 'x2': 2.489674078083618}}\n",
      "Iteration 32: \n",
      "\t{'target': -0.0001238204073618583, 'params': {'x1': 0.988953837555453, 'x2': 2.4986573523878035}}\n",
      "Iteration 33: \n",
      "\t{'target': -0.0001860008201409102, 'params': {'x1': 0.9915377886654955, 'x2': 2.5106954102058356}}\n",
      "Iteration 34: \n",
      "\t{'target': -0.0002188549952387125, 'params': {'x1': 1.0098134426259926, 'x2': 2.4889297091698066}}\n",
      "Iteration 35: \n",
      "\t{'target': -0.0003961206494056331, 'params': {'x1': 0.9844378128657802, 'x2': 2.5124072148770447}}\n",
      "Iteration 36: \n",
      "\t{'target': -0.00019864619444191035, 'params': {'x1': 0.9932608490657289, 'x2': 2.4876213878351803}}\n",
      "Iteration 37: \n",
      "\t{'target': -0.00010205260925003873, 'params': {'x1': 0.9955473380320423, 'x2': 2.509067877957336}}\n",
      "Iteration 38: \n",
      "\t{'target': -0.00018736342066899724, 'params': {'x1': 0.9979679440271609, 'x2': 2.486463598366174}}\n",
      "Iteration 39: \n",
      "\t{'target': -0.00022837412685781705, 'params': {'x1': 0.9994010055242668, 'x2': 2.484899823428984}}\n",
      "Iteration 40: \n",
      "\t{'target': -0.0006442966715312412, 'params': {'x1': 1.0220502559285038, 'x2': 2.5125731016467188}}\n",
      "Iteration 41: \n",
      "\t{'target': -0.00011633073677713438, 'params': {'x1': 0.9972562827884275, 'x2': 2.510430855796149}}\n",
      "{'target': -2.017178533514881e-06, 'params': {'x1': 0.9993227627401029, 'x2': 2.501248410280045}}\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as optimize\n",
    "from bayes_opt import BayesianOptimization\n",
    "fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
    "res = optimize.minimize(fun, (2, 0), method='TNC', tol=1e-10)\n",
    "print(res.x)\n",
    "print(res.fun)\n",
    "def fun(x1,x2):\n",
    "      return -((x1-1)**2+ (x2-2.5)**2)\n",
    "BO = BayesianOptimization(fun, {'x1': (0., 2.), 'x2': (0., 3.) }, verbose=2)\n",
    "BO.maximize(init_points=2, n_iter=40)\n",
    "for i, res in enumerate(BO.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "print(BO.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve (Receiver Operating Characteristic curve) \n",
    " TPR : True Positive Rate (=민감도, true accept rate)\n",
    "1인 케이스에 대해 1로 잘 예측한 비율."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  colSam   | maxDepth  | minChi... | numLeaves | scaleW... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7972  \u001b[0m | \u001b[0m 86.5    \u001b[0m | \u001b[0m 35.77   \u001b[0m | \u001b[0m 63.27   \u001b[0m | \u001b[0m 9.807e+0\u001b[0m | \u001b[0m 0.4328  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7384  \u001b[0m | \u001b[0m 56.14   \u001b[0m | \u001b[0m 69.72   \u001b[0m | \u001b[0m 67.79   \u001b[0m | \u001b[0m 1.8e+03 \u001b[0m | \u001b[0m 0.5788  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8372  \u001b[0m | \u001b[95m 0.8569  \u001b[0m | \u001b[95m 34.02   \u001b[0m | \u001b[95m 39.03   \u001b[0m | \u001b[95m 54.98   \u001b[0m | \u001b[95m 117.2   \u001b[0m | \u001b[95m 0.6857  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9603  \u001b[0m | \u001b[0m 49.61   \u001b[0m | \u001b[0m 45.77   \u001b[0m | \u001b[0m 37.83   \u001b[0m | \u001b[0m 6.511e+0\u001b[0m | \u001b[0m 0.7051  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.836   \u001b[0m | \u001b[0m 0.5606  \u001b[0m | \u001b[0m 21.07   \u001b[0m | \u001b[0m 39.74   \u001b[0m | \u001b[0m 15.94   \u001b[0m | \u001b[0m 125.3   \u001b[0m | \u001b[0m 0.8265  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.781   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.256e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7135  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 3.383e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.056e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8271  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 649.8   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 20.77   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.134e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.864e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.844   \u001b[0m | \u001b[95m 0.4094  \u001b[0m | \u001b[95m 24.16   \u001b[0m | \u001b[95m 20.31   \u001b[0m | \u001b[95m 21.48   \u001b[0m | \u001b[95m 124.5   \u001b[0m | \u001b[95m 0.8953  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6667  \u001b[0m | \u001b[0m 0.4892  \u001b[0m | \u001b[0m 3.104   \u001b[0m | \u001b[0m 14.84   \u001b[0m | \u001b[0m 67.27   \u001b[0m | \u001b[0m 370.8   \u001b[0m | \u001b[0m 0.6032  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7668  \u001b[0m | \u001b[0m 0.8779  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 910.8   \u001b[0m | \u001b[0m 0.6263  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 4.494e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.9974  \u001b[0m | \u001b[0m 23.97   \u001b[0m | \u001b[0m 21.51   \u001b[0m | \u001b[0m 18.82   \u001b[0m | \u001b[0m 115.0   \u001b[0m | \u001b[0m 0.5412  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 748.1   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.8374  \u001b[0m | \u001b[0m 0.8932  \u001b[0m | \u001b[0m 31.28   \u001b[0m | \u001b[0m 34.23   \u001b[0m | \u001b[0m 18.41   \u001b[0m | \u001b[0m 128.0   \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8411  \u001b[0m | \u001b[0m 89.0    \u001b[0m | \u001b[0m 7.518   \u001b[0m | \u001b[0m 66.67   \u001b[0m | \u001b[0m 1.148e+0\u001b[0m | \u001b[0m 0.7092  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8369  \u001b[0m | \u001b[0m 0.5857  \u001b[0m | \u001b[0m 18.92   \u001b[0m | \u001b[0m 33.74   \u001b[0m | \u001b[0m 19.62   \u001b[0m | \u001b[0m 122.1   \u001b[0m | \u001b[0m 0.4859  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.839   \u001b[0m | \u001b[0m 0.775   \u001b[0m | \u001b[0m 23.73   \u001b[0m | \u001b[0m 40.27   \u001b[0m | \u001b[0m 48.65   \u001b[0m | \u001b[0m 121.9   \u001b[0m | \u001b[0m 0.6417  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7153  \u001b[0m | \u001b[0m 0.5513  \u001b[0m | \u001b[0m 78.52   \u001b[0m | \u001b[0m 2.59    \u001b[0m | \u001b[0m 26.97   \u001b[0m | \u001b[0m 3.017e+0\u001b[0m | \u001b[0m 0.4605  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.6022  \u001b[0m | \u001b[0m 0.9258  \u001b[0m | \u001b[0m 8.183   \u001b[0m | \u001b[0m 4.401   \u001b[0m | \u001b[0m 33.76   \u001b[0m | \u001b[0m 2.646e+0\u001b[0m | \u001b[0m 0.729   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4448  \u001b[0m | \u001b[0m 80.54   \u001b[0m | \u001b[0m 7.008   \u001b[0m | \u001b[0m 84.19   \u001b[0m | \u001b[0m 8.956e+0\u001b[0m | \u001b[0m 0.9688  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.8376  \u001b[0m | \u001b[0m 0.5335  \u001b[0m | \u001b[0m 23.59   \u001b[0m | \u001b[0m 21.03   \u001b[0m | \u001b[0m 24.43   \u001b[0m | \u001b[0m 114.7   \u001b[0m | \u001b[0m 0.5985  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4603  \u001b[0m | \u001b[0m 82.23   \u001b[0m | \u001b[0m 67.76   \u001b[0m | \u001b[0m 87.78   \u001b[0m | \u001b[0m 7.332e+0\u001b[0m | \u001b[0m 0.4634  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9666  \u001b[0m | \u001b[0m 9.353   \u001b[0m | \u001b[0m 16.86   \u001b[0m | \u001b[0m 73.99   \u001b[0m | \u001b[0m 5.78e+03\u001b[0m | \u001b[0m 0.8313  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.541   \u001b[0m | \u001b[0m 0.6581  \u001b[0m | \u001b[0m 80.84   \u001b[0m | \u001b[0m 2.335   \u001b[0m | \u001b[0m 89.9    \u001b[0m | \u001b[0m 2.251e+0\u001b[0m | \u001b[0m 0.827   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.209e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4949  \u001b[0m | \u001b[0m 8.483   \u001b[0m | \u001b[0m 65.9    \u001b[0m | \u001b[0m 20.6    \u001b[0m | \u001b[0m 9.374e+0\u001b[0m | \u001b[0m 0.4354  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.566   \u001b[0m | \u001b[0m 71.25   \u001b[0m | \u001b[0m 38.85   \u001b[0m | \u001b[0m 5.258   \u001b[0m | \u001b[0m 4.142e+0\u001b[0m | \u001b[0m 0.6     \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.8333  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 222.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8957  \u001b[0m | \u001b[0m 71.99   \u001b[0m | \u001b[0m 67.22   \u001b[0m | \u001b[0m 6.784   \u001b[0m | \u001b[0m 8.538e+0\u001b[0m | \u001b[0m 0.6269  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.727   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 6.925e+0\u001b[0m | \u001b[0m 0.4911  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.734e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 3.567e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 2.855e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 530.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.8299  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 759.7   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.8378  \u001b[0m | \u001b[0m 0.6232  \u001b[0m | \u001b[0m 17.52   \u001b[0m | \u001b[0m 49.66   \u001b[0m | \u001b[0m 8.003   \u001b[0m | \u001b[0m 247.9   \u001b[0m | \u001b[0m 0.9371  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7146  \u001b[0m | \u001b[0m 82.42   \u001b[0m | \u001b[0m 68.81   \u001b[0m | \u001b[0m 40.91   \u001b[0m | \u001b[0m 6.138e+0\u001b[0m | \u001b[0m 0.8671  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.825e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.8354  \u001b[0m | \u001b[0m 0.4964  \u001b[0m | \u001b[0m 3.159   \u001b[0m | \u001b[0m 54.2    \u001b[0m | \u001b[0m 82.3    \u001b[0m | \u001b[0m 222.3   \u001b[0m | \u001b[0m 0.6759  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 26.94   \u001b[0m | \u001b[0m 1.669   \u001b[0m | \u001b[0m 77.41   \u001b[0m | \u001b[0m 5.397e+0\u001b[0m | \u001b[0m 0.8759  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.7105  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.087e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4844  \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 9.05    \u001b[0m | \u001b[0m 6.165   \u001b[0m | \u001b[0m 1.512e+0\u001b[0m | \u001b[0m 0.9682  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.828   \u001b[0m | \u001b[0m 0.7936  \u001b[0m | \u001b[0m 87.97   \u001b[0m | \u001b[0m 64.69   \u001b[0m | \u001b[0m 52.81   \u001b[0m | \u001b[0m 6.408   \u001b[0m | \u001b[0m 0.6478  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.8345  \u001b[0m | \u001b[0m 0.4175  \u001b[0m | \u001b[0m 15.17   \u001b[0m | \u001b[0m 7.428   \u001b[0m | \u001b[0m 81.64   \u001b[0m | \u001b[0m 13.27   \u001b[0m | \u001b[0m 0.4889  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.776e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.8401  \u001b[0m | \u001b[0m 0.5116  \u001b[0m | \u001b[0m 29.64   \u001b[0m | \u001b[0m 3.407   \u001b[0m | \u001b[0m 6.248   \u001b[0m | \u001b[0m 7.47    \u001b[0m | \u001b[0m 0.9173  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.7814  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.372e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.7706  \u001b[0m | \u001b[0m 0.5203  \u001b[0m | \u001b[0m 15.8    \u001b[0m | \u001b[0m 1.234   \u001b[0m | \u001b[0m 25.65   \u001b[0m | \u001b[0m 210.7   \u001b[0m | \u001b[0m 0.5014  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.7182  \u001b[0m | \u001b[0m 0.5099  \u001b[0m | \u001b[0m 83.78   \u001b[0m | \u001b[0m 69.7    \u001b[0m | \u001b[0m 62.15   \u001b[0m | \u001b[0m 294.7   \u001b[0m | \u001b[0m 0.5651  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4587  \u001b[0m | \u001b[0m 10.18   \u001b[0m | \u001b[0m 34.86   \u001b[0m | \u001b[0m 8.09    \u001b[0m | \u001b[0m 9.998e+0\u001b[0m | \u001b[0m 0.9009  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.8431  \u001b[0m | \u001b[0m 0.7277  \u001b[0m | \u001b[0m 88.92   \u001b[0m | \u001b[0m 11.23   \u001b[0m | \u001b[0m 79.7    \u001b[0m | \u001b[0m 64.34   \u001b[0m | \u001b[0m 0.9486  \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.8288  \u001b[0m | \u001b[0m 0.5004  \u001b[0m | \u001b[0m 86.77   \u001b[0m | \u001b[0m 7.878   \u001b[0m | \u001b[0m 8.958   \u001b[0m | \u001b[0m 2.261   \u001b[0m | \u001b[0m 0.8533  \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.313e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.8423  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 201.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 2.049e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.8351  \u001b[0m | \u001b[0m 0.8761  \u001b[0m | \u001b[0m 81.02   \u001b[0m | \u001b[0m 58.85   \u001b[0m | \u001b[0m 89.96   \u001b[0m | \u001b[0m 167.1   \u001b[0m | \u001b[0m 0.6887  \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.8241  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5269  \u001b[0m | \u001b[0m 3.742   \u001b[0m | \u001b[0m 47.03   \u001b[0m | \u001b[0m 18.02   \u001b[0m | \u001b[0m 2.45e+03\u001b[0m | \u001b[0m 0.6923  \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 4.319e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.057e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 977.9   \u001b[0m | \u001b[0m 0.5613  \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.5309  \u001b[0m | \u001b[0m 0.8353  \u001b[0m | \u001b[0m 88.02   \u001b[0m | \u001b[0m 62.53   \u001b[0m | \u001b[0m 89.31   \u001b[0m | \u001b[0m 673.3   \u001b[0m | \u001b[0m 0.6552  \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.6567  \u001b[0m | \u001b[0m 0.8095  \u001b[0m | \u001b[0m 88.15   \u001b[0m | \u001b[0m 3.088   \u001b[0m | \u001b[0m 28.51   \u001b[0m | \u001b[0m 838.5   \u001b[0m | \u001b[0m 0.7307  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.943e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.8109  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 683.5   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 9.571e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7878  \u001b[0m | \u001b[0m 8.909   \u001b[0m | \u001b[0m 9.71    \u001b[0m | \u001b[0m 87.08   \u001b[0m | \u001b[0m 8.732e+0\u001b[0m | \u001b[0m 0.7587  \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.7196  \u001b[0m | \u001b[0m 0.7571  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 8.349e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.8256  \u001b[0m | \u001b[0m 0.4448  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 819.8   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 8.297e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.8344  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 194.4   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.6547  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.54e+03\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7088  \u001b[0m | \u001b[0m 89.94   \u001b[0m | \u001b[0m 65.87   \u001b[0m | \u001b[0m 18.87   \u001b[0m | \u001b[0m 9.154e+0\u001b[0m | \u001b[0m 0.8279  \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.8308  \u001b[0m | \u001b[0m 0.9641  \u001b[0m | \u001b[0m 10.04   \u001b[0m | \u001b[0m 8.278   \u001b[0m | \u001b[0m 87.31   \u001b[0m | \u001b[0m 159.8   \u001b[0m | \u001b[0m 0.6083  \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9519  \u001b[0m | \u001b[0m 74.97   \u001b[0m | \u001b[0m 69.37   \u001b[0m | \u001b[0m 16.15   \u001b[0m | \u001b[0m 5.618e+0\u001b[0m | \u001b[0m 0.5779  \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.438e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.557   \u001b[0m | \u001b[0m 0.4264  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 6.312e+0\u001b[0m | \u001b[0m 0.6982  \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.8308  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 265.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.76    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 320.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.749   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.498e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 2.94e+03\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 6.725e+0\u001b[0m | \u001b[0m 0.9469  \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4831  \u001b[0m | \u001b[0m 87.73   \u001b[0m | \u001b[0m 7.114   \u001b[0m | \u001b[0m 9.782   \u001b[0m | \u001b[0m 3.155e+0\u001b[0m | \u001b[0m 0.7417  \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.8419  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 63.09   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.7074  \u001b[0m | \u001b[0m 89.52   \u001b[0m | \u001b[0m 4.831   \u001b[0m | \u001b[0m 7.926   \u001b[0m | \u001b[0m 187.4   \u001b[0m | \u001b[0m 0.6665  \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 907.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.6994  \u001b[0m | \u001b[0m 6.948   \u001b[0m | \u001b[0m 69.5    \u001b[0m | \u001b[0m 83.54   \u001b[0m | \u001b[0m 1.353e+0\u001b[0m | \u001b[0m 0.8921  \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 0.635   \u001b[0m | \u001b[0m 0.5388  \u001b[0m | \u001b[0m 7.912   \u001b[0m | \u001b[0m 2.89    \u001b[0m | \u001b[0m 84.61   \u001b[0m | \u001b[0m 773.1   \u001b[0m | \u001b[0m 0.8112  \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7384  \u001b[0m | \u001b[0m 2.469   \u001b[0m | \u001b[0m 12.73   \u001b[0m | \u001b[0m 35.27   \u001b[0m | \u001b[0m 610.8   \u001b[0m | \u001b[0m 0.492   \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.8307  \u001b[0m | \u001b[0m 0.5667  \u001b[0m | \u001b[0m 44.51   \u001b[0m | \u001b[0m 0.4908  \u001b[0m | \u001b[0m 56.97   \u001b[0m | \u001b[0m 35.52   \u001b[0m | \u001b[0m 0.5824  \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 61.88   \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 848.7   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.8424  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 57.5    \u001b[0m | \u001b[0m 54.27   \u001b[0m | \u001b[0m 50.44   \u001b[0m | \u001b[0m 213.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.8336  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 21.56   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 274.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.8366  \u001b[0m | \u001b[0m 0.7364  \u001b[0m | \u001b[0m 88.49   \u001b[0m | \u001b[0m 22.81   \u001b[0m | \u001b[0m 15.48   \u001b[0m | \u001b[0m 98.25   \u001b[0m | \u001b[0m 0.4898  \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8954  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.527e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 0.6962  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.971e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4042  \u001b[0m | \u001b[0m 78.86   \u001b[0m | \u001b[0m 43.23   \u001b[0m | \u001b[0m 82.56   \u001b[0m | \u001b[0m 5.944e+0\u001b[0m | \u001b[0m 0.5812  \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.203e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.434e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7133  \u001b[0m | \u001b[0m 5.504   \u001b[0m | \u001b[0m 10.78   \u001b[0m | \u001b[0m 88.28   \u001b[0m | \u001b[0m 4.596e+0\u001b[0m | \u001b[0m 0.5099  \u001b[0m |\n",
      "| \u001b[0m 106     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.6957  \u001b[0m | \u001b[0m 7.343   \u001b[0m | \u001b[0m 7.883   \u001b[0m | \u001b[0m 74.31   \u001b[0m | \u001b[0m 5.19e+03\u001b[0m | \u001b[0m 0.9048  \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m 0.8381  \u001b[0m | \u001b[0m 0.6492  \u001b[0m | \u001b[0m 66.79   \u001b[0m | \u001b[0m 4.022   \u001b[0m | \u001b[0m 80.77   \u001b[0m | \u001b[0m 164.3   \u001b[0m | \u001b[0m 0.6041  \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m 0.8416  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 289.9   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 8.454e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[95m 110     \u001b[0m | \u001b[95m 0.8444  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 90.0    \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 48.69   \u001b[0m | \u001b[95m 701.5   \u001b[0m | \u001b[95m 0.4     \u001b[0m |\n",
      "| \u001b[0m 111     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.696e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 112     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4824  \u001b[0m | \u001b[0m 3.647   \u001b[0m | \u001b[0m 69.14   \u001b[0m | \u001b[0m 81.11   \u001b[0m | \u001b[0m 7.009e+0\u001b[0m | \u001b[0m 0.4516  \u001b[0m |\n",
      "| \u001b[0m 113     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.033e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 114     \u001b[0m | \u001b[0m 0.8431  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 132.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 115     \u001b[0m | \u001b[0m 0.8425  \u001b[0m | \u001b[0m 0.6275  \u001b[0m | \u001b[0m 3.343   \u001b[0m | \u001b[0m 57.13   \u001b[0m | \u001b[0m 89.87   \u001b[0m | \u001b[0m 71.32   \u001b[0m | \u001b[0m 0.9498  \u001b[0m |\n",
      "| \u001b[0m 116     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 3.34e+03\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 117     \u001b[0m | \u001b[0m 0.6521  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.313e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 118     \u001b[0m | \u001b[0m 0.8413  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 145.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 119     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 3.955e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 120     \u001b[0m | \u001b[0m 0.6093  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.162e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 121     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 1.618e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 122     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.671e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 123     \u001b[0m | \u001b[0m 0.7177  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.925e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 124     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 4.865e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 125     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.979e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 126     \u001b[0m | \u001b[0m 0.7608  \u001b[0m | \u001b[0m 0.9549  \u001b[0m | \u001b[0m 15.05   \u001b[0m | \u001b[0m 2.3     \u001b[0m | \u001b[0m 8.681   \u001b[0m | \u001b[0m 305.9   \u001b[0m | \u001b[0m 0.9465  \u001b[0m |\n",
      "| \u001b[0m 127     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 9.663e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[95m 128     \u001b[0m | \u001b[95m 0.8452  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 90.0    \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 90.0    \u001b[0m | \u001b[95m 702.8   \u001b[0m | \u001b[95m 0.4     \u001b[0m |\n",
      "| \u001b[0m 129     \u001b[0m | \u001b[0m 0.7643  \u001b[0m | \u001b[0m 0.4063  \u001b[0m | \u001b[0m 82.1    \u001b[0m | \u001b[0m 4.4     \u001b[0m | \u001b[0m 79.71   \u001b[0m | \u001b[0m 315.7   \u001b[0m | \u001b[0m 0.8586  \u001b[0m |\n",
      "| \u001b[0m 130     \u001b[0m | \u001b[0m 0.836   \u001b[0m | \u001b[0m 0.7051  \u001b[0m | \u001b[0m 14.74   \u001b[0m | \u001b[0m 11.65   \u001b[0m | \u001b[0m 83.95   \u001b[0m | \u001b[0m 87.86   \u001b[0m | \u001b[0m 0.6302  \u001b[0m |\n",
      "| \u001b[0m 131     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 1.968e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 132     \u001b[0m | \u001b[0m 0.8389  \u001b[0m | \u001b[0m 0.5487  \u001b[0m | \u001b[0m 53.08   \u001b[0m | \u001b[0m 50.14   \u001b[0m | \u001b[0m 85.97   \u001b[0m | \u001b[0m 26.52   \u001b[0m | \u001b[0m 0.9184  \u001b[0m |\n",
      "| \u001b[0m 133     \u001b[0m | \u001b[0m 0.7494  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.446e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 134     \u001b[0m | \u001b[0m 0.8066  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 979.1   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 135     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8919  \u001b[0m | \u001b[0m 87.07   \u001b[0m | \u001b[0m 30.98   \u001b[0m | \u001b[0m 87.74   \u001b[0m | \u001b[0m 2.57e+03\u001b[0m | \u001b[0m 0.5917  \u001b[0m |\n",
      "| \u001b[0m 136     \u001b[0m | \u001b[0m 0.8398  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 23.71   \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 119.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 137     \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 1.003e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 138     \u001b[0m | \u001b[0m 0.8144  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.045e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 139     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 6.937e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 140     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.868e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 141     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.6843  \u001b[0m | \u001b[0m 89.6    \u001b[0m | \u001b[0m 55.23   \u001b[0m | \u001b[0m 74.99   \u001b[0m | \u001b[0m 1.022e+0\u001b[0m | \u001b[0m 0.9345  \u001b[0m |\n",
      "| \u001b[95m 142     \u001b[0m | \u001b[95m 0.8496  \u001b[0m | \u001b[95m 0.5594  \u001b[0m | \u001b[95m 68.76   \u001b[0m | \u001b[95m 0.4753  \u001b[0m | \u001b[95m 5.657   \u001b[0m | \u001b[95m 47.63   \u001b[0m | \u001b[95m 0.8616  \u001b[0m |\n",
      "| \u001b[0m 143     \u001b[0m | \u001b[0m 0.8095  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.08e+03\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 144     \u001b[0m | \u001b[0m 0.6803  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 9.23e+03\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 145     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.174e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 146     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.338e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 147     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8915  \u001b[0m | \u001b[0m 28.23   \u001b[0m | \u001b[0m 3.197   \u001b[0m | \u001b[0m 5.042   \u001b[0m | \u001b[0m 1.017e+0\u001b[0m | \u001b[0m 0.9589  \u001b[0m |\n",
      "| \u001b[0m 148     \u001b[0m | \u001b[0m 0.8343  \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 85.16   \u001b[0m | \u001b[0m 66.68   \u001b[0m | \u001b[0m 13.24   \u001b[0m | \u001b[0m 71.81   \u001b[0m | \u001b[0m 0.7239  \u001b[0m |\n",
      "| \u001b[0m 149     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.075e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 150     \u001b[0m | \u001b[0m 0.6494  \u001b[0m | \u001b[0m 0.6106  \u001b[0m | \u001b[0m 88.92   \u001b[0m | \u001b[0m 7.155   \u001b[0m | \u001b[0m 8.264   \u001b[0m | \u001b[0m 359.5   \u001b[0m | \u001b[0m 0.8247  \u001b[0m |\n",
      "| \u001b[0m 151     \u001b[0m | \u001b[0m 0.8315  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 47.87   \u001b[0m | \u001b[0m 41.16   \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 253.3   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 152     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.897e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 153     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.856   \u001b[0m | \u001b[0m 87.7    \u001b[0m | \u001b[0m 8.81    \u001b[0m | \u001b[0m 87.54   \u001b[0m | \u001b[0m 9.315e+0\u001b[0m | \u001b[0m 0.5822  \u001b[0m |\n",
      "| \u001b[0m 154     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4754  \u001b[0m | \u001b[0m 2.642   \u001b[0m | \u001b[0m 0.8511  \u001b[0m | \u001b[0m 61.16   \u001b[0m | \u001b[0m 9.115e+0\u001b[0m | \u001b[0m 0.8772  \u001b[0m |\n",
      "| \u001b[0m 155     \u001b[0m | \u001b[0m 0.8434  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 56.34   \u001b[0m | \u001b[0m 119.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 156     \u001b[0m | \u001b[0m 0.7045  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.426e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 157     \u001b[0m | \u001b[0m 0.827   \u001b[0m | \u001b[0m 0.8577  \u001b[0m | \u001b[0m 7.526   \u001b[0m | \u001b[0m 0.4351  \u001b[0m | \u001b[0m 9.564   \u001b[0m | \u001b[0m 60.56   \u001b[0m | \u001b[0m 0.445   \u001b[0m |\n",
      "| \u001b[0m 158     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.294e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 159     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.101e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 160     \u001b[0m | \u001b[0m 0.7377  \u001b[0m | \u001b[0m 0.7111  \u001b[0m | \u001b[0m 2.911   \u001b[0m | \u001b[0m 59.99   \u001b[0m | \u001b[0m 47.74   \u001b[0m | \u001b[0m 285.9   \u001b[0m | \u001b[0m 0.421   \u001b[0m |\n",
      "| \u001b[0m 161     \u001b[0m | \u001b[0m 0.7747  \u001b[0m | \u001b[0m 0.7411  \u001b[0m | \u001b[0m 64.51   \u001b[0m | \u001b[0m 67.71   \u001b[0m | \u001b[0m 5.283   \u001b[0m | \u001b[0m 265.7   \u001b[0m | \u001b[0m 0.4043  \u001b[0m |\n",
      "| \u001b[0m 162     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4898  \u001b[0m | \u001b[0m 89.27   \u001b[0m | \u001b[0m 68.95   \u001b[0m | \u001b[0m 10.24   \u001b[0m | \u001b[0m 2.74e+03\u001b[0m | \u001b[0m 0.4586  \u001b[0m |\n",
      "| \u001b[0m 163     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4672  \u001b[0m | \u001b[0m 10.22   \u001b[0m | \u001b[0m 62.73   \u001b[0m | \u001b[0m 5.674   \u001b[0m | \u001b[0m 2.264e+0\u001b[0m | \u001b[0m 0.9853  \u001b[0m |\n",
      "| \u001b[0m 164     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.149e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 165     \u001b[0m | \u001b[0m 0.837   \u001b[0m | \u001b[0m 0.4232  \u001b[0m | \u001b[0m 89.49   \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 60.84   \u001b[0m | \u001b[0m 38.65   \u001b[0m | \u001b[0m 0.4679  \u001b[0m |\n",
      "| \u001b[0m 166     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.428e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 167     \u001b[0m | \u001b[0m 0.7708  \u001b[0m | \u001b[0m 0.9168  \u001b[0m | \u001b[0m 48.85   \u001b[0m | \u001b[0m 69.59   \u001b[0m | \u001b[0m 33.3    \u001b[0m | \u001b[0m 165.8   \u001b[0m | \u001b[0m 0.4485  \u001b[0m |\n",
      "| \u001b[0m 168     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.701e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 169     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.895e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 170     \u001b[0m | \u001b[0m 0.7233  \u001b[0m | \u001b[0m 0.6954  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.906e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 171     \u001b[0m | \u001b[0m 0.8116  \u001b[0m | \u001b[0m 0.8002  \u001b[0m | \u001b[0m 86.88   \u001b[0m | \u001b[0m 24.18   \u001b[0m | \u001b[0m 16.7    \u001b[0m | \u001b[0m 230.2   \u001b[0m | \u001b[0m 0.8269  \u001b[0m |\n",
      "| \u001b[0m 172     \u001b[0m | \u001b[0m 0.7598  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 349.4   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 173     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 4.473e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 174     \u001b[0m | \u001b[0m 0.7785  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.523e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 175     \u001b[0m | \u001b[0m 0.8335  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 37.07   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 217.1   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 176     \u001b[0m | \u001b[0m 0.8408  \u001b[0m | \u001b[0m 0.8572  \u001b[0m | \u001b[0m 81.64   \u001b[0m | \u001b[0m 68.35   \u001b[0m | \u001b[0m 86.84   \u001b[0m | \u001b[0m 208.3   \u001b[0m | \u001b[0m 0.8048  \u001b[0m |\n",
      "| \u001b[0m 177     \u001b[0m | \u001b[0m 0.8415  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 45.24   \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 96.55   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 178     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 4.68e+03\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 179     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.468e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 180     \u001b[0m | \u001b[0m 0.6518  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 7.005e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 181     \u001b[0m | \u001b[0m 0.6566  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 8.249e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 182     \u001b[0m | \u001b[0m 0.7018  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.523e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 183     \u001b[0m | \u001b[0m 0.6782  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 5.572e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 184     \u001b[0m | \u001b[0m 0.5695  \u001b[0m | \u001b[0m 0.9208  \u001b[0m | \u001b[0m 89.33   \u001b[0m | \u001b[0m 1.063   \u001b[0m | \u001b[0m 89.85   \u001b[0m | \u001b[0m 5.531e+0\u001b[0m | \u001b[0m 0.7758  \u001b[0m |\n",
      "| \u001b[0m 185     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.474e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 186     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.6234  \u001b[0m | \u001b[0m 46.47   \u001b[0m | \u001b[0m 2.148   \u001b[0m | \u001b[0m 87.98   \u001b[0m | \u001b[0m 670.8   \u001b[0m | \u001b[0m 0.9807  \u001b[0m |\n",
      "| \u001b[0m 187     \u001b[0m | \u001b[0m 0.5978  \u001b[0m | \u001b[0m 0.8717  \u001b[0m | \u001b[0m 5.314   \u001b[0m | \u001b[0m 64.29   \u001b[0m | \u001b[0m 5.839   \u001b[0m | \u001b[0m 449.3   \u001b[0m | \u001b[0m 0.6559  \u001b[0m |\n",
      "| \u001b[0m 188     \u001b[0m | \u001b[0m 0.766   \u001b[0m | \u001b[0m 0.4035  \u001b[0m | \u001b[0m 84.44   \u001b[0m | \u001b[0m 2.612   \u001b[0m | \u001b[0m 63.39   \u001b[0m | \u001b[0m 262.7   \u001b[0m | \u001b[0m 0.7659  \u001b[0m |\n",
      "| \u001b[0m 189     \u001b[0m | \u001b[0m 0.845   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 31.54   \u001b[0m | \u001b[0m 48.04   \u001b[0m | \u001b[0m 172.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 190     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4726  \u001b[0m | \u001b[0m 88.91   \u001b[0m | \u001b[0m 12.73   \u001b[0m | \u001b[0m 8.924   \u001b[0m | \u001b[0m 653.1   \u001b[0m | \u001b[0m 0.9197  \u001b[0m |\n",
      "| \u001b[0m 191     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.651e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 192     \u001b[0m | \u001b[0m 0.8074  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 1.082e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 193     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 6.862e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 194     \u001b[0m | \u001b[0m 0.6495  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 1e+04   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 195     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 3.754e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 196     \u001b[0m | \u001b[0m 0.8383  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 32.35   \u001b[0m | \u001b[0m 749.0   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 197     \u001b[0m | \u001b[0m 0.836   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 156.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 198     \u001b[0m | \u001b[0m 0.825   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 41.59   \u001b[0m | \u001b[0m 132.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 199     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.702e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 200     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.384e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 201     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 2.128e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 202     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.238e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 203     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.313e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 204     \u001b[0m | \u001b[0m 0.5834  \u001b[0m | \u001b[0m 0.9129  \u001b[0m | \u001b[0m 61.36   \u001b[0m | \u001b[0m 3.482   \u001b[0m | \u001b[0m 75.99   \u001b[0m | \u001b[0m 4.231e+0\u001b[0m | \u001b[0m 0.7966  \u001b[0m |\n",
      "| \u001b[0m 205     \u001b[0m | \u001b[0m 0.7642  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 385.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=================================================================================================\n",
      "[{'target': 0.5, 'params': {'colSam': 0.797215042933678, 'maxDepth': 86.50433485762665, 'minChildWeight': 35.769833101035246, 'numLeaves': 63.266825125793744, 'scaleWeight': 9806.891996478165, 'subsample': 0.4328398988446089}}, {'target': 0.5, 'params': {'colSam': 0.7383567469067223, 'maxDepth': 56.1431396792129, 'minChildWeight': 69.71926284391516, 'numLeaves': 67.78608326905163, 'scaleWeight': 1799.8679654522603, 'subsample': 0.5788326610317268}}, {'target': 0.8371595926887361, 'params': {'colSam': 0.8568625882796963, 'maxDepth': 34.0163556161186, 'minChildWeight': 39.03070214276818, 'numLeaves': 54.98046721580537, 'scaleWeight': 117.15343135491054, 'subsample': 0.6856838683426101}}, {'target': 0.5, 'params': {'colSam': 0.9603306346116159, 'maxDepth': 49.610868692556664, 'minChildWeight': 45.76874972076309, 'numLeaves': 37.82719577311491, 'scaleWeight': 6511.249117721922, 'subsample': 0.705118599237889}}, {'target': 0.8360145315385233, 'params': {'colSam': 0.5605659834677535, 'maxDepth': 21.07185279585078, 'minChildWeight': 39.73733538163984, 'numLeaves': 15.941219085371948, 'scaleWeight': 125.2547838729252, 'subsample': 0.8264690527647856}}, {'target': 0.7810275109053443, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4256.4381987283905, 'subsample': 0.4}}, {'target': 0.7135231621484197, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 3383.003104447091, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 5056.484016199423, 'subsample': 0.4}}, {'target': 0.8270971713008806, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 649.7595446284728, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 20.77363448222064, 'numLeaves': 5.0, 'scaleWeight': 8133.79926596438, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 3863.7280338196924, 'subsample': 0.4}}, {'target': 0.8440042684721814, 'params': {'colSam': 0.40943895371327965, 'maxDepth': 24.157682719503367, 'minChildWeight': 20.312686077948932, 'numLeaves': 21.47655754320299, 'scaleWeight': 124.51062602141626, 'subsample': 0.8953235192046871}}, {'target': 0.666701099900982, 'params': {'colSam': 0.48915457740591517, 'maxDepth': 3.1037853184650697, 'minChildWeight': 14.836812243615773, 'numLeaves': 67.27371604258995, 'scaleWeight': 370.83114810938156, 'subsample': 0.6031703845544093}}, {'target': 0.7668094508523563, 'params': {'colSam': 0.8779018306360259, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 74.94184282939757, 'scaleWeight': 910.803016727243, 'subsample': 0.6262623612622418}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 4494.444856555245, 'subsample': 1.0}}, {'target': 0.8372682447078972, 'params': {'colSam': 0.997386594744409, 'maxDepth': 23.968168013647247, 'minChildWeight': 21.505841037478845, 'numLeaves': 18.81664152194987, 'scaleWeight': 114.99187466700802, 'subsample': 0.541219296575423}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 748.0589156917276, 'subsample': 1.0}}, {'target': 0.8373756924559104, 'params': {'colSam': 0.8931828070826657, 'maxDepth': 31.27500309532356, 'minChildWeight': 34.227491970234624, 'numLeaves': 18.41031077337348, 'scaleWeight': 127.97093522561734, 'subsample': 0.6738082426423184}}, {'target': 0.5, 'params': {'colSam': 0.841066092001493, 'maxDepth': 89.00282947812477, 'minChildWeight': 7.518428298638422, 'numLeaves': 66.67221030679785, 'scaleWeight': 1147.706611507983, 'subsample': 0.7092227061811587}}, {'target': 0.8369468381191961, 'params': {'colSam': 0.5856631159841712, 'maxDepth': 18.91992123776804, 'minChildWeight': 33.73588156643416, 'numLeaves': 19.623595953769666, 'scaleWeight': 122.13833708336492, 'subsample': 0.48590223222766155}}, {'target': 0.8389595097278347, 'params': {'colSam': 0.7749784438098424, 'maxDepth': 23.73206027706634, 'minChildWeight': 40.2694204768056, 'numLeaves': 48.646970464545724, 'scaleWeight': 121.85194649675654, 'subsample': 0.6416830454316147}}, {'target': 0.715323915486927, 'params': {'colSam': 0.5512972467058296, 'maxDepth': 78.52321423930958, 'minChildWeight': 2.589770045772786, 'numLeaves': 26.96956825152563, 'scaleWeight': 3016.643036625232, 'subsample': 0.4604882718074247}}, {'target': 0.6021897998233735, 'params': {'colSam': 0.9257897098435411, 'maxDepth': 8.182666783092671, 'minChildWeight': 4.401377944735658, 'numLeaves': 33.763240636013876, 'scaleWeight': 2646.297621329839, 'subsample': 0.728975575889814}}, {'target': 0.5, 'params': {'colSam': 0.44479427323670256, 'maxDepth': 80.53987508502779, 'minChildWeight': 7.008247901098254, 'numLeaves': 84.18952687243943, 'scaleWeight': 8955.59596175939, 'subsample': 0.9687886725903655}}, {'target': 0.8376386249899644, 'params': {'colSam': 0.5334780150040057, 'maxDepth': 23.587116031387048, 'minChildWeight': 21.028050995770027, 'numLeaves': 24.428178042525403, 'scaleWeight': 114.69839600656042, 'subsample': 0.5984820687485721}}, {'target': 0.5, 'params': {'colSam': 0.46026997995199537, 'maxDepth': 82.23015655704565, 'minChildWeight': 67.75699256034191, 'numLeaves': 87.77911168484381, 'scaleWeight': 7331.972034715822, 'subsample': 0.4634495251019937}}, {'target': 0.5, 'params': {'colSam': 0.9666175966769146, 'maxDepth': 9.352997640094515, 'minChildWeight': 16.856989490670337, 'numLeaves': 73.98609883725301, 'scaleWeight': 5779.851953132475, 'subsample': 0.831332169330129}}, {'target': 0.5410067706800118, 'params': {'colSam': 0.6580649953856827, 'maxDepth': 80.84284474431223, 'minChildWeight': 2.3354153932455888, 'numLeaves': 89.8975981197381, 'scaleWeight': 2251.2845963379286, 'subsample': 0.8270033635973668}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 3209.106696022417, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4949218540621216, 'maxDepth': 8.483043530550349, 'minChildWeight': 65.89896165323027, 'numLeaves': 20.601572564471674, 'scaleWeight': 9373.847888963683, 'subsample': 0.4354426799505445}}, {'target': 0.5, 'params': {'colSam': 0.5659935388608257, 'maxDepth': 71.25263068773343, 'minChildWeight': 38.84675673759456, 'numLeaves': 5.257771801276419, 'scaleWeight': 4141.754287699751, 'subsample': 0.6000227813345866}}, {'target': 0.8333409826852571, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 222.229549684919, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.8956508140692675, 'maxDepth': 71.9945859217369, 'minChildWeight': 67.2193194945912, 'numLeaves': 6.783972547056094, 'scaleWeight': 8537.883177109205, 'subsample': 0.6269302181842945}}, {'target': 0.726992600422833, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 6924.597377389897, 'subsample': 0.49107509957762085}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 7734.432805457356, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 3567.3062068657123, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 2854.898696817365, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 530.6155943404093, 'subsample': 1.0}}, {'target': 0.8298962988733376, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 759.6750581676221, 'subsample': 0.4}}, {'target': 0.8377624642063853, 'params': {'colSam': 0.6232001069787123, 'maxDepth': 17.51874015258677, 'minChildWeight': 49.6642803784717, 'numLeaves': 8.002901077744337, 'scaleWeight': 247.91498461654402, 'subsample': 0.9370527650992293}}, {'target': 0.5, 'params': {'colSam': 0.7146140932990042, 'maxDepth': 82.42463507640682, 'minChildWeight': 68.8125061513588, 'numLeaves': 40.91433038465509, 'scaleWeight': 6138.141407102892, 'subsample': 0.8670794547035342}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 6825.175321908876, 'subsample': 1.0}}, {'target': 0.8353673696041962, 'params': {'colSam': 0.4964452280126311, 'maxDepth': 3.1591013433702715, 'minChildWeight': 54.20334045766, 'numLeaves': 82.30485165766655, 'scaleWeight': 222.2977792623627, 'subsample': 0.6759017429429903}}, {'target': 0.5, 'params': {'colSam': 0.8348053262482567, 'maxDepth': 26.941643332021364, 'minChildWeight': 1.6685557754607778, 'numLeaves': 77.40830553128548, 'scaleWeight': 5397.205074769604, 'subsample': 0.8759358178094057}}, {'target': 0.710495490673589, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 7086.91943989827, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4843570794291702, 'maxDepth': 10.264292677634451, 'minChildWeight': 9.049629733985428, 'numLeaves': 6.164669656257638, 'scaleWeight': 1512.0547501576707, 'subsample': 0.9682309476432993}}, {'target': 0.8279734458211792, 'params': {'colSam': 0.7936137588448049, 'maxDepth': 87.96706860609785, 'minChildWeight': 64.68925650698132, 'numLeaves': 52.806643583950176, 'scaleWeight': 6.408325896140651, 'subsample': 0.6478189645193414}}, {'target': 0.8344646345706105, 'params': {'colSam': 0.4175334103042353, 'maxDepth': 15.172942731516386, 'minChildWeight': 7.428232454228968, 'numLeaves': 81.63914330997262, 'scaleWeight': 13.27391968056802, 'subsample': 0.48889380592337806}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4776.440586430256, 'subsample': 1.0}}, {'target': 0.8401225011373672, 'params': {'colSam': 0.5115559790536702, 'maxDepth': 29.64087991513589, 'minChildWeight': 3.4066708720395367, 'numLeaves': 6.248233368569183, 'scaleWeight': 7.469998783983744, 'subsample': 0.9173148553436229}}, {'target': 0.7813593210586881, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4371.575662090927, 'subsample': 0.4}}, {'target': 0.7705684159820162, 'params': {'colSam': 0.5203359272241255, 'maxDepth': 15.800135807392293, 'minChildWeight': 1.234000391550006, 'numLeaves': 25.653686491022533, 'scaleWeight': 210.743440395611, 'subsample': 0.5013565067059148}}, {'target': 0.7182436374340996, 'params': {'colSam': 0.5099228097716159, 'maxDepth': 83.777744433353, 'minChildWeight': 69.69518560517255, 'numLeaves': 62.1475998690819, 'scaleWeight': 294.721589829528, 'subsample': 0.5650834937790608}}, {'target': 0.5, 'params': {'colSam': 0.45869105759881235, 'maxDepth': 10.17989427490076, 'minChildWeight': 34.860885107089366, 'numLeaves': 8.090048225148678, 'scaleWeight': 9997.759121112127, 'subsample': 0.9008629935185238}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.000000000000007, 'scaleWeight': 1.0, 'subsample': 1.0}}, {'target': 0.8430932373484625, 'params': {'colSam': 0.7276948479530948, 'maxDepth': 88.91894960453516, 'minChildWeight': 11.231750445006998, 'numLeaves': 79.70131200719835, 'scaleWeight': 64.33879859444092, 'subsample': 0.9486298408954104}}, {'target': 0.8288179409639522, 'params': {'colSam': 0.5004214440986542, 'maxDepth': 86.77362534789088, 'minChildWeight': 7.8783442951761185, 'numLeaves': 8.957549442165968, 'scaleWeight': 2.2610116706045327, 'subsample': 0.8533369805321473}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 4312.531128408362, 'subsample': 0.4}}, {'target': 0.8422646318944524, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 201.17385173017317, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 2049.3573577879524, 'subsample': 0.4}}, {'target': 0.8350917253191319, 'params': {'colSam': 0.8760841203260484, 'maxDepth': 81.022147115586, 'minChildWeight': 58.845837482553115, 'numLeaves': 89.96290728405206, 'scaleWeight': 167.12150082073506, 'subsample': 0.6886806670492855}}, {'target': 0.8240802044584793, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 1.0, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.5269438621143889, 'maxDepth': 3.741794706460425, 'minChildWeight': 47.03342994958316, 'numLeaves': 18.016271703046026, 'scaleWeight': 2449.564478455862, 'subsample': 0.6922954520066756}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 4318.86766075693, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 7057.389056451535, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 977.9030947728168, 'subsample': 0.5612593732895931}}, {'target': 0.5309096261407124, 'params': {'colSam': 0.8352598287803439, 'maxDepth': 88.02284698776496, 'minChildWeight': 62.53451737100684, 'numLeaves': 89.30853227837167, 'scaleWeight': 673.2953909470051, 'subsample': 0.6551585441533669}}, {'target': 0.6567456579334707, 'params': {'colSam': 0.8095349484271971, 'maxDepth': 88.14895039404274, 'minChildWeight': 3.0884673620812175, 'numLeaves': 28.507352701234876, 'scaleWeight': 838.4544455789885, 'subsample': 0.7306579204993857}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 7943.18907405614, 'subsample': 0.4}}, {'target': 0.8108904247062917, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 683.4710691881971, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 9571.007256020828, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.7878195134329864, 'maxDepth': 8.90882663119244, 'minChildWeight': 9.710430362889813, 'numLeaves': 87.07795681092819, 'scaleWeight': 8731.63675335484, 'subsample': 0.7587151984704019}}, {'target': 0.7196000816228223, 'params': {'colSam': 0.7571478999366261, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 8348.772385062806, 'subsample': 0.4}}, {'target': 0.8256311049856826, 'params': {'colSam': 0.4448414698718192, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 819.7649955459528, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 8297.21954665114, 'subsample': 0.4}}, {'target': 0.834363776861937, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 194.40140785221092, 'subsample': 0.4}}, {'target': 0.654680767789761, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 7540.322773950786, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.7088380083191121, 'maxDepth': 89.94040685923513, 'minChildWeight': 65.87330822505791, 'numLeaves': 18.872486115434015, 'scaleWeight': 9153.893022580232, 'subsample': 0.8279313752046797}}, {'target': 0.830803115048037, 'params': {'colSam': 0.9641417794444014, 'maxDepth': 10.038861731008106, 'minChildWeight': 8.277894439653963, 'numLeaves': 87.31386913221132, 'scaleWeight': 159.83924050647212, 'subsample': 0.6083237215612541}}, {'target': 0.5, 'params': {'colSam': 0.9518659189727463, 'maxDepth': 74.96891135047912, 'minChildWeight': 69.37440016125316, 'numLeaves': 16.150400437568386, 'scaleWeight': 5618.143252224564, 'subsample': 0.5779464630489985}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 3438.029026929832, 'subsample': 0.4}}, {'target': 0.5569991570101962, 'params': {'colSam': 0.4264215293346237, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 6311.957259312307, 'subsample': 0.6982462268346921}}, {'target': 0.8307577876200927, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 265.7971015292758, 'subsample': 1.0}}, {'target': 0.7600125779431048, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 320.56545428745824, 'subsample': 1.0}}, {'target': 0.7490394934032704, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4498.255458488057, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 2940.3491279844316, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 6724.934477425839, 'subsample': 0.9469483026258921}}, {'target': 0.5, 'params': {'colSam': 0.48305714881831957, 'maxDepth': 87.73322704684276, 'minChildWeight': 7.114070355335308, 'numLeaves': 9.78161554919314, 'scaleWeight': 3154.618807520898, 'subsample': 0.7417351768755509}}, {'target': 0.8419457007520004, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 63.09230221617349, 'subsample': 1.0}}, {'target': 0.8347631600074932, 'params': {'colSam': 0.7074329880166401, 'maxDepth': 89.52487004889689, 'minChildWeight': 4.830804967301298, 'numLeaves': 7.925869623735894, 'scaleWeight': 187.4236230696354, 'subsample': 0.6664646740727536}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 907.7643079156101, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.6993785486108712, 'maxDepth': 6.947578016281062, 'minChildWeight': 69.49549404517067, 'numLeaves': 83.53632287835705, 'scaleWeight': 1353.392732251613, 'subsample': 0.892097628214668}}, {'target': 0.6350426847218134, 'params': {'colSam': 0.5387550082436772, 'maxDepth': 7.911645637375482, 'minChildWeight': 2.889912933204087, 'numLeaves': 84.60621454049146, 'scaleWeight': 773.1106750928952, 'subsample': 0.8112395140753696}}, {'target': 0.5, 'params': {'colSam': 0.7383973614321379, 'maxDepth': 2.4688677132183408, 'minChildWeight': 12.733287101414726, 'numLeaves': 35.27251095363172, 'scaleWeight': 610.7857095813024, 'subsample': 0.4919603209131558}}, {'target': 0.8306670993122275, 'params': {'colSam': 0.5667366189018019, 'maxDepth': 44.51053514767847, 'minChildWeight': 0.49082362353628667, 'numLeaves': 56.96520201820472, 'scaleWeight': 35.51573053389881, 'subsample': 0.5824057386534182}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 61.878383531034764, 'numLeaves': 90.0, 'scaleWeight': 848.7210154848096, 'subsample': 0.4}}, {'target': 0.8423589664677389, 'params': {'colSam': 0.4, 'maxDepth': 57.50259849167115, 'minChildWeight': 54.27388366133429, 'numLeaves': 50.44031109942919, 'scaleWeight': 213.84834898095028, 'subsample': 1.0}}, {'target': 0.8336208419193406, 'params': {'colSam': 0.4, 'maxDepth': 21.561869609792737, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 274.0325635574267, 'subsample': 1.0}}, {'target': 0.8365748521422646, 'params': {'colSam': 0.7364372784470343, 'maxDepth': 88.48585390170437, 'minChildWeight': 22.806070825324188, 'numLeaves': 15.482510112118904, 'scaleWeight': 98.25441344271856, 'subsample': 0.4898109731516255}}, {'target': 0.5, 'params': {'colSam': 0.8953517672250667, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 7527.073575624758, 'subsample': 0.4}}, {'target': 0.6962461530227206, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 5970.774769811081, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.40419638525505963, 'maxDepth': 78.8638997783289, 'minChildWeight': 43.227968257024344, 'numLeaves': 82.56321785654094, 'scaleWeight': 5944.06139175569, 'subsample': 0.5811826969450848}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 7203.497120788633, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 4433.565434471366, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.7132960308927443, 'maxDepth': 5.503749913539727, 'minChildWeight': 10.782902322831903, 'numLeaves': 88.28128912746983, 'scaleWeight': 4596.1641911421875, 'subsample': 0.5098611526040331}}, {'target': 0.5, 'params': {'colSam': 0.6957104071221969, 'maxDepth': 7.342529501386285, 'minChildWeight': 7.882731776445366, 'numLeaves': 74.30586788210333, 'scaleWeight': 5190.202732723292, 'subsample': 0.9048320035427116}}, {'target': 0.8381125056868359, 'params': {'colSam': 0.649230608232698, 'maxDepth': 66.78912181449593, 'minChildWeight': 4.021765271483151, 'numLeaves': 80.77291270725385, 'scaleWeight': 164.27963438262756, 'subsample': 0.6040528675075816}}, {'target': 0.8415505994594161, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 289.8573900902708, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 8454.173459235128, 'subsample': 1.0}}, {'target': 0.8443913078384672, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 48.6934847598728, 'scaleWeight': 701.4675449272929, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 3695.583896777436, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4823951932594899, 'maxDepth': 3.6465118391683164, 'minChildWeight': 69.14004455336637, 'numLeaves': 81.11090173209861, 'scaleWeight': 7008.644929958561, 'subsample': 0.45160160652797426}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 3032.6873430287756, 'subsample': 0.4}}, {'target': 0.84309865656863, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 132.23281242989097, 'subsample': 1.0}}, {'target': 0.842470227741055, 'params': {'colSam': 0.6274881700202324, 'maxDepth': 3.343023067788148, 'minChildWeight': 57.12795689138936, 'numLeaves': 89.87030966147353, 'scaleWeight': 71.32227107668666, 'subsample': 0.9498460929429292}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 3339.654784684938, 'subsample': 1.0}}, {'target': 0.652096836781117, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 8313.43410317404, 'subsample': 0.4}}, {'target': 0.8412633807905372, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 145.69791289798903, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 3955.164978223102, 'subsample': 1.0}}, {'target': 0.6093262102925041, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4161.983371564589, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 1618.261799818787, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 7670.77883300847, 'subsample': 1.0}}, {'target': 0.7176770947627585, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4924.8421586724235, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 4864.938345169005, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4979.333212622398, 'subsample': 1.0}}, {'target': 0.7607768886985842, 'params': {'colSam': 0.9549429249188647, 'maxDepth': 15.050268215975018, 'minChildWeight': 2.3002867817446755, 'numLeaves': 8.680523695420895, 'scaleWeight': 305.93790047582104, 'subsample': 0.9465258229004672}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 9663.05798476411, 'subsample': 0.4}}, {'target': 0.8451920143442073, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 702.8330155883361, 'subsample': 0.4}}, {'target': 0.7642987127679504, 'params': {'colSam': 0.4062544102580588, 'maxDepth': 82.10273997804428, 'minChildWeight': 4.400078528562394, 'numLeaves': 79.71450817205587, 'scaleWeight': 315.6858223341293, 'subsample': 0.8585568489648618}}, {'target': 0.835993122273664, 'params': {'colSam': 0.7050562946788785, 'maxDepth': 14.739759522630909, 'minChildWeight': 11.654463122055045, 'numLeaves': 83.95460063744365, 'scaleWeight': 87.8628408773716, 'subsample': 0.6302478060384636}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 1967.8825548656475, 'subsample': 1.0}}, {'target': 0.8389354912088207, 'params': {'colSam': 0.5487279042477097, 'maxDepth': 53.08149208597921, 'minChildWeight': 50.14031101284301, 'numLeaves': 85.9676718279158, 'scaleWeight': 26.518632434076352, 'subsample': 0.9184219663239896}}, {'target': 0.7494243249391175, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4446.366655146315, 'subsample': 0.4}}, {'target': 0.8066466400834962, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 979.0969294738126, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.8918904750666803, 'maxDepth': 87.07444524879391, 'minChildWeight': 30.978040774955783, 'numLeaves': 87.73872802054639, 'scaleWeight': 2570.451685797781, 'subsample': 0.5916526406331395}}, {'target': 0.8397703856343833, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 23.708905631348888, 'numLeaves': 90.0, 'scaleWeight': 119.19553759246753, 'subsample': 1.0}}, {'target': 0.8097279016244279, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 1002.7148254120318, 'subsample': 0.4}}, {'target': 0.8144279712045389, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 1045.3206589395006, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 6937.454623320655, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 8867.810598121812, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.6842543303532564, 'maxDepth': 89.60494494063623, 'minChildWeight': 55.234515912257194, 'numLeaves': 74.98668421801555, 'scaleWeight': 1022.1186126504808, 'subsample': 0.9344840842242405}}, {'target': 0.8496254047689138, 'params': {'colSam': 0.5593590555658796, 'maxDepth': 68.76292283106957, 'minChildWeight': 0.47531461929432267, 'numLeaves': 5.656841739486678, 'scaleWeight': 47.62676436798938, 'subsample': 0.8616359953595865}}, {'target': 0.8094799555757755, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 1080.214949284999, 'subsample': 0.4}}, {'target': 0.6803079588942114, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 9229.845822782743, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 1173.7967164758672, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 6338.28370330582, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.8914895350204624, 'maxDepth': 28.227253907966364, 'minChildWeight': 3.197159588491308, 'numLeaves': 5.041758813702371, 'scaleWeight': 1017.4740461559375, 'subsample': 0.9588912406319382}}, {'target': 0.8342746273449837, 'params': {'colSam': 0.997932692939427, 'maxDepth': 85.15597876712809, 'minChildWeight': 66.68099232867561, 'numLeaves': 13.242696656551836, 'scaleWeight': 71.8084213442262, 'subsample': 0.7238676365821626}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 6074.948845257711, 'subsample': 1.0}}, {'target': 0.6493718387882356, 'params': {'colSam': 0.6105626400133627, 'maxDepth': 88.91672602093996, 'minChildWeight': 7.15546653111977, 'numLeaves': 8.263728333379472, 'scaleWeight': 359.50922598536937, 'subsample': 0.8246803197198834}}, {'target': 0.8314989830599192, 'params': {'colSam': 1.0, 'maxDepth': 47.87171995521846, 'minChildWeight': 41.161265181251245, 'numLeaves': 90.0, 'scaleWeight': 253.34111859569282, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 7897.176917732872, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.8560142811661406, 'maxDepth': 87.70190803204456, 'minChildWeight': 8.80971156644644, 'numLeaves': 87.54078407664883, 'scaleWeight': 9315.090684044568, 'subsample': 0.5822200212389159}}, {'target': 0.5, 'params': {'colSam': 0.4753549070446569, 'maxDepth': 2.642214085262201, 'minChildWeight': 0.8510764424848788, 'numLeaves': 61.164486887470446, 'scaleWeight': 9115.119607783552, 'subsample': 0.8771591988426648}}, {'target': 0.8433832659833543, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 56.34324457221259, 'scaleWeight': 119.61192592981348, 'subsample': 1.0}}, {'target': 0.7045203655631975, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 7425.917727271907, 'subsample': 0.4}}, {'target': 0.8270275911900876, 'params': {'colSam': 0.8576841852149688, 'maxDepth': 7.525827802852545, 'minChildWeight': 0.4350643919736462, 'numLeaves': 9.56405402561576, 'scaleWeight': 60.562621354302365, 'subsample': 0.4450208536627181}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 5294.470148980985, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 1101.0650533100873, 'subsample': 0.4}}, {'target': 0.7376612385259721, 'params': {'colSam': 0.7111443319155276, 'maxDepth': 2.9105663645545716, 'minChildWeight': 59.99071828946621, 'numLeaves': 47.74417457792508, 'scaleWeight': 285.8879034337552, 'subsample': 0.42103464689691755}}, {'target': 0.7746504937511708, 'params': {'colSam': 0.7411085484597079, 'maxDepth': 64.50605167637653, 'minChildWeight': 67.71054745270091, 'numLeaves': 5.2832060073743285, 'scaleWeight': 265.747289092022, 'subsample': 0.40430654570438607}}, {'target': 0.5, 'params': {'colSam': 0.4897774835239151, 'maxDepth': 89.26639274959618, 'minChildWeight': 68.94720546408202, 'numLeaves': 10.243131269419258, 'scaleWeight': 2739.738851041863, 'subsample': 0.458630718548357}}, {'target': 0.5, 'params': {'colSam': 0.46719464903525626, 'maxDepth': 10.222057388607183, 'minChildWeight': 62.7294185732084, 'numLeaves': 5.673966511263007, 'scaleWeight': 2263.7227607792966, 'subsample': 0.9852615769532916}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 7148.812229122262, 'subsample': 1.0}}, {'target': 0.837040905076672, 'params': {'colSam': 0.4231916937263943, 'maxDepth': 89.48847763502823, 'minChildWeight': 32.06520325153595, 'numLeaves': 60.84267775140783, 'scaleWeight': 38.65405121255265, 'subsample': 0.4679320009048959}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 7427.953663232113, 'subsample': 1.0}}, {'target': 0.7708169976182192, 'params': {'colSam': 0.9167965163068827, 'maxDepth': 48.854200486935554, 'minChildWeight': 69.59429736688823, 'numLeaves': 33.296687050546744, 'scaleWeight': 165.82098582288702, 'subsample': 0.4485072366521889}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 1700.9409911144, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 5894.663036539382, 'subsample': 0.4}}, {'target': 0.7232624039928278, 'params': {'colSam': 0.6953841098590341, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 6905.748776161145, 'subsample': 0.4}}, {'target': 0.8116096421976611, 'params': {'colSam': 0.8002411901323987, 'maxDepth': 86.87585876287301, 'minChildWeight': 24.18339790038684, 'numLeaves': 16.69699452995935, 'scaleWeight': 230.19722203364088, 'subsample': 0.8268885692129521}}, {'target': 0.7597555998608397, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 349.4071402308705, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 4473.287999359431, 'subsample': 1.0}}, {'target': 0.7784803436187009, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4523.476956990037, 'subsample': 0.4}}, {'target': 0.8335272432895335, 'params': {'colSam': 0.4, 'maxDepth': 37.07250915250368, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 217.07016697591925, 'subsample': 1.0}}, {'target': 0.8408097719913293, 'params': {'colSam': 0.8571549911754405, 'maxDepth': 81.63703463569513, 'minChildWeight': 68.34506989549796, 'numLeaves': 86.84398080627828, 'scaleWeight': 208.27889280685125, 'subsample': 0.804787705926271}}, {'target': 0.8415159097599487, 'params': {'colSam': 0.4, 'maxDepth': 45.2350017443619, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 96.54516652323694, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 4680.419559006411, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 7468.276267097813, 'subsample': 1.0}}, {'target': 0.6518218280836032, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 7004.759764544308, 'subsample': 0.4}}, {'target': 0.6565735809671635, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 8249.499397741105, 'subsample': 0.4}}, {'target': 0.7018473519415528, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 5523.412259450534, 'subsample': 0.4}}, {'target': 0.6782122260283139, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 5572.486339241167, 'subsample': 0.4}}, {'target': 0.5695440830144245, 'params': {'colSam': 0.92077900984587, 'maxDepth': 89.33074300763934, 'minChildWeight': 1.0631552954008976, 'numLeaves': 89.85229340896734, 'scaleWeight': 5530.782832274734, 'subsample': 0.775752976024928}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 5474.0993665728365, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.6233585823284432, 'maxDepth': 46.474512210354774, 'minChildWeight': 2.148169632936082, 'numLeaves': 87.98128438833761, 'scaleWeight': 670.8043545397137, 'subsample': 0.9807067257403537}}, {'target': 0.5977991610244333, 'params': {'colSam': 0.8717201503409753, 'maxDepth': 5.314471421646896, 'minChildWeight': 64.29068168794885, 'numLeaves': 5.838638049262698, 'scaleWeight': 449.28669671713146, 'subsample': 0.6559394086538393}}, {'target': 0.7660103834934568, 'params': {'colSam': 0.4034742966216456, 'maxDepth': 84.44473003967857, 'minChildWeight': 2.611998907532005, 'numLeaves': 63.3855508596618, 'scaleWeight': 262.6869891248887, 'subsample': 0.7658634600097989}}, {'target': 0.8449660127920359, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 31.538902088948735, 'numLeaves': 48.03874410096271, 'scaleWeight': 172.23438085458253, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.472560401512867, 'maxDepth': 88.90860022440827, 'minChildWeight': 12.727691449112196, 'numLeaves': 8.924454690967334, 'scaleWeight': 653.0553661541476, 'subsample': 0.9196631362776361}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 6651.248292479945, 'subsample': 0.4}}, {'target': 0.8074456739904192, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 1082.499147052663, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 6861.717487892625, 'subsample': 0.4}}, {'target': 0.6494606872374019, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 10000.0, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 3754.385323240019, 'subsample': 1.0}}, {'target': 0.8382814381673669, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 32.35246175589857, 'scaleWeight': 748.9873980246255, 'subsample': 0.4}}, {'target': 0.8360164717531513, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 155.9859016618273, 'subsample': 1.0}}, {'target': 0.8249755800572698, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 41.58550869729, 'scaleWeight': 132.2464871179246, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 8702.291659579274, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 8383.64434405077, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 2127.889672386802, 'subsample': 0.4}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 4237.557921853503, 'subsample': 1.0}}, {'target': 0.5, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4313.279011874702, 'subsample': 1.0}}, {'target': 0.5833606858993229, 'params': {'colSam': 0.9128852594112019, 'maxDepth': 61.35791440717635, 'minChildWeight': 3.481592142087134, 'numLeaves': 75.99168805843435, 'scaleWeight': 4230.673154240471, 'subsample': 0.7966028961028533}}, {'target': 0.764172197393422, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 384.9770089000569, 'subsample': 1.0}}]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import loadtxt\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import numpy as np\n",
    "def lgb_evaluate(numLeaves, maxDepth, scaleWeight, minChildWeight, subsample, colSam):\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        objective = 'binary',\n",
    "        metric= 'auc',\n",
    "        reg_alpha= 0,\n",
    "        reg_lambda= 2,\n",
    "#       bagging_fraction= 0.999,\n",
    "        min_split_gain= 0,\n",
    "        min_child_samples= 10,\n",
    "        subsample_freq= 3,\n",
    "#       subsample_for_bin= 50000,\n",
    "#       n_estimators= 9999999,\n",
    "        n_estimators= 99,\n",
    "        num_leaves= int(numLeaves),\n",
    "        max_depth= int(maxDepth),\n",
    "        scale_pos_weight= scaleWeight,\n",
    "        min_child_weight= minChildWeight,\n",
    "        subsample= subsample,\n",
    "        colsample_bytree= colSam,\n",
    "        verbose =-1)\n",
    "    scores = cross_val_score(clf, train_x, train_y, cv=5, scoring='roc_auc')\n",
    "    return np.mean(scores)\n",
    "def bayesOpt(train_x, train_y):\n",
    "    lgbBO = BayesianOptimization(lgb_evaluate, {                                               \n",
    "                                                'numLeaves':  (5, 90),\n",
    "                                                'maxDepth': (2, 90),\n",
    "                                                'scaleWeight': (1, 10000),\n",
    "                                                'minChildWeight': (0.01, 70),\n",
    "                                                'subsample': (0.4, 1),                                               \n",
    "                                                'colSam': (0.4, 1)\n",
    "                                            })\n",
    "    lgbBO.maximize(init_points=5, n_iter=200)\n",
    "    print(lgbBO.res)\n",
    "dataset = loadtxt('C:/Users/In-Ho Lee/testAI/scikitlearn_keras_examples/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "train_x, X_test, train_y, y_test = train_test_split(X, y, test_size=0.2)\n",
    "bayesOpt(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  colSam   | maxDepth  | minChi... | numLeaves | scaleW... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-38.21   \u001b[0m | \u001b[0m 0.605   \u001b[0m | \u001b[0m 9.563   \u001b[0m | \u001b[0m 53.51   \u001b[0m | \u001b[0m 16.33   \u001b[0m | \u001b[0m 3.317e+0\u001b[0m | \u001b[0m 0.9142  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-34.82   \u001b[0m | \u001b[95m 0.4793  \u001b[0m | \u001b[95m 29.08   \u001b[0m | \u001b[95m 22.63   \u001b[0m | \u001b[95m 38.24   \u001b[0m | \u001b[95m 566.8   \u001b[0m | \u001b[95m 0.9535  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-34.67   \u001b[0m | \u001b[95m 0.6035  \u001b[0m | \u001b[95m 56.02   \u001b[0m | \u001b[95m 21.76   \u001b[0m | \u001b[95m 48.17   \u001b[0m | \u001b[95m 665.5   \u001b[0m | \u001b[95m 0.8058  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-34.31   \u001b[0m | \u001b[95m 0.5417  \u001b[0m | \u001b[95m 62.7    \u001b[0m | \u001b[95m 17.74   \u001b[0m | \u001b[95m 24.08   \u001b[0m | \u001b[95m 7.923e+0\u001b[0m | \u001b[95m 0.687   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-40.87   \u001b[0m | \u001b[0m 0.9021  \u001b[0m | \u001b[0m 8.076   \u001b[0m | \u001b[0m 63.55   \u001b[0m | \u001b[0m 86.9    \u001b[0m | \u001b[0m 1.776e+0\u001b[0m | \u001b[0m 0.9201  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-39.78   \u001b[0m | \u001b[0m 0.4473  \u001b[0m | \u001b[0m 21.19   \u001b[0m | \u001b[0m 60.05   \u001b[0m | \u001b[0m 34.88   \u001b[0m | \u001b[0m 8.73e+03\u001b[0m | \u001b[0m 0.9325  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.5158  \u001b[0m | \u001b[0m 56.04   \u001b[0m | \u001b[0m 17.36   \u001b[0m | \u001b[0m 40.49   \u001b[0m | \u001b[0m 662.5   \u001b[0m | \u001b[0m 0.5144  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-35.37   \u001b[0m | \u001b[0m 0.4365  \u001b[0m | \u001b[0m 68.08   \u001b[0m | \u001b[0m 25.57   \u001b[0m | \u001b[0m 48.15   \u001b[0m | \u001b[0m 7.776e+0\u001b[0m | \u001b[0m 0.8843  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.9599  \u001b[0m | \u001b[0m 66.87   \u001b[0m | \u001b[0m 9.295   \u001b[0m | \u001b[0m 21.11   \u001b[0m | \u001b[0m 7.928e+0\u001b[0m | \u001b[0m 0.843   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-42.21   \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 9.325   \u001b[0m | \u001b[0m 68.33   \u001b[0m | \u001b[0m 82.45   \u001b[0m | \u001b[0m 8.103e+0\u001b[0m | \u001b[0m 0.7565  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 834.4   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.015e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-42.21   \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 87.68   \u001b[0m | \u001b[0m 68.92   \u001b[0m | \u001b[0m 72.44   \u001b[0m | \u001b[0m 1.142e+0\u001b[0m | \u001b[0m 0.8845  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-42.86   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 891.9   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-34.67   \u001b[0m | \u001b[0m 0.4667  \u001b[0m | \u001b[0m 69.39   \u001b[0m | \u001b[0m 21.67   \u001b[0m | \u001b[0m 57.52   \u001b[0m | \u001b[0m 7.778e+0\u001b[0m | \u001b[0m 0.855   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.8542  \u001b[0m | \u001b[0m 87.11   \u001b[0m | \u001b[0m 11.51   \u001b[0m | \u001b[0m 87.75   \u001b[0m | \u001b[0m 7.854e+0\u001b[0m | \u001b[0m 0.6836  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 739.8   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.9184  \u001b[0m | \u001b[0m 2.799   \u001b[0m | \u001b[0m 6.152   \u001b[0m | \u001b[0m 73.43   \u001b[0m | \u001b[0m 7.838e+0\u001b[0m | \u001b[0m 0.8493  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.8593  \u001b[0m | \u001b[0m 54.61   \u001b[0m | \u001b[0m 19.7    \u001b[0m | \u001b[0m 28.1    \u001b[0m | \u001b[0m 660.1   \u001b[0m | \u001b[0m 0.8605  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-34.67   \u001b[0m | \u001b[0m 0.9556  \u001b[0m | \u001b[0m 54.86   \u001b[0m | \u001b[0m 21.32   \u001b[0m | \u001b[0m 50.27   \u001b[0m | \u001b[0m 672.9   \u001b[0m | \u001b[0m 0.6488  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-34.67   \u001b[0m | \u001b[0m 0.5093  \u001b[0m | \u001b[0m 59.13   \u001b[0m | \u001b[0m 21.19   \u001b[0m | \u001b[0m 39.45   \u001b[0m | \u001b[0m 665.2   \u001b[0m | \u001b[0m 0.8538  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.9586  \u001b[0m | \u001b[0m 84.51   \u001b[0m | \u001b[0m 19.76   \u001b[0m | \u001b[0m 89.74   \u001b[0m | \u001b[0m 7.636e+0\u001b[0m | \u001b[0m 0.8729  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 7.516e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-40.31   \u001b[0m | \u001b[0m 0.9014  \u001b[0m | \u001b[0m 87.79   \u001b[0m | \u001b[0m 62.76   \u001b[0m | \u001b[0m 9.483   \u001b[0m | \u001b[0m 7.477e+0\u001b[0m | \u001b[0m 0.4467  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-35.52   \u001b[0m | \u001b[0m 0.8281  \u001b[0m | \u001b[0m 4.489   \u001b[0m | \u001b[0m 28.19   \u001b[0m | \u001b[0m 84.41   \u001b[0m | \u001b[0m 7.68e+03\u001b[0m | \u001b[0m 0.449   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4172  \u001b[0m | \u001b[0m 6.993   \u001b[0m | \u001b[0m 11.76   \u001b[0m | \u001b[0m 55.91   \u001b[0m | \u001b[0m 371.0   \u001b[0m | \u001b[0m 0.9156  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-34.67   \u001b[0m | \u001b[0m 0.5091  \u001b[0m | \u001b[0m 82.98   \u001b[0m | \u001b[0m 21.69   \u001b[0m | \u001b[0m 40.33   \u001b[0m | \u001b[0m 259.7   \u001b[0m | \u001b[0m 0.4519  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-42.86   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 395.9   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 268.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.8445  \u001b[0m | \u001b[0m 19.9    \u001b[0m | \u001b[0m 4.888   \u001b[0m | \u001b[0m 28.5    \u001b[0m | \u001b[0m 143.9   \u001b[0m | \u001b[0m 0.8796  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-42.86   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 148.7   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 28.84   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.9561  \u001b[0m | \u001b[0m 67.82   \u001b[0m | \u001b[0m 1.022   \u001b[0m | \u001b[0m 44.35   \u001b[0m | \u001b[0m 4.708e+0\u001b[0m | \u001b[0m 0.6735  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.7712  \u001b[0m | \u001b[0m 63.55   \u001b[0m | \u001b[0m 0.048   \u001b[0m | \u001b[0m 52.81   \u001b[0m | \u001b[0m 4.586e+0\u001b[0m | \u001b[0m 0.7001  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-42.86   \u001b[0m | \u001b[0m 0.8119  \u001b[0m | \u001b[0m 9.904   \u001b[0m | \u001b[0m 69.87   \u001b[0m | \u001b[0m 7.571   \u001b[0m | \u001b[0m 4.615e+0\u001b[0m | \u001b[0m 0.7184  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 245.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.6217  \u001b[0m | \u001b[0m 79.08   \u001b[0m | \u001b[0m 7.313   \u001b[0m | \u001b[0m 38.14   \u001b[0m | \u001b[0m 4.816e+0\u001b[0m | \u001b[0m 0.8153  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.6895  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.493e+0\u001b[0m | \u001b[0m 0.6262  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-36.79   \u001b[0m | \u001b[0m 0.615   \u001b[0m | \u001b[0m 7.281   \u001b[0m | \u001b[0m 48.87   \u001b[0m | \u001b[0m 83.49   \u001b[0m | \u001b[0m 4.89e+03\u001b[0m | \u001b[0m 0.6355  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.6888  \u001b[0m | \u001b[0m 31.25   \u001b[0m | \u001b[0m 2.066   \u001b[0m | \u001b[0m 68.79   \u001b[0m | \u001b[0m 4.375e+0\u001b[0m | \u001b[0m 0.6601  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-36.17   \u001b[0m | \u001b[0m 0.9016  \u001b[0m | \u001b[0m 80.04   \u001b[0m | \u001b[0m 42.23   \u001b[0m | \u001b[0m 6.28    \u001b[0m | \u001b[0m 4.293e+0\u001b[0m | \u001b[0m 0.9937  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-34.89   \u001b[0m | \u001b[0m 0.7841  \u001b[0m | \u001b[0m 71.71   \u001b[0m | \u001b[0m 23.4    \u001b[0m | \u001b[0m 5.417   \u001b[0m | \u001b[0m 4.436e+0\u001b[0m | \u001b[0m 0.6029  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 750.5   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-41.88   \u001b[0m | \u001b[0m 0.4179  \u001b[0m | \u001b[0m 4.802   \u001b[0m | \u001b[0m 67.6    \u001b[0m | \u001b[0m 87.45   \u001b[0m | \u001b[0m 4.439e+0\u001b[0m | \u001b[0m 0.8386  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-34.64   \u001b[0m | \u001b[0m 0.7172  \u001b[0m | \u001b[0m 8.5     \u001b[0m | \u001b[0m 20.37   \u001b[0m | \u001b[0m 89.03   \u001b[0m | \u001b[0m 4.155e+0\u001b[0m | \u001b[0m 0.8185  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.5605  \u001b[0m | \u001b[0m 12.44   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.034e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-42.21   \u001b[0m | \u001b[0m 0.7659  \u001b[0m | \u001b[0m 64.32   \u001b[0m | \u001b[0m 68.74   \u001b[0m | \u001b[0m 28.71   \u001b[0m | \u001b[0m 4.084e+0\u001b[0m | \u001b[0m 0.9777  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.993   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 4.268e+0\u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.8819  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.927e+0\u001b[0m | \u001b[0m 0.4647  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 717.6   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 795.0   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 82.53   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.829e+0\u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.8048  \u001b[0m | \u001b[0m 4.153   \u001b[0m | \u001b[0m 16.85   \u001b[0m | \u001b[0m 11.0    \u001b[0m | \u001b[0m 3.809e+0\u001b[0m | \u001b[0m 0.5877  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-42.86   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 23.03   \u001b[0m | \u001b[0m 70.0    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 3.748e+0\u001b[0m | \u001b[0m 0.6123  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-34.31   \u001b[0m | \u001b[0m 0.6646  \u001b[0m | \u001b[0m 85.53   \u001b[0m | \u001b[0m 2.719   \u001b[0m | \u001b[0m 26.61   \u001b[0m | \u001b[0m 3.887e+0\u001b[0m | \u001b[0m 0.8995  \u001b[0m |\n",
      "=================================================================================================\n",
      "[{'target': -38.212221463503866, 'params': {'colSam': 0.6050379639556435, 'maxDepth': 9.563369770402062, 'minChildWeight': 53.510996827338715, 'numLeaves': 16.325273144649806, 'scaleWeight': 3316.7046527329085, 'subsample': 0.9142239475127502}}, {'target': -34.82178575118136, 'params': {'colSam': 0.4793295011373687, 'maxDepth': 29.07524678704818, 'minChildWeight': 22.629122982618004, 'numLeaves': 38.23925194269444, 'scaleWeight': 566.8111557516714, 'subsample': 0.9535251743983582}}, {'target': -34.673030243125694, 'params': {'colSam': 0.6035425132587587, 'maxDepth': 56.018683019923344, 'minChildWeight': 21.756952427353333, 'numLeaves': 48.172360452166224, 'scaleWeight': 665.5115354719, 'subsample': 0.8058472582524933}}, {'target': -34.31360727455804, 'params': {'colSam': 0.5417439272834579, 'maxDepth': 62.70026829519553, 'minChildWeight': 17.73746955673307, 'numLeaves': 24.08350494293382, 'scaleWeight': 7922.87923014646, 'subsample': 0.6869806073240404}}, {'target': -40.86664487717754, 'params': {'colSam': 0.9021314758344248, 'maxDepth': 8.076288082125679, 'minChildWeight': 63.54790136848709, 'numLeaves': 86.89879843750772, 'scaleWeight': 1776.4095645254401, 'subsample': 0.9201264421274596}}, {'target': -39.77881367806462, 'params': {'colSam': 0.44729243354629433, 'maxDepth': 21.187409339817847, 'minChildWeight': 60.05408429055687, 'numLeaves': 34.87644432874529, 'scaleWeight': 8729.501994646238, 'subsample': 0.9325419807121922}}, {'target': -34.31360727455804, 'params': {'colSam': 0.5157825468651684, 'maxDepth': 56.039721902743445, 'minChildWeight': 17.35543660588507, 'numLeaves': 40.488108275091264, 'scaleWeight': 662.4622404059985, 'subsample': 0.5143659392679633}}, {'target': -35.37008243510225, 'params': {'colSam': 0.4364779083513199, 'maxDepth': 68.08135434892466, 'minChildWeight': 25.569795986858377, 'numLeaves': 48.15386925777597, 'scaleWeight': 7776.248333361143, 'subsample': 0.8842758880666975}}, {'target': -34.31360727455804, 'params': {'colSam': 0.9598718583825046, 'maxDepth': 66.87222635273301, 'minChildWeight': 9.295338303719946, 'numLeaves': 21.11010486759652, 'scaleWeight': 7927.907389857338, 'subsample': 0.8429672142092655}}, {'target': -42.20978279987651, 'params': {'colSam': 0.8785084647210502, 'maxDepth': 9.324829748420099, 'minChildWeight': 68.3317981348776, 'numLeaves': 82.44661431491579, 'scaleWeight': 8102.987417286017, 'subsample': 0.7564876918234027}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 834.4239179732145, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 1014.953543537036, 'subsample': 0.4}}, {'target': -42.20978279987651, 'params': {'colSam': 0.6969632590316934, 'maxDepth': 87.68043802576663, 'minChildWeight': 68.91508468926689, 'numLeaves': 72.44293874376471, 'scaleWeight': 1142.4973057025034, 'subsample': 0.884545166459548}}, {'target': -42.86321750024972, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 891.9414383829503, 'subsample': 0.4}}, {'target': -34.673030243125694, 'params': {'colSam': 0.46669962908601875, 'maxDepth': 69.38710714118311, 'minChildWeight': 21.667607973720184, 'numLeaves': 57.52358403981446, 'scaleWeight': 7777.718500137145, 'subsample': 0.8549546038009129}}, {'target': -34.31360727455804, 'params': {'colSam': 0.8541904306772576, 'maxDepth': 87.11006441972286, 'minChildWeight': 11.509351081024452, 'numLeaves': 87.74685766329935, 'scaleWeight': 7853.541097156227, 'subsample': 0.6835537166522849}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 739.8240688481845, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 0.9184109879199404, 'maxDepth': 2.7994859341856744, 'minChildWeight': 6.15236232895971, 'numLeaves': 73.43446545475177, 'scaleWeight': 7837.6350880445025, 'subsample': 0.8493389574415606}}, {'target': -34.31360727455804, 'params': {'colSam': 0.8592950008873368, 'maxDepth': 54.61495766812215, 'minChildWeight': 19.69978910747226, 'numLeaves': 28.1031788690583, 'scaleWeight': 660.087151944373, 'subsample': 0.8604646608966879}}, {'target': -34.673030243125694, 'params': {'colSam': 0.9556348514461854, 'maxDepth': 54.86324328484775, 'minChildWeight': 21.324410382501803, 'numLeaves': 50.26506596067947, 'scaleWeight': 672.9025316893255, 'subsample': 0.6488187460427478}}, {'target': -34.673030243125694, 'params': {'colSam': 0.5092901679015147, 'maxDepth': 59.133514272566615, 'minChildWeight': 21.194083833448833, 'numLeaves': 39.45129467550681, 'scaleWeight': 665.2465217511574, 'subsample': 0.8538265981311306}}, {'target': -34.31360727455804, 'params': {'colSam': 0.9585803595730387, 'maxDepth': 84.50993954602508, 'minChildWeight': 19.7622365047226, 'numLeaves': 89.74112660582439, 'scaleWeight': 7636.334385451178, 'subsample': 0.8729373027001727}}, {'target': -34.31360727455804, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 7516.291088719325, 'subsample': 0.4}}, {'target': -40.3054332487089, 'params': {'colSam': 0.901443259355229, 'maxDepth': 87.79227227857581, 'minChildWeight': 62.75997803870098, 'numLeaves': 9.48325352027492, 'scaleWeight': 7476.780827661764, 'subsample': 0.4466704376640014}}, {'target': -35.523804197246186, 'params': {'colSam': 0.8281056367196838, 'maxDepth': 4.488619679412936, 'minChildWeight': 28.18942166403389, 'numLeaves': 84.40868175426719, 'scaleWeight': 7679.806615526464, 'subsample': 0.44903490034121896}}, {'target': -34.31360727455804, 'params': {'colSam': 0.41715736649685453, 'maxDepth': 6.992886641765902, 'minChildWeight': 11.76414464953981, 'numLeaves': 55.910391501849915, 'scaleWeight': 371.0387087264252, 'subsample': 0.9156318854061642}}, {'target': -34.673030243125694, 'params': {'colSam': 0.5091315854335592, 'maxDepth': 82.9822819267828, 'minChildWeight': 21.687236448333994, 'numLeaves': 40.33173556823298, 'scaleWeight': 259.6979163516152, 'subsample': 0.4518550634839024}}, {'target': -42.86321750024972, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 5.0, 'scaleWeight': 395.8914553645558, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 268.5507027067066, 'subsample': 1.0}}, {'target': -34.31360727455804, 'params': {'colSam': 0.8445474116564273, 'maxDepth': 19.895246650733984, 'minChildWeight': 4.8878980868689315, 'numLeaves': 28.49737133910258, 'scaleWeight': 143.86390130027135, 'subsample': 0.8795888078977194}}, {'target': -42.86321750024972, 'params': {'colSam': 0.4, 'maxDepth': 90.0, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 148.68381102735415, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 28.83900319551138, 'subsample': 1.0}}, {'target': -34.31360727455804, 'params': {'colSam': 0.9560727459196806, 'maxDepth': 67.81884534686124, 'minChildWeight': 1.0216997234707281, 'numLeaves': 44.346876096263685, 'scaleWeight': 4707.93167033557, 'subsample': 0.6734747403954611}}, {'target': -34.31360727455804, 'params': {'colSam': 0.7712454839136114, 'maxDepth': 63.550095412527185, 'minChildWeight': 0.04799949664378428, 'numLeaves': 52.81130953776617, 'scaleWeight': 4585.784694143745, 'subsample': 0.7001256810582449}}, {'target': -42.86321750024972, 'params': {'colSam': 0.8119416684679152, 'maxDepth': 9.903849890220652, 'minChildWeight': 69.87346555194371, 'numLeaves': 7.5713621664914115, 'scaleWeight': 4614.968838664676, 'subsample': 0.7183881339351529}}, {'target': -34.31360727455804, 'params': {'colSam': 1.0, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 244.97670181420875, 'subsample': 1.0}}, {'target': -34.31360727455804, 'params': {'colSam': 0.6216877121792217, 'maxDepth': 79.08155132769404, 'minChildWeight': 7.313333855779861, 'numLeaves': 38.140212508816184, 'scaleWeight': 4816.2424468086565, 'subsample': 0.8152536454206569}}, {'target': -34.31360727455804, 'params': {'colSam': 0.6894909705357428, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4492.999957401231, 'subsample': 0.6262021840193196}}, {'target': -36.78637060407853, 'params': {'colSam': 0.6149926702867499, 'maxDepth': 7.280642606583123, 'minChildWeight': 48.87422864526509, 'numLeaves': 83.48538722304103, 'scaleWeight': 4890.47654164152, 'subsample': 0.635502962848704}}, {'target': -34.31360727455804, 'params': {'colSam': 0.68875413950766, 'maxDepth': 31.24890974452113, 'minChildWeight': 2.066468627758109, 'numLeaves': 68.7856440934454, 'scaleWeight': 4375.30594629326, 'subsample': 0.6601452786408631}}, {'target': -36.17429739230949, 'params': {'colSam': 0.9016465681707629, 'maxDepth': 80.0448514178957, 'minChildWeight': 42.23309316165087, 'numLeaves': 6.279506756258231, 'scaleWeight': 4292.6541099572305, 'subsample': 0.9937200404790633}}, {'target': -34.88835232882974, 'params': {'colSam': 0.7841270506013098, 'maxDepth': 71.70677053764787, 'minChildWeight': 23.403806228655128, 'numLeaves': 5.416555137980495, 'scaleWeight': 4436.2357738996325, 'subsample': 0.6028730895536849}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 750.4577114359151, 'subsample': 0.4}}, {'target': -41.88055219581003, 'params': {'colSam': 0.41790181755297073, 'maxDepth': 4.802451598673876, 'minChildWeight': 67.60051672343398, 'numLeaves': 87.45287810778487, 'scaleWeight': 4438.692141296204, 'subsample': 0.8386206133561577}}, {'target': -34.642176807318876, 'params': {'colSam': 0.7172316734210727, 'maxDepth': 8.499702581501088, 'minChildWeight': 20.373588342096912, 'numLeaves': 89.02670699288498, 'scaleWeight': 4154.6373667558255, 'subsample': 0.8185252942646687}}, {'target': -34.31360727455804, 'params': {'colSam': 0.5605109753503217, 'maxDepth': 12.435026025733304, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4034.139234815194, 'subsample': 1.0}}, {'target': -42.20978279987651, 'params': {'colSam': 0.7658839280708682, 'maxDepth': 64.32267020582165, 'minChildWeight': 68.7373578100887, 'numLeaves': 28.70609391400375, 'scaleWeight': 4083.939316992359, 'subsample': 0.9776749592463223}}, {'target': -34.31360727455804, 'params': {'colSam': 0.9930138570681035, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 4267.508158166999, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 0.8818675765201868, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 3927.224727639284, 'subsample': 0.4646659896354137}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 2.0, 'minChildWeight': 0.01, 'numLeaves': 5.0, 'scaleWeight': 717.6308913547869, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 1.0, 'maxDepth': 90.0, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 795.0013547779894, 'subsample': 0.4}}, {'target': -34.31360727455804, 'params': {'colSam': 0.4, 'maxDepth': 82.53289252582141, 'minChildWeight': 0.01, 'numLeaves': 90.0, 'scaleWeight': 3828.855241803294, 'subsample': 1.0}}, {'target': -34.31360727455804, 'params': {'colSam': 0.8048029718486793, 'maxDepth': 4.152881353977363, 'minChildWeight': 16.851064908654855, 'numLeaves': 10.996098789486943, 'scaleWeight': 3808.5836064611767, 'subsample': 0.5876642770222945}}, {'target': -42.86321750024972, 'params': {'colSam': 1.0, 'maxDepth': 23.034300552997806, 'minChildWeight': 70.0, 'numLeaves': 90.0, 'scaleWeight': 3747.8799339011343, 'subsample': 0.6122943483690754}}, {'target': -34.31360727455804, 'params': {'colSam': 0.6646084145527652, 'maxDepth': 85.5332669833348, 'minChildWeight': 2.7190823621738778, 'numLeaves': 26.606218029206534, 'scaleWeight': 3887.170245456212, 'subsample': 0.8995052449855669}}]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import numpy as np\n",
    "def lgb_evaluate(numLeaves, maxDepth, scaleWeight, minChildWeight, subsample, colSam):\n",
    "    reg=lgb.LGBMRegressor(num_leaves=31,\n",
    "                          max_depth= 2,\n",
    "                          scale_pos_weight= scaleWeight,\n",
    "                          min_child_weight= minChildWeight,\n",
    "                          subsample= 0.4,\n",
    "                          colsample_bytree= 0.4,\n",
    "                         learning_rate=0.05,\n",
    "                         n_estimators=20)\n",
    "#   scores = cross_val_score(reg, train_x, train_y, cv=5, scoring='roc_auc')\n",
    "    scores = cross_val_score(reg, train_x, train_y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return np.mean(scores)\n",
    "def bayesOpt(train_x, train_y):\n",
    "    lgbBO = BayesianOptimization(lgb_evaluate, {                                               \n",
    "                                                'numLeaves':  (5, 90),\n",
    "                                                'maxDepth': (2, 90),\n",
    "                                                'scaleWeight': (1, 10000),\n",
    "                                                'minChildWeight': (0.01, 70),\n",
    "                                                'subsample': (0.4, 1),                                               \n",
    "                                                'colSam': (0.4, 1)\n",
    "                                            })\n",
    "    lgbBO.maximize(init_points=5, n_iter=50)\n",
    "    print(lgbBO.res)\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "train_x, X_test, train_y, y_test = train_test_split(X, y, test_size=0.2)\n",
    "bayesOpt(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsample |   gamma   | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8298  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.602   \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 5.116   \u001b[0m | \u001b[0m 232.1   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8147  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.728   \u001b[0m | \u001b[0m 0.4028  \u001b[0m | \u001b[0m 6.772   \u001b[0m | \u001b[0m 477.3   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8291  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.391   \u001b[0m | \u001b[0m 0.03711 \u001b[0m | \u001b[0m 7.693   \u001b[0m | \u001b[0m 475.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8301  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 3.498   \u001b[0m | \u001b[95m 0.09245 \u001b[0m | \u001b[95m 4.648   \u001b[0m | \u001b[95m 232.0   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8074  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.749   \u001b[0m | \u001b[0m 0.8485  \u001b[0m | \u001b[0m 9.74    \u001b[0m | \u001b[0m 473.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8266  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.927   \u001b[0m | \u001b[0m 0.2933  \u001b[0m | \u001b[0m 4.461   \u001b[0m | \u001b[0m 233.5   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8161  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.049   \u001b[0m | \u001b[0m 0.5459  \u001b[0m | \u001b[0m 5.381   \u001b[0m | \u001b[0m 232.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8285  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.526   \u001b[0m | \u001b[0m 0.1804  \u001b[0m | \u001b[0m 4.33    \u001b[0m | \u001b[0m 230.4   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8304  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 4.628   \u001b[0m | \u001b[95m 0.2627  \u001b[0m | \u001b[95m 5.44    \u001b[0m | \u001b[95m 474.5   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[12:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8187  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.876   \u001b[0m | \u001b[0m 0.8737  \u001b[0m | \u001b[0m 3.465   \u001b[0m | \u001b[0m 472.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8165  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.909   \u001b[0m | \u001b[0m 0.9084  \u001b[0m | \u001b[0m 5.282   \u001b[0m | \u001b[0m 476.4   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8227  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.801   \u001b[0m | \u001b[0m 0.179   \u001b[0m | \u001b[0m 6.604   \u001b[0m | \u001b[0m 473.1   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.814   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.577   \u001b[0m | \u001b[0m 0.4614  \u001b[0m | \u001b[0m 6.14    \u001b[0m | \u001b[0m 230.4   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8258  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.508   \u001b[0m | \u001b[0m 0.2351  \u001b[0m | \u001b[0m 6.865   \u001b[0m | \u001b[0m 474.6   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:54:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.8305  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 4.756   \u001b[0m | \u001b[95m 0.4546  \u001b[0m | \u001b[95m 4.182   \u001b[0m | \u001b[95m 231.6   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8145  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.322   \u001b[0m | \u001b[0m 0.901   \u001b[0m | \u001b[0m 5.433   \u001b[0m | \u001b[0m 473.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8288  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.548   \u001b[0m | \u001b[0m 0.2893  \u001b[0m | \u001b[0m 4.018   \u001b[0m | \u001b[0m 233.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.8205  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.649   \u001b[0m | \u001b[0m 0.4846  \u001b[0m | \u001b[0m 4.908   \u001b[0m | \u001b[0m 235.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.8033  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.479   \u001b[0m | \u001b[0m 0.9573  \u001b[0m | \u001b[0m 5.04    \u001b[0m | \u001b[0m 232.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8053  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.6905  \u001b[0m | \u001b[0m 0.31    \u001b[0m | \u001b[0m 9.17    \u001b[0m | \u001b[0m 920.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.8296  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.166   \u001b[0m | \u001b[0m 0.1047  \u001b[0m | \u001b[0m 3.708   \u001b[0m | \u001b[0m 233.1   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m 0.8314  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 3.939   \u001b[0m | \u001b[95m 0.7519  \u001b[0m | \u001b[95m 4.028   \u001b[0m | \u001b[95m 231.4   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.8241  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.158   \u001b[0m | \u001b[0m 0.9316  \u001b[0m | \u001b[0m 3.219   \u001b[0m | \u001b[0m 232.4   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.8297  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.264   \u001b[0m | \u001b[0m 0.2604  \u001b[0m | \u001b[0m 4.223   \u001b[0m | \u001b[0m 232.3   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.8313  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.354   \u001b[0m | \u001b[0m 0.4168  \u001b[0m | \u001b[0m 3.353   \u001b[0m | \u001b[0m 232.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.827   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.463   \u001b[0m | \u001b[0m 0.1881  \u001b[0m | \u001b[0m 3.808   \u001b[0m | \u001b[0m 229.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:55:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.8314  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.409   \u001b[0m | \u001b[0m 0.4507  \u001b[0m | \u001b[0m 3.719   \u001b[0m | \u001b[0m 234.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pbounds = {  'learning_rate': (0.01, 1.0),  'n_estimators': (100, 1000), 'max_depth': (3,10),   \n",
    "    'subsample': (1.0, 1.0),  # Change for big datasets\n",
    "    'colsample': (1.0, 1.0),   # Change for datasets with lots of features\n",
    "    'gamma': (0, 5)}\n",
    "def xgboost_hyper_param(learning_rate, n_estimators, max_depth, subsample, colsample, gamma):\n",
    "    dataset = loadtxt('C:/Users/In-Ho Lee/testAI/scikitlearn_keras_examples/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "    X = dataset[:,0:8]\n",
    "    y = dataset[:,8]\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    clf = XGBClassifier( max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, colsample=colsample, gamma=gamma)\n",
    "    return np.mean(cross_val_score(clf, X, y, cv=3, scoring='roc_auc'))\n",
    "optimizer = BayesianOptimization( f=xgboost_hyper_param, pbounds=pbounds, random_state=1)\n",
    "optimizer.maximize(init_points=3, n_iter=24, acq='ei', xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pbounds = {\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3,10),\n",
    "    'subsample': (1.0, 1.0),  # Change for big datasets\n",
    "    'colsample': (1.0, 1.0),  # Change for datasets with lots of features\n",
    "    'gamma': (0, 5)}\n",
    "def xgboost_hyper_param(learning_rate, n_estimators, max_depth, subsample, colsample, gamma):\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    clf = XGBClassifier( max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators,\n",
    "         subsample=subsample, colsample=colsample, gamma=gamma)\n",
    "    return np.mean(cross_val_score(clf, X, y, cv=3, scoring='roc_auc'))\n",
    "def boexe(X,y):\n",
    "    optimizer = BayesianOptimization( f=xgboost_hyper_param, pbounds=pbounds, random_state=1)\n",
    "    optimizer.maximize(init_points=5, n_iter=50, acq='ei', xi=0.01)\n",
    "dataset = loadtxt('C:/Users/In-Ho Lee/testAI/scikitlearn_keras_examples/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "boexe(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 249, number of negative: 149\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3970\n",
      "[LightGBM] [Info] Number of data points in the train set: 398, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625628 -> initscore=0.513507\n",
      "[LightGBM] [Info] Start training from score 0.513507\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965424739195231"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_breast_cancer,load_boston,load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\n",
    "pd.options.display.max_columns = 999\n",
    "#loading the breast cancer dataset\n",
    "X=load_breast_cancer()\n",
    "df=pd.DataFrame(X.data,columns=X.feature_names)\n",
    "Y=X.target \n",
    "#scaling the features using Standard Scaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(df)\n",
    "X=pd.DataFrame(sc.fit_transform(df))\n",
    "#train_test_split \n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=0)\n",
    "#converting the dataset into proper LGB format \n",
    "d_train=lgb.Dataset(X_train, label=y_train)\n",
    "#Specifying the parameter\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='binary' #Binary target feature\n",
    "params['metric']='binary_logloss' #metric for binary classification\n",
    "params['max_depth']=10\n",
    "#train the model \n",
    "clf=lgb.train(params,d_train,100) #train the model on 100 epocs\n",
    "#prediction on the test set\n",
    "y_pred=clf.predict(X_test)\n",
    "#rounding the values\n",
    "y_pred=y_pred.round(0)\n",
    "#converting from float to integer\n",
    "y_pred=y_pred.astype(int)\n",
    "#roc_auc_score metric\n",
    "roc_auc_score(y_pred,y_test)\n",
    "#0.9672167056074766\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 456\n",
      "[LightGBM] [Info] Number of data points in the train set: 124, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -1.131402\n",
      "[LightGBM] [Info] Start training from score -0.928461\n",
      "[LightGBM] [Info] Start training from score -1.264934\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9696969696969697"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset\n",
    "X1=load_wine()\n",
    "df_1=pd.DataFrame(X1.data,columns=X1.feature_names)\n",
    "Y_1=X1.target\n",
    "#Scaling using the Standard Scaler\n",
    "sc_1=StandardScaler()\n",
    "sc_1.fit(df_1)\n",
    "X_1=pd.DataFrame(sc_1.fit_transform(df_1))\n",
    "#train-test-split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_1,Y_1,test_size=0.3,random_state=0)\n",
    "#Converting the dataset in proper LGB format\n",
    "d_train=lgb.Dataset(X_train, label=y_train)\n",
    "#setting up the parameters\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_class']=3 #no.of unique values in the target class not inclusive of the end value\n",
    "#training the model\n",
    "clf=lgb.train(params,d_train,100)  #training the model on 100 epocs\n",
    "#prediction on the test dataset\n",
    "y_pred_1=clf.predict(X_test)\n",
    "#printing the predictions\n",
    "y_pred_1\n",
    "#argmax() method \n",
    "y_pred_1 = [np.argmax(line) for line in y_pred_1]\n",
    "#printing the predictions\n",
    "[0,2,1,0,1,0,0,2,...]\n",
    "#using precision score for error metrics\n",
    "precision_score(y_pred_1,y_test,average=None).mean()\n",
    "# 0.9545454545454546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 354, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.745480\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\In-Ho Lee\\.conda\\envs\\testAI\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.1990255464617"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the Boston Dataset\n",
    "X=load_boston()\n",
    "df=pd.DataFrame(X.data,columns=X.feature_names)\n",
    "Y=X.target\n",
    "#Scaling using the Standard Scaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(df)\n",
    "X=pd.DataFrame(sc.fit_transform(df))\n",
    "#train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=0)\n",
    "#Converting the data into proper LGB Dataset Format\n",
    "d_train=lgb.Dataset(X_train, label=y_train)\n",
    "#Declaring the parameters\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='regression'#regression task\n",
    "params['n_estimators']=100\n",
    "params['max_depth']=10\n",
    "#model creation and training\n",
    "clf=lgb.train(params,d_train,100)\n",
    "#model prediction on X_test\n",
    "y_pred=clf.predict(X_test)\n",
    "#using RMSE error metric\n",
    "mean_squared_error(y_pred,y_test)\n",
    "# 0.9672167056074766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testAI",
   "language": "python",
   "name": "testai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
